{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"../images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Build a Named Entity Recognizer\n",
    "\n",
    "In this notebook, you'll build an NER (named entity recognition) application that finds disease names in medical disease abstracts. The model does not \"search\" for names from a list, but rather \"recognizes\" that certain words are disease references from the context of the language. \n",
    "\n",
    "**[3.1 Token Classification from the Command Line](#3.1-Token-Classification-from-the-Command-Line)**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.1.1 Data Input](#3.1.1-Data-Input)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[3.1.1.1 IOB Tagging](#3.1.1.1-IOB-Tagging)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.1.2 Configuration File](#3.1.2-Configuration-File)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.1.3 Hydra-Enabled Python Scripts](#3.1.3-Hydra-Enabled-Python-Scripts)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.1.4 Exercise: Train the Model](#3.1.4-Exercise:-Train-the-Model)<br>\n",
    "**[3.2 Domain-Specific Training](#3.2-Domain-Specific-Training)**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.2.1 Visualize the Results with TensorBoard](#3.2.1-Visualize-the-Results-with-TensorBoard)<br>\n",
    "**[3.3 Evaluation](#3.3-Evaluation)**<br>\n",
    "**[3.4 Inference](#3.4-Inference)**<br>\n",
    "\n",
    "For the NER task, you'll follow the same basic steps as in the text classification task to build your project, train it, and test it.  This time, however, you'll train a classifier on the *domain-specific* BioMegatron language model.  BioMegatron is a [BERT](https://arxiv.org/abs/1810.04805)-like [Megatron-LM](https://arxiv.org/pdf/1909.08053.pdf) model pre-trained on a large biomedical text corpus ([PubMed](https://pubmed.ncbi.nlm.nih.gov/) abstracts and full-text commercial use collection).  We can expect to have better performance compared to the general language models, because our disease dataset is from the same biomedical domain.\n",
    "\n",
    "There are some alternatives of BioMegatron, most notably [BioBERT](https://arxiv.org/abs/1901.08746). Compared to BioBERT, BioMegatron is larger by model size and pre-trained on larger text corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3.1 Token Classification from the Command Line\n",
    "The question we want to answer is:\n",
    "\n",
    "**Given sentences from medical abstracts, what diseases are mentioned?**<br>\n",
    "\n",
    "Recall the NLP models available with NeMo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tree nemo/examples/nlp -L 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the [token classification](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/token_classification.html) model for NER because we are classifying at the \"token\" level, in this case classifying words related to diseases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.1 Data Input\n",
    "As we saw in the [1.0 Explore the Data](010_ExploreData.ipynb) notebook, the dataset for the NER project is made up of sentences with IOB tagging for disease names, where each word in a sentence is tagged as inside, outside, or the beginning of a named entity. \n",
    "\n",
    "The training text and label files are `text_train.txt` and `labels_train.txt`, respectively.  The validation and test files follow a similar naming pattern. Verify the location of the data files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4.0M\n",
      "-rw-rw-r-- 1 1000 1000  181K May 20 23:32 dev.tsv\n",
      "-rw-rw-r-- 1 1000 1000     5 May 20 23:32 label_ids.csv\n",
      "-rw-rw-r-- 1 1000 1000    52 May 20 23:32 label_stats.tsv\n",
      "-rw-rw-r-- 1 1000 1000   48K May 20 23:32 labels_dev.txt\n",
      "-rw-rw-r-- 1 1000 1000   49K May 20 23:32 labels_test.txt\n",
      "-rw-rw-r-- 1 1000 1000  271K May 20 23:32 labels_train.txt\n",
      "-rw-rw-r-- 1 1000 1000  185K May 20 23:32 test.tsv\n",
      "-rw-rw-r-- 1 1000 1000  135K May 20 23:32 text_dev.txt\n",
      "-rw-rw-r-- 1 1000 1000  138K May 20 23:32 text_test.txt\n",
      "-rw-rw-r-- 1 1000 1000  758K May 20 23:32 text_train.txt\n",
      "-rw-rw-r-- 1 1000 1000 1023K May 20 23:32 train.tsv\n",
      "-rw-rw-r-- 1 1000 1000  1.2M May 20 23:32 train_dev.tsv\n"
     ]
    }
   ],
   "source": [
    "NER3_DATA_DIR = 'data/NCBI_ner-3'\n",
    "!ls -lh $NER3_DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****\n",
      "text_test.txt sample\n",
      "*****\n",
      "Clustering of missense mutations in the ataxia - telangiectasia gene in a sporadic T - cell leukaemia . \n",
      "Ataxia - telangiectasia ( A - T ) is a recessive multi - system disorder caused by mutations in the ATM gene at 11q22 - q23 ( ref . 3 ) . \n",
      "The risk of cancer , especially lymphoid neoplasias , is substantially elevated in A - T patients and has long been associated with chromosomal instability . \n",
      "\n",
      "*****\n",
      "labels_test.txt sample\n",
      "*****\n",
      "O O O O O O B I I O O O B I I I I O \n",
      "B I I O B I I O O O B I I I I O O O O O O O O O O O O O O O O O \n",
      "O O O B O O B I O O O O O B I I O O O O O O O O O O \n"
     ]
    }
   ],
   "source": [
    "# Take a look at the data\n",
    "print(\"*****\\ntext_test.txt sample\\n*****\")\n",
    "!head -n 3 $NER3_DATA_DIR/text_test.txt\n",
    "print(\"\\n*****\\nlabels_test.txt sample\\n*****\")\n",
    "!head -n 3 $NER3_DATA_DIR/labels_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1.1 IOB Tagging\n",
    "Recall that the sentences and labels in the NER dataset map to each other with _inside, outside, beginning (IOB)_ tagging.\n",
    "This mechanism can be used in a general way for multiple named entity types:\n",
    "* B-{CHUNK_TYPE} – for the word in the Beginning chunk\n",
    "* I-{CHUNK_TYPE} – for words Inside the chunk\n",
    "* O – Outside any chunk\n",
    "\n",
    "In our case, we are only looking for \"disease\" as our entity (or chunk) type, so we don't need to identify beyond the three classes: I, O, and B.\n",
    "**Three classes**\n",
    "* B - Beginning of disease name\n",
    "* I - Inside word of disease name\n",
    "* O - Outside of all disease names\n",
    "\n",
    "```text\n",
    "Identification of APC2 , a homologue of the adenomatous polyposis coli tumour suppressor .\n",
    "O              O  O    O O O         O  O   B           I         I    I      O          O  \n",
    "```\n",
    "\n",
    "These are defined in our `labels.csv` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O\n",
      "B\n",
      "I"
     ]
    }
   ],
   "source": [
    "!head $NER3_DATA_DIR/label_ids.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were looking for two kinds of named entities, such as nouns and verbs in a parts-of-speech analysis, we would use a five-class IOB scheme:<br>\n",
    "**Five classes**\n",
    "* B-N - Beginning of noun word or phrase\n",
    "* I-N - Inside noun word or phrase\n",
    "* B-V - Beginning of verb word or phrase\n",
    "* I-V - Inside verb word or phrase\n",
    "* O   - Outside all nouns and verbs\n",
    "\n",
    "If you are intereested in learning more, take a look at [this paper](http://cs229.stanford.edu/proj2005/KrishnanGanapathy-NamedEntityRecognition.pdf) on the subject."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NCBI_ner-3 disease data is in the correct format for token classification as described in the [documentation](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/token_classification.html#data-input-for-token-classification-model), so we are ready to look at the configuration file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.2 Configuration File\n",
    "Look at more detail for the NeMo token classification directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mnemo/examples/nlp/token_classification\u001b[00m\n",
      "├── \u001b[01;34mdata\u001b[00m\n",
      "│   ├── get_tatoeba_data.py\n",
      "│   ├── import_from_iob_format.py\n",
      "│   └── prepare_data_for_punctuation_capitalization.py\n",
      "├── punctuation_capitalization_evaluate.py\n",
      "├── punctuation_capitalization_train.py\n",
      "├── token_classification_evaluate.py\n",
      "└── token_classification_train.py\n",
      "\n",
      "1 directory, 7 files\n"
     ]
    }
   ],
   "source": [
    "TC_DIR = \"nemo/examples/nlp/token_classification\"\n",
    "!tree $TC_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The config file for NER, `token_classification_config.yaml`, specifies model, training, and experiment management details, such as file locations, pretrained models, and hyperparameters.  This is the same general pattern used in the text classification configuration file.  We'll take a look at the details of each section using the `OmegaConf` tool introduced in the text classification project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_ids: null\n",
      "class_labels:\n",
      "  class_labels_file: label_ids.csv\n",
      "dataset:\n",
      "  data_dir: ???\n",
      "  class_balancing: null\n",
      "  max_seq_length: 128\n",
      "  pad_label: O\n",
      "  ignore_extra_tokens: false\n",
      "  ignore_start_end: false\n",
      "  use_cache: true\n",
      "  num_workers: 2\n",
      "  pin_memory: false\n",
      "  drop_last: false\n",
      "train_ds:\n",
      "  text_file: text_train.txt\n",
      "  labels_file: labels_train.txt\n",
      "  shuffle: true\n",
      "  num_samples: -1\n",
      "  batch_size: 64\n",
      "validation_ds:\n",
      "  text_file: text_dev.txt\n",
      "  labels_file: labels_dev.txt\n",
      "  shuffle: false\n",
      "  num_samples: -1\n",
      "  batch_size: 64\n",
      "test_ds:\n",
      "  text_file: text_dev.txt\n",
      "  labels_file: labels_dev.txt\n",
      "  shuffle: false\n",
      "  num_samples: -1\n",
      "  batch_size: 64\n",
      "tokenizer:\n",
      "  tokenizer_name: ${model.language_model.pretrained_model_name}\n",
      "  vocab_file: null\n",
      "  tokenizer_model: null\n",
      "  special_tokens: null\n",
      "language_model:\n",
      "  pretrained_model_name: bert-base-uncased\n",
      "  lm_checkpoint: null\n",
      "  config_file: null\n",
      "  config: null\n",
      "head:\n",
      "  num_fc_layers: 2\n",
      "  fc_dropout: 0.5\n",
      "  activation: relu\n",
      "  use_transformer_init: true\n",
      "optim:\n",
      "  name: adam\n",
      "  lr: 5.0e-05\n",
      "  weight_decay: 0.0\n",
      "  sched:\n",
      "    name: WarmupAnnealing\n",
      "    warmup_steps: null\n",
      "    warmup_ratio: 0.1\n",
      "    last_epoch: -1\n",
      "    monitor: val_loss\n",
      "    reduce_on_plateau: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "CONFIG_DIR = \"nemo/examples/nlp/token_classification/conf\"\n",
    "CONFIG_FILE = \"token_classification_config.yaml\"\n",
    "\n",
    "config = OmegaConf.load(CONFIG_DIR + \"/\" + CONFIG_FILE)\n",
    "\n",
    "# print the model section\n",
    "print(OmegaConf.to_yaml(config.model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['megatron-bert-345m-uncased',\n",
       " 'megatron-bert-345m-cased',\n",
       " 'megatron-bert-uncased',\n",
       " 'megatron-bert-cased',\n",
       " 'biomegatron-bert-345m-uncased',\n",
       " 'biomegatron-bert-345m-cased',\n",
       " 'bert-base-uncased',\n",
       " 'bert-large-uncased',\n",
       " 'bert-base-cased',\n",
       " 'bert-large-cased',\n",
       " 'bert-base-multilingual-uncased',\n",
       " 'bert-base-multilingual-cased',\n",
       " 'bert-base-chinese',\n",
       " 'bert-base-german-cased',\n",
       " 'bert-large-uncased-whole-word-masking',\n",
       " 'bert-large-cased-whole-word-masking',\n",
       " 'bert-large-uncased-whole-word-masking-finetuned-squad',\n",
       " 'bert-large-cased-whole-word-masking-finetuned-squad',\n",
       " 'bert-base-cased-finetuned-mrpc',\n",
       " 'bert-base-german-dbmdz-cased',\n",
       " 'bert-base-german-dbmdz-uncased',\n",
       " 'cl-tohoku/bert-base-japanese',\n",
       " 'cl-tohoku/bert-base-japanese-whole-word-masking',\n",
       " 'cl-tohoku/bert-base-japanese-char',\n",
       " 'cl-tohoku/bert-base-japanese-char-whole-word-masking',\n",
       " 'TurkuNLP/bert-base-finnish-cased-v1',\n",
       " 'TurkuNLP/bert-base-finnish-uncased-v1',\n",
       " 'wietsedv/bert-base-dutch-cased',\n",
       " 'distilbert-base-uncased',\n",
       " 'distilbert-base-uncased-distilled-squad',\n",
       " 'distilbert-base-cased',\n",
       " 'distilbert-base-cased-distilled-squad',\n",
       " 'distilbert-base-german-cased',\n",
       " 'distilbert-base-multilingual-cased',\n",
       " 'distilbert-base-uncased-finetuned-sst-2-english',\n",
       " 'roberta-base',\n",
       " 'roberta-large',\n",
       " 'roberta-large-mnli',\n",
       " 'distilroberta-base',\n",
       " 'roberta-base-openai-detector',\n",
       " 'roberta-large-openai-detector',\n",
       " 'albert-base-v1',\n",
       " 'albert-large-v1',\n",
       " 'albert-xlarge-v1',\n",
       " 'albert-xxlarge-v1',\n",
       " 'albert-base-v2',\n",
       " 'albert-large-v2',\n",
       " 'albert-xlarge-v2',\n",
       " 'albert-xxlarge-v2']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# complete list of supported BERT-like models\n",
    "from nemo.collections import nlp as nemo_nlp\n",
    "nemo_nlp.modules.get_pretrained_lm_models_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `model` section, a path for `dataset.data_dir` that contains all the data files is required.  The actual file names we are using already conform to the default values, so we don't need to override those.\n",
    "\n",
    "For our first try, we can override `language_model.pretrained_model_name` to `bert-base-cased`, so we can compare the results to the domain-specific `biomegatron-bert-345m-cased` in another experiment.  Since we will need to conserve memory space to run BioMegatron, we will go ahead and reduce `dataset.max_seq_length` and the `batch_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpus: 1\n",
      "num_nodes: 1\n",
      "max_epochs: 5\n",
      "max_steps: null\n",
      "accumulate_grad_batches: 1\n",
      "gradient_clip_val: 0.0\n",
      "amp_level: O0\n",
      "precision: 16\n",
      "accelerator: ddp\n",
      "checkpoint_callback: false\n",
      "logger: false\n",
      "log_every_n_steps: 1\n",
      "val_check_interval: 1.0\n",
      "resume_from_checkpoint: null\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the trainer section\n",
    "print(OmegaConf.to_yaml(config.trainer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For efficiency, we can set the `amp_level` to 'O1'.  Since the language models we are going to compare are large and take a long time to run, we will override the `max_epochs` to a small number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp_dir: null\n",
      "name: token_classification_model\n",
      "create_tensorboard_logger: true\n",
      "create_checkpoint_callback: true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the experiment manager section\n",
    "print(OmegaConf.to_yaml(config.exp_manager))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no need to change the `exp_manger` default settings for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3.1.3 Hydra-Enabled Python Scripts\n",
    "The Python scripts, `token_classification_train.py` and `token_evaluate.py`, encapsulate everything needed to run a token classification experiment defined by the configuration file.  Training and evaluation are expected to be run separately in this case.  As with text classification, both scripts employ Facebook's [Hydra](https://hydra.cc/) tool for configuration management, which allows the entire experiment to be run from the command line, overriding config file values as needed.\n",
    "\n",
    "To recap, the parameters we need to change or override are:\n",
    "\n",
    "* `model.language_model.pretrained_model_name`: set to 'bert-base-cased'\n",
    "* `model.dataset.data_dir`: set to /dli/task/data/NCBI_ner-3\n",
    "* `model.dataset.max_seq_length`: 64\n",
    "* `model.train_ds.batch_size`: set to 32\n",
    "* `model.val_ds.batch_size`: set to 32\n",
    "* `model.test_ds.batch_size`: set to 32\n",
    "* `trainer.amp_level`: set to \"O1\"\n",
    "* `trainer.max_epochs`: set to 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.4 Exercise: Train the Model\n",
    "Run the training script, `token_classification_train.py` just as you ran similar experiments in text classification notebook.   \n",
    "\n",
    "The new values for overrides are provided for you in the cell below.  Add the command with appropriate overrides and run the cell.  If you get stuck, refer to the [solution](solutions/ex3.1.4.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Using native 16bit precision.\n",
      "[NeMo I 2024-05-23 02:01:37 exp_manager:216] Experiments will be logged at /dli/task/Downloads/nvidia-dli-llm/part_2/nemo_experiments/token_classification_model/2024-05-23_02-01-37\n",
      "[NeMo I 2024-05-23 02:01:37 exp_manager:563] TensorboardLogger has been set up\n",
      "[NeMo I 2024-05-23 02:01:37 token_classification_train:109] Config: pretrained_model: null\n",
      "    trainer:\n",
      "      gpus: 1\n",
      "      num_nodes: 1\n",
      "      max_epochs: 3\n",
      "      max_steps: null\n",
      "      accumulate_grad_batches: 1\n",
      "      gradient_clip_val: 0.0\n",
      "      amp_level: O1\n",
      "      precision: 16\n",
      "      accelerator: ddp\n",
      "      checkpoint_callback: false\n",
      "      logger: false\n",
      "      log_every_n_steps: 1\n",
      "      val_check_interval: 1.0\n",
      "      resume_from_checkpoint: null\n",
      "    exp_manager:\n",
      "      exp_dir: null\n",
      "      name: token_classification_model\n",
      "      create_tensorboard_logger: true\n",
      "      create_checkpoint_callback: true\n",
      "    model:\n",
      "      label_ids: null\n",
      "      class_labels:\n",
      "        class_labels_file: label_ids.csv\n",
      "      dataset:\n",
      "        data_dir: data/NCBI_ner-3\n",
      "        class_balancing: null\n",
      "        max_seq_length: 16\n",
      "        pad_label: O\n",
      "        ignore_extra_tokens: false\n",
      "        ignore_start_end: false\n",
      "        use_cache: true\n",
      "        num_workers: 2\n",
      "        pin_memory: false\n",
      "        drop_last: false\n",
      "      train_ds:\n",
      "        text_file: text_train.txt\n",
      "        labels_file: labels_train.txt\n",
      "        shuffle: true\n",
      "        num_samples: -1\n",
      "        batch_size: 16\n",
      "      validation_ds:\n",
      "        text_file: text_dev.txt\n",
      "        labels_file: labels_dev.txt\n",
      "        shuffle: false\n",
      "        num_samples: -1\n",
      "        batch_size: 16\n",
      "      test_ds:\n",
      "        text_file: text_dev.txt\n",
      "        labels_file: labels_dev.txt\n",
      "        shuffle: false\n",
      "        num_samples: -1\n",
      "        batch_size: 16\n",
      "      tokenizer:\n",
      "        tokenizer_name: ${model.language_model.pretrained_model_name}\n",
      "        vocab_file: null\n",
      "        tokenizer_model: null\n",
      "        special_tokens: null\n",
      "      language_model:\n",
      "        pretrained_model_name: bert-base-cased\n",
      "        lm_checkpoint: null\n",
      "        config_file: null\n",
      "        config: null\n",
      "      head:\n",
      "        num_fc_layers: 2\n",
      "        fc_dropout: 0.5\n",
      "        activation: relu\n",
      "        use_transformer_init: true\n",
      "      optim:\n",
      "        name: adam\n",
      "        lr: 5.0e-05\n",
      "        weight_decay: 0.0\n",
      "        sched:\n",
      "          name: WarmupAnnealing\n",
      "          warmup_steps: null\n",
      "          warmup_ratio: 0.1\n",
      "          last_epoch: -1\n",
      "          monitor: val_loss\n",
      "          reduce_on_plateau: false\n",
      "    \n",
      "Downloading: 100%|██████████████████████████████| 570/570 [00:00<00:00, 579kB/s]\n",
      "Downloading: 100%|███████████████████████████| 213k/213k [00:00<00:00, 5.46MB/s]\n",
      "Downloading: 100%|███████████████████████████| 49.0/49.0 [00:00<00:00, 55.8kB/s]\n",
      "Downloading: 100%|███████████████████████████| 436k/436k [00:00<00:00, 14.6MB/s]\n",
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "[NeMo I 2024-05-23 02:01:39 token_classification_utils:54] Processing data/NCBI_ner-3/labels_train.txt\n",
      "[NeMo I 2024-05-23 02:01:39 token_classification_utils:90] Labels mapping {'O': 0, 'B': 1, 'I': 2} saved to : data/NCBI_ner-3/label_ids.csv\n",
      "[NeMo I 2024-05-23 02:01:39 token_classification_utils:99] Three most popular labels in data/NCBI_ner-3/labels_train.txt:\n",
      "[NeMo I 2024-05-23 02:01:39 data_preprocessing:135] label: 0, 124452 out of 135701 (91.71%).\n",
      "[NeMo I 2024-05-23 02:01:39 data_preprocessing:135] label: 2, 6115 out of 135701 (4.51%).\n",
      "[NeMo I 2024-05-23 02:01:39 data_preprocessing:135] label: 1, 5134 out of 135701 (3.78%).\n",
      "[NeMo I 2024-05-23 02:01:39 token_classification_utils:101] Total labels: 135701. Label frequencies - {0: 124452, 2: 6115, 1: 5134}\n",
      "[NeMo I 2024-05-23 02:01:39 token_classification_utils:107] Class weights restored from data/NCBI_ner-3/labels_train_weights.p\n",
      "[NeMo W 2024-05-23 02:01:39 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/nemo/core/classes/modelPT.py:243: UserWarning: update_node() is deprecated, use OmegaConf.update(). (Since 2.0)\n",
      "      self.cfg.update_node(config_path, return_path)\n",
      "    \n",
      "[NeMo I 2024-05-23 02:01:47 token_classification_dataset:116] Setting Max Seq length to: 16\n",
      "[NeMo I 2024-05-23 02:01:47 data_preprocessing:299] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2024-05-23 02:01:47 data_preprocessing:301] Min: 4 |                  Max: 178 |                  Mean: 35.938237463126846 |                  Median: 34.0\n",
      "[NeMo I 2024-05-23 02:01:47 data_preprocessing:307] 75 percentile: 45.00\n",
      "[NeMo I 2024-05-23 02:01:47 data_preprocessing:308] 99 percentile: 88.77\n",
      "[NeMo W 2024-05-23 02:01:47 token_classification_dataset:145] 4923 are longer than 16\n",
      "[NeMo I 2024-05-23 02:01:47 token_classification_dataset:148] *** Example ***\n",
      "[NeMo I 2024-05-23 02:01:47 token_classification_dataset:149] i: 0\n",
      "[NeMo I 2024-05-23 02:01:47 token_classification_dataset:150] subtokens: [CLS] I ##dent ##ification of AP ##C ##2 , a ho ##mo ##logue of the ad ##eno ##mat ##ous p ##oly ##po ##sis co ##li t ##umour suppress ##or . [SEP]\n",
      "[NeMo I 2024-05-23 02:01:47 token_classification_dataset:151] loss_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2024-05-23 02:01:47 token_classification_dataset:152] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2024-05-23 02:01:47 token_classification_dataset:153] subtokens_mask: 0 0 0 0 1 0 0 0 1 0 1 0 1 0 1 0\n",
      "[NeMo I 2024-05-23 02:01:47 token_classification_dataset:155] labels: 0 1 1 1 2 2 2 2 2 2 2 2 0 0 0 0\n",
      "[NeMo I 2024-05-23 02:01:47 token_classification_dataset:264] features saved to data/NCBI_ner-3/cached_text_train.txt_BertTokenizer_16_28996_-1\n",
      "[NeMo I 2024-05-23 02:01:47 token_classification_utils:54] Processing data/NCBI_ner-3/labels_dev.txt\n",
      "[NeMo I 2024-05-23 02:01:47 token_classification_utils:74] Using provided labels mapping {'O': 0, 'B': 1, 'I': 2}\n",
      "[NeMo I 2024-05-23 02:01:47 token_classification_utils:96] data/NCBI_ner-3/labels_dev_label_stats.tsv found, skipping stats calculation.\n",
      "[NeMo I 2024-05-23 02:01:49 token_classification_dataset:116] Setting Max Seq length to: 16\n",
      "[NeMo I 2024-05-23 02:01:49 data_preprocessing:299] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2024-05-23 02:01:49 data_preprocessing:301] Min: 4 |                  Max: 122 |                  Mean: 36.812567713976165 |                  Median: 34.0\n",
      "[NeMo I 2024-05-23 02:01:49 data_preprocessing:307] 75 percentile: 47.00\n",
      "[NeMo I 2024-05-23 02:01:49 data_preprocessing:308] 99 percentile: 83.56\n",
      "[NeMo W 2024-05-23 02:01:49 token_classification_dataset:145] 854 are longer than 16\n",
      "[NeMo I 2024-05-23 02:01:49 token_classification_dataset:148] *** Example ***\n",
      "[NeMo I 2024-05-23 02:01:49 token_classification_dataset:149] i: 0\n",
      "[NeMo I 2024-05-23 02:01:49 token_classification_dataset:150] subtokens: [CLS] BR ##CA ##1 is secret ##ed and exhibits properties of a g ##rani ##n . [SEP]\n",
      "[NeMo I 2024-05-23 02:01:49 token_classification_dataset:151] loss_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2024-05-23 02:01:49 token_classification_dataset:152] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2024-05-23 02:01:49 token_classification_dataset:153] subtokens_mask: 0 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0\n",
      "[NeMo I 2024-05-23 02:01:49 token_classification_dataset:155] labels: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2024-05-23 02:01:49 token_classification_dataset:264] features saved to data/NCBI_ner-3/cached_text_dev.txt_BertTokenizer_16_28996_-1\n",
      "[NeMo I 2024-05-23 02:01:49 token_classification_utils:54] Processing data/NCBI_ner-3/labels_dev.txt\n",
      "[NeMo I 2024-05-23 02:01:49 token_classification_utils:74] Using provided labels mapping {'O': 0, 'B': 1, 'I': 2}\n",
      "[NeMo I 2024-05-23 02:01:49 token_classification_utils:96] data/NCBI_ner-3/labels_dev_label_stats.tsv found, skipping stats calculation.\n",
      "[NeMo I 2024-05-23 02:01:49 token_classification_dataset:272] features restored from data/NCBI_ner-3/cached_text_dev.txt_BertTokenizer_16_28996_-1\n",
      "[NeMo W 2024-05-23 02:01:49 modelPT:197] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact forit has already been registered.\n",
      "Downloading: 100%|███████████████████████████| 436M/436M [01:15<00:00, 5.80MB/s]\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertEncoder: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[NeMo I 2024-05-23 02:03:07 modelPT:748] Optimizer config = Adam (\n",
      "    Parameter Group 0\n",
      "        amsgrad: False\n",
      "        betas: (0.9, 0.999)\n",
      "        eps: 1e-08\n",
      "        lr: 5e-05\n",
      "        weight_decay: 0.0\n",
      "    )\n",
      "[NeMo I 2024-05-23 02:03:07 lr_scheduler:617] Scheduler \"<nemo.core.optim.lr_scheduler.WarmupAnnealing object at 0x7310f37436a0>\" \n",
      "    will be used during training (effective maximum steps = 1017) - \n",
      "    Parameters : \n",
      "    (warmup_steps: null\n",
      "    warmup_ratio: 0.1\n",
      "    last_epoch: -1\n",
      "    max_steps: 1017\n",
      "    )\n",
      "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "INFO:root:Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "\n",
      "  | Name                  | Type                 | Params\n",
      "---------------------------------------------------------------\n",
      "0 | bert_model            | BertEncoder          | 108 M \n",
      "1 | classifier            | TokenClassifier      | 592 K \n",
      "2 | loss                  | CrossEntropyLoss     | 0     \n",
      "3 | classification_report | ClassificationReport | 0     \n",
      "---------------------------------------------------------------\n",
      "108 M     Trainable params\n",
      "0         Non-trainable params\n",
      "108 M     Total params\n",
      "435.613   Total estimated model params size (MB)\n",
      "[NeMo W 2024-05-23 02:03:09 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      warnings.warn(*args, **kwargs)\n",
      "    \n",
      "Validation sanity check:  50%|██████████          | 1/2 [00:00<00:00,  1.49it/s][NeMo I 2024-05-23 02:03:10 token_classification_model:177] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                          0.00       0.00       0.00        312\n",
      "    B (label_id: 1)                                          3.04     100.00       5.90          9\n",
      "    I (label_id: 2)                                          5.26      15.38       7.84         13\n",
      "    -------------------\n",
      "    micro avg                                                3.29       3.29       3.29        334\n",
      "    macro avg                                                2.77      38.46       4.58        334\n",
      "    weighted avg                                             0.29       3.29       0.46        334\n",
      "    \n",
      "[NeMo W 2024-05-23 02:03:10 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      warnings.warn(*args, **kwargs)\n",
      "    \n",
      "Epoch 0:  85%|▊| 339/397 [00:19<00:03, 17.34it/s, loss=0.112, v_num=1-37, val_lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/58 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  86%|▊| 342/397 [00:19<00:03, 17.37it/s, loss=0.112, v_num=1-37, val_lo\u001b[A\n",
      "Epoch 0:  88%|▉| 350/397 [00:19<00:02, 17.68it/s, loss=0.112, v_num=1-37, val_lo\u001b[A\n",
      "Epoch 0:  90%|▉| 358/397 [00:19<00:02, 17.99it/s, loss=0.112, v_num=1-37, val_lo\u001b[A\n",
      "Epoch 0:  92%|▉| 366/397 [00:20<00:01, 18.29it/s, loss=0.112, v_num=1-37, val_lo\u001b[A\n",
      "Epoch 0:  94%|▉| 374/397 [00:20<00:01, 18.59it/s, loss=0.112, v_num=1-37, val_lo\u001b[A\n",
      "Epoch 0:  96%|▉| 382/397 [00:20<00:00, 18.88it/s, loss=0.112, v_num=1-37, val_lo\u001b[A\n",
      "Epoch 0:  98%|▉| 390/397 [00:20<00:00, 19.18it/s, loss=0.112, v_num=1-37, val_lo\u001b[A\n",
      "Validating:  98%|██████████████████████████████▍| 57/58 [00:00<00:00, 72.60it/s]\u001b[A[NeMo I 2024-05-23 02:03:30 token_classification_model:177] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         99.19      98.52      98.85       8718\n",
      "    B (label_id: 1)                                         73.24      85.22      78.78        318\n",
      "    I (label_id: 2)                                         83.86      85.05      84.45        495\n",
      "    -------------------\n",
      "    micro avg                                               97.38      97.38      97.38       9531\n",
      "    macro avg                                               85.43      89.60      87.36       9531\n",
      "    weighted avg                                            97.53      97.38      97.44       9531\n",
      "    \n",
      "Epoch 0: 100%|█| 397/397 [00:20<00:00, 19.34it/s, loss=0.112, v_num=1-37, val_lo\n",
      "                                                                                \u001b[AEpoch 0, global step 338: val_loss reached 0.10953 (best 0.10953), saving model to \"/dli/task/Downloads/nvidia-dli-llm/part_2/nemo_experiments/token_classification_model/2024-05-23_02-01-37/checkpoints/token_classification_model--val_loss=0.11-epoch=0.ckpt\" as top 3\n",
      "Epoch 1:  85%|▊| 339/397 [00:19<00:03, 17.02it/s, loss=0.0656, v_num=1-37, val_l\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/58 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  87%|▊| 344/397 [00:20<00:03, 17.11it/s, loss=0.0656, v_num=1-37, val_l\u001b[A\n",
      "Epoch 1:  89%|▉| 352/397 [00:20<00:02, 17.41it/s, loss=0.0656, v_num=1-37, val_l\u001b[A\n",
      "Epoch 1:  91%|▉| 360/397 [00:20<00:02, 17.71it/s, loss=0.0656, v_num=1-37, val_l\u001b[A\n",
      "Epoch 1:  93%|▉| 368/397 [00:20<00:01, 18.01it/s, loss=0.0656, v_num=1-37, val_l\u001b[A\n",
      "Epoch 1:  95%|▉| 376/397 [00:20<00:01, 18.29it/s, loss=0.0656, v_num=1-37, val_l\u001b[A\n",
      "Epoch 1:  97%|▉| 384/397 [00:20<00:00, 18.58it/s, loss=0.0656, v_num=1-37, val_l\u001b[A\n",
      "Epoch 1:  99%|▉| 392/397 [00:20<00:00, 18.87it/s, loss=0.0656, v_num=1-37, val_l\u001b[A\n",
      "Validating:  98%|██████████████████████████████▍| 57/58 [00:00<00:00, 71.61it/s]\u001b[A[NeMo I 2024-05-23 02:03:56 token_classification_model:177] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         98.85      99.31      99.08       8718\n",
      "    B (label_id: 1)                                         87.38      84.91      86.12        318\n",
      "    I (label_id: 2)                                         89.63      83.84      86.64        495\n",
      "    -------------------\n",
      "    micro avg                                               98.03      98.03      98.03       9531\n",
      "    macro avg                                               91.95      89.35      90.61       9531\n",
      "    weighted avg                                            97.99      98.03      98.00       9531\n",
      "    \n",
      "Epoch 1: 100%|█| 397/397 [00:20<00:00, 18.96it/s, loss=0.0656, v_num=1-37, val_l\n",
      "                                                                                \u001b[AEpoch 1, global step 677: val_loss reached 0.09005 (best 0.09005), saving model to \"/dli/task/Downloads/nvidia-dli-llm/part_2/nemo_experiments/token_classification_model/2024-05-23_02-01-37/checkpoints/token_classification_model--val_loss=0.09-epoch=1.ckpt\" as top 3\n",
      "Epoch 2:  85%|▊| 339/397 [00:18<00:03, 18.78it/s, loss=0.0274, v_num=1-37, val_l\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/58 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  87%|▊| 344/397 [00:18<00:02, 18.87it/s, loss=0.0274, v_num=1-37, val_l\u001b[A\n",
      "Epoch 2:  89%|▉| 352/397 [00:18<00:02, 19.19it/s, loss=0.0274, v_num=1-37, val_l\u001b[A\n",
      "Epoch 2:  91%|▉| 360/397 [00:18<00:01, 19.51it/s, loss=0.0274, v_num=1-37, val_l\u001b[A\n",
      "Epoch 2:  93%|▉| 368/397 [00:18<00:01, 19.82it/s, loss=0.0274, v_num=1-37, val_l\u001b[A\n",
      "Epoch 2:  95%|▉| 376/397 [00:18<00:01, 20.14it/s, loss=0.0274, v_num=1-37, val_l\u001b[A\n",
      "Epoch 2:  97%|▉| 384/397 [00:18<00:00, 20.45it/s, loss=0.0274, v_num=1-37, val_l\u001b[A\n",
      "Epoch 2:  99%|▉| 392/397 [00:18<00:00, 20.76it/s, loss=0.0274, v_num=1-37, val_l\u001b[A\n",
      "Validating:  98%|██████████████████████████████▍| 57/58 [00:00<00:00, 72.86it/s]\u001b[A[NeMo I 2024-05-23 02:04:20 token_classification_model:177] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         99.21      99.21      99.21       8718\n",
      "    B (label_id: 1)                                         85.67      88.36      87.00        318\n",
      "    I (label_id: 2)                                         89.48      87.68      88.57        495\n",
      "    -------------------\n",
      "    micro avg                                               98.25      98.25      98.25       9531\n",
      "    macro avg                                               91.45      91.75      91.59       9531\n",
      "    weighted avg                                            98.25      98.25      98.25       9531\n",
      "    \n",
      "Epoch 2: 100%|█| 397/397 [00:19<00:00, 20.83it/s, loss=0.0274, v_num=1-37, val_l\n",
      "                                                                                \u001b[AEpoch 2, global step 1016: val_loss reached 0.10511 (best 0.09005), saving model to \"/dli/task/Downloads/nvidia-dli-llm/part_2/nemo_experiments/token_classification_model/2024-05-23_02-01-37/checkpoints/token_classification_model--val_loss=0.11-epoch=2.ckpt\" as top 3\n",
      "Saving latest checkpoint...\n",
      "Epoch 2: 100%|█| 397/397 [00:24<00:00, 16.46it/s, loss=0.0274, v_num=1-37, val_l\n",
      "[NeMo W 2024-05-23 02:04:25 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/nemo/core/classes/modelPT.py:308: UserWarning: update_node() is deprecated, use OmegaConf.update(). (Since 2.0)\n",
      "      conf.update_node(conf_path, item.path)\n",
      "    \n",
      "CPU times: user 2.12 s, sys: 654 ms, total: 2.77 s\n",
      "Wall time: 3min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# The training takes about 2 minutes to run\n",
    "   \n",
    "TOKEN_DIR = \"nemo/examples/nlp/token_classification\"\n",
    "\n",
    "# set the values we want to override\n",
    "PRETRAINED_MODEL_NAME = 'bert-base-cased'\n",
    "DATA_DIR = 'data/NCBI_ner-3'\n",
    "MAX_SEQ_LENGTH = 16\n",
    "BATCH_SIZE = 16\n",
    "AMP_LEVEL = 'O1'\n",
    "MAX_EPOCHS = 3\n",
    "\n",
    "# Override the config values in the command line\n",
    "!python $TOKEN_DIR/token_classification_train.py \\\n",
    "        model.language_model.pretrained_model_name=$PRETRAINED_MODEL_NAME \\\n",
    "        model.dataset.data_dir=$DATA_DIR \\\n",
    "        model.dataset.max_seq_length=$MAX_SEQ_LENGTH \\\n",
    "        model.train_ds.batch_size=$BATCH_SIZE \\\n",
    "        model.validation_ds.batch_size=$BATCH_SIZE \\\n",
    "        model.test_ds.batch_size=$BATCH_SIZE \\\n",
    "        trainer.amp_level=$AMP_LEVEL \\\n",
    "        trainer.max_epochs=$MAX_EPOCHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How were the results?  Your log should have included something like:\n",
    "\n",
    "```\n",
    "    label                                                precision    recall       f1           support   \n",
    "    O (label_id: 0)                                         99.34      99.35      99.34      21648\n",
    "    B (label_id: 1)                                         85.86      89.21      87.50        769\n",
    "    I (label_id: 2)                                         91.74      89.00      90.35       1073\n",
    "    -------------------\n",
    "    micro avg                                               98.54      98.54      98.54      23490\n",
    "    macro avg                                               92.31      92.52      92.40      23490\n",
    "    weighted avg                                            98.55      98.54      98.55      23490\n",
    "    \n",
    "Epoch 2: 100%|█| 199/199 [00:15<00:00, 12.45it/s, loss=0.0251, v_num=4-43, val_l\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3.2 Domain-Specific Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try another experiment, this time overriding the `model.language_model.pretrained_model_name` with `biomegatron-bert-345m-cased`.  This is a large model with 345 million parameter.  Therefore, it takes longer to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Using native 16bit precision.\n",
      "[NeMo I 2024-05-23 02:09:48 exp_manager:216] Experiments will be logged at /dli/task/Downloads/nvidia-dli-llm/part_2/nemo_experiments/token_classification_model/2024-05-23_02-09-48\n",
      "[NeMo I 2024-05-23 02:09:48 exp_manager:563] TensorboardLogger has been set up\n",
      "[NeMo I 2024-05-23 02:09:48 token_classification_train:109] Config: pretrained_model: null\n",
      "    trainer:\n",
      "      gpus: 1\n",
      "      num_nodes: 1\n",
      "      max_epochs: 3\n",
      "      max_steps: null\n",
      "      accumulate_grad_batches: 1\n",
      "      gradient_clip_val: 0.0\n",
      "      amp_level: O1\n",
      "      precision: 16\n",
      "      accelerator: ddp\n",
      "      checkpoint_callback: false\n",
      "      logger: false\n",
      "      log_every_n_steps: 1\n",
      "      val_check_interval: 1.0\n",
      "      resume_from_checkpoint: null\n",
      "    exp_manager:\n",
      "      exp_dir: null\n",
      "      name: token_classification_model\n",
      "      create_tensorboard_logger: true\n",
      "      create_checkpoint_callback: false\n",
      "    model:\n",
      "      label_ids: null\n",
      "      class_labels:\n",
      "        class_labels_file: label_ids.csv\n",
      "      dataset:\n",
      "        data_dir: data/NCBI_ner-3\n",
      "        class_balancing: null\n",
      "        max_seq_length: 2\n",
      "        pad_label: O\n",
      "        ignore_extra_tokens: false\n",
      "        ignore_start_end: false\n",
      "        use_cache: true\n",
      "        num_workers: 2\n",
      "        pin_memory: false\n",
      "        drop_last: false\n",
      "      train_ds:\n",
      "        text_file: text_train.txt\n",
      "        labels_file: labels_train.txt\n",
      "        shuffle: true\n",
      "        num_samples: -1\n",
      "        batch_size: 2\n",
      "      validation_ds:\n",
      "        text_file: text_dev.txt\n",
      "        labels_file: labels_dev.txt\n",
      "        shuffle: false\n",
      "        num_samples: -1\n",
      "        batch_size: 2\n",
      "      test_ds:\n",
      "        text_file: text_dev.txt\n",
      "        labels_file: labels_dev.txt\n",
      "        shuffle: false\n",
      "        num_samples: -1\n",
      "        batch_size: 2\n",
      "      tokenizer:\n",
      "        tokenizer_name: ${model.language_model.pretrained_model_name}\n",
      "        vocab_file: null\n",
      "        tokenizer_model: null\n",
      "        special_tokens: null\n",
      "      language_model:\n",
      "        pretrained_model_name: biomegatron-bert-345m-cased\n",
      "        lm_checkpoint: null\n",
      "        config_file: null\n",
      "        config: null\n",
      "      head:\n",
      "        num_fc_layers: 2\n",
      "        fc_dropout: 0.5\n",
      "        activation: relu\n",
      "        use_transformer_init: true\n",
      "      optim:\n",
      "        name: adam\n",
      "        lr: 5.0e-05\n",
      "        weight_decay: 0.0\n",
      "        sched:\n",
      "          name: WarmupAnnealing\n",
      "          warmup_steps: null\n",
      "          warmup_ratio: 0.1\n",
      "          last_epoch: -1\n",
      "          monitor: val_loss\n",
      "          reduce_on_plateau: false\n",
      "    \n",
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "[NeMo I 2024-05-23 02:09:49 token_classification_utils:54] Processing data/NCBI_ner-3/labels_train.txt\n",
      "[NeMo I 2024-05-23 02:09:49 token_classification_utils:90] Labels mapping {'O': 0, 'B': 1, 'I': 2} saved to : data/NCBI_ner-3/label_ids.csv\n",
      "[NeMo I 2024-05-23 02:09:49 token_classification_utils:99] Three most popular labels in data/NCBI_ner-3/labels_train.txt:\n",
      "[NeMo I 2024-05-23 02:09:49 data_preprocessing:135] label: 0, 124452 out of 135701 (91.71%).\n",
      "[NeMo I 2024-05-23 02:09:49 data_preprocessing:135] label: 2, 6115 out of 135701 (4.51%).\n",
      "[NeMo I 2024-05-23 02:09:49 data_preprocessing:135] label: 1, 5134 out of 135701 (3.78%).\n",
      "[NeMo I 2024-05-23 02:09:49 token_classification_utils:101] Total labels: 135701. Label frequencies - {0: 124452, 2: 6115, 1: 5134}\n",
      "[NeMo I 2024-05-23 02:09:49 token_classification_utils:107] Class weights restored from data/NCBI_ner-3/labels_train_weights.p\n",
      "[NeMo W 2024-05-23 02:09:49 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/nemo/core/classes/modelPT.py:243: UserWarning: update_node() is deprecated, use OmegaConf.update(). (Since 2.0)\n",
      "      self.cfg.update_node(config_path, return_path)\n",
      "    \n",
      "[NeMo I 2024-05-23 02:09:57 token_classification_dataset:116] Setting Max Seq length to: 2\n",
      "[NeMo I 2024-05-23 02:09:57 data_preprocessing:299] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2024-05-23 02:09:57 data_preprocessing:301] Min: 4 |                  Max: 178 |                  Mean: 35.938237463126846 |                  Median: 34.0\n",
      "[NeMo I 2024-05-23 02:09:57 data_preprocessing:307] 75 percentile: 45.00\n",
      "[NeMo I 2024-05-23 02:09:57 data_preprocessing:308] 99 percentile: 88.77\n",
      "[NeMo W 2024-05-23 02:09:57 token_classification_dataset:145] 5424 are longer than 2\n",
      "[NeMo I 2024-05-23 02:09:57 token_classification_dataset:148] *** Example ***\n",
      "[NeMo I 2024-05-23 02:09:57 token_classification_dataset:149] i: 0\n",
      "[NeMo I 2024-05-23 02:09:57 token_classification_dataset:150] subtokens: [CLS] I ##dent ##ification of AP ##C ##2 , a ho ##mo ##logue of the ad ##eno ##mat ##ous p ##oly ##po ##sis co ##li t ##umour suppress ##or . [SEP]\n",
      "[NeMo I 2024-05-23 02:09:57 token_classification_dataset:151] loss_mask: 1 1\n",
      "[NeMo I 2024-05-23 02:09:57 token_classification_dataset:152] input_mask: 1 1\n",
      "[NeMo I 2024-05-23 02:09:57 token_classification_dataset:153] subtokens_mask: 0 0\n",
      "[NeMo I 2024-05-23 02:09:57 token_classification_dataset:155] labels: 0 0\n",
      "[NeMo I 2024-05-23 02:09:57 token_classification_dataset:264] features saved to data/NCBI_ner-3/cached_text_train.txt_BertTokenizer_2_28996_-1\n",
      "[NeMo I 2024-05-23 02:09:57 token_classification_utils:54] Processing data/NCBI_ner-3/labels_dev.txt\n",
      "[NeMo I 2024-05-23 02:09:57 token_classification_utils:74] Using provided labels mapping {'O': 0, 'B': 1, 'I': 2}\n",
      "[NeMo I 2024-05-23 02:09:57 token_classification_utils:96] data/NCBI_ner-3/labels_dev_label_stats.tsv found, skipping stats calculation.\n",
      "[NeMo I 2024-05-23 02:09:59 token_classification_dataset:116] Setting Max Seq length to: 2\n",
      "[NeMo I 2024-05-23 02:09:59 data_preprocessing:299] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2024-05-23 02:09:59 data_preprocessing:301] Min: 4 |                  Max: 122 |                  Mean: 36.812567713976165 |                  Median: 34.0\n",
      "[NeMo I 2024-05-23 02:09:59 data_preprocessing:307] 75 percentile: 47.00\n",
      "[NeMo I 2024-05-23 02:09:59 data_preprocessing:308] 99 percentile: 83.56\n",
      "[NeMo W 2024-05-23 02:09:59 token_classification_dataset:145] 923 are longer than 2\n",
      "[NeMo I 2024-05-23 02:09:59 token_classification_dataset:148] *** Example ***\n",
      "[NeMo I 2024-05-23 02:09:59 token_classification_dataset:149] i: 0\n",
      "[NeMo I 2024-05-23 02:09:59 token_classification_dataset:150] subtokens: [CLS] BR ##CA ##1 is secret ##ed and exhibits properties of a g ##rani ##n . [SEP]\n",
      "[NeMo I 2024-05-23 02:09:59 token_classification_dataset:151] loss_mask: 1 1\n",
      "[NeMo I 2024-05-23 02:09:59 token_classification_dataset:152] input_mask: 1 1\n",
      "[NeMo I 2024-05-23 02:09:59 token_classification_dataset:153] subtokens_mask: 0 0\n",
      "[NeMo I 2024-05-23 02:09:59 token_classification_dataset:155] labels: 0 0\n",
      "[NeMo I 2024-05-23 02:09:59 token_classification_dataset:264] features saved to data/NCBI_ner-3/cached_text_dev.txt_BertTokenizer_2_28996_-1\n",
      "[NeMo I 2024-05-23 02:09:59 token_classification_utils:54] Processing data/NCBI_ner-3/labels_dev.txt\n",
      "[NeMo I 2024-05-23 02:09:59 token_classification_utils:74] Using provided labels mapping {'O': 0, 'B': 1, 'I': 2}\n",
      "[NeMo I 2024-05-23 02:09:59 token_classification_utils:96] data/NCBI_ner-3/labels_dev_label_stats.tsv found, skipping stats calculation.\n",
      "[NeMo I 2024-05-23 02:09:59 token_classification_dataset:272] features restored from data/NCBI_ner-3/cached_text_dev.txt_BertTokenizer_2_28996_-1\n",
      "[NeMo W 2024-05-23 02:09:59 modelPT:197] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact forit has already been registered.\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "setting global batch size to 1\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_load ....................................... None\n",
      "  bias_dropout_fusion ............................. False\n",
      "  bias_gelu_fusion ................................ False\n",
      "  block_data_path ................................. None\n",
      "  checkpoint_activations .......................... False\n",
      "  checkpoint_num_layers ........................... 1\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... infer\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... None\n",
      "  DDP_impl ........................................ local\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 100\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... None\n",
      "  faiss_use_gpu ................................... False\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_allreduce .................................. False\n",
      "  fp32_residual_connection ........................ False\n",
      "  global_batch_size ............................... 1\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 1024\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... True\n",
      "  load ............................................ None\n",
      "  local_rank ...................................... None\n",
      "  log_interval .................................... 100\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. None\n",
      "  lr_decay_iters .................................. None\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. linear\n",
      "  lr_warmup_fraction .............................. None\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  max_position_embeddings ......................... 512\n",
      "  merge_file ...................................... None\n",
      "  micro_batch_size ................................ 1\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 0.0\n",
      "  mmap_warmup ..................................... False\n",
      "  no_load_optim ................................... False\n",
      "  no_load_rng ..................................... False\n",
      "  no_save_optim ................................... False\n",
      "  no_save_rng ..................................... False\n",
      "  num_attention_heads ............................. 16\n",
      "  num_layers ...................................... 24\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... True\n",
      "  openai_gelu ..................................... False\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  report_topk_accuracies .......................... []\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  save ............................................ None\n",
      "  save_interval ................................... None\n",
      "  scaled_masked_softmax_fusion .................... False\n",
      "  scaled_upper_triang_masked_softmax_fusion ....... False\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... None\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 969, 30, 1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. BertWordPieceLowerCase\n",
      "  train_iters ..................................... None\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_cpu_initialization .......................... False\n",
      "  use_one_sent_docs ............................... False\n",
      "  vocab_file ...................................... /root/.cache/huggingface/nemo_nlp_tmp/c6c377d258d448da6d9259a4c9660f26/tokenizer.vocab_file\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building BertWordPieceLowerCase tokenizer ...\n",
      " > padded vocab (size: 28996) with 60 dummy tokens (new size: 29056)\n",
      "[NeMo I 2024-05-23 02:09:59 megatron_bert:109] Megatron-lm argparse args: Namespace(DDP_impl='local', adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, adlr_autoresume=False, adlr_autoresume_interval=1000, apply_query_key_layer_scaling=True, apply_residual_connection_post_layernorm=False, attention_dropout=0.1, attention_softmax_in_fp32=False, bert_load=None, bias_dropout_fusion=False, bias_gelu_fusion=False, block_data_path=None, checkpoint_activations=False, checkpoint_num_layers=1, clip_grad=1.0, consumed_train_samples=0, consumed_valid_samples=0, data_impl='infer', data_parallel_size=1, data_path=None, distribute_checkpointed_activations=False, distributed_backend='nccl', eod_mask_loss=False, eval_interval=1000, eval_iters=100, exit_duration_in_mins=None, exit_interval=None, faiss_use_gpu=False, finetune=False, fp16=False, fp16_lm_cross_entropy=False, fp32_allreduce=False, fp32_residual_connection=False, global_batch_size=1, hidden_dropout=0.1, hidden_size=1024, hysteresis=2, ict_head_size=None, ict_load=None, indexer_batch_size=128, indexer_log_interval=1000, init_method_std=0.02, initial_loss_scale=4294967296, layernorm_epsilon=1e-05, lazy_mpu_init=True, load=None, local_rank=None, log_interval=100, loss_scale=None, loss_scale_window=1000, lr=None, lr_decay_iters=None, lr_decay_samples=None, lr_decay_style='linear', lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, make_vocab_size_divisible_by=128, mask_prob=0.15, max_position_embeddings=512, merge_file=None, micro_batch_size=1, min_loss_scale=1.0, min_lr=0.0, mmap_warmup=False, no_load_optim=False, no_load_rng=False, no_save_optim=False, no_save_rng=False, num_attention_heads=16, num_layers=24, num_workers=2, onnx_safe=True, openai_gelu=False, override_lr_scheduler=False, padded_vocab_size=29056, params_dtype=torch.float32, pipeline_model_parallel_size=1, query_in_block_prob=0.1, rampup_batch_size=None, rank=0, report_topk_accuracies=[], reset_attention_mask=False, reset_position_ids=False, save=None, save_interval=None, scaled_masked_softmax_fusion=False, scaled_upper_triang_masked_softmax_fusion=False, seed=1234, seq_length=None, short_seq_prob=0.1, split='969, 30, 1', tensor_model_parallel_size=1, tensorboard_dir=None, titles_data_path=None, tokenizer_type='BertWordPieceLowerCase', train_iters=None, train_samples=None, use_checkpoint_lr_scheduler=False, use_cpu_initialization=True, use_one_sent_docs=False, vocab_file='/root/.cache/huggingface/nemo_nlp_tmp/c6c377d258d448da6d9259a4c9660f26/tokenizer.vocab_file', weight_decay=0.01, world_size=1)\n",
      "[NeMo W 2024-05-23 02:10:01 megatron_bert:185] Megatron-lm checkpoint version not found. Setting checkpoint_version to 0.\n",
      "[NeMo I 2024-05-23 02:10:01 megatron_bert:192] Checkpoint loaded from from /root/.cache/torch/megatron/biomegatron-bert-345m-cased\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[NeMo I 2024-05-23 02:10:01 modelPT:748] Optimizer config = Adam (\n",
      "    Parameter Group 0\n",
      "        amsgrad: False\n",
      "        betas: (0.9, 0.999)\n",
      "        eps: 1e-08\n",
      "        lr: 5e-05\n",
      "        weight_decay: 0.0\n",
      "    )\n",
      "[NeMo I 2024-05-23 02:10:01 lr_scheduler:617] Scheduler \"<nemo.core.optim.lr_scheduler.WarmupAnnealing object at 0x7ee1b8d65b80>\" \n",
      "    will be used during training (effective maximum steps = 8136) - \n",
      "    Parameters : \n",
      "    (warmup_steps: null\n",
      "    warmup_ratio: 0.1\n",
      "    last_epoch: -1\n",
      "    max_steps: 8136\n",
      "    )\n",
      "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "INFO:root:Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "\n",
      "  | Name                  | Type                 | Params\n",
      "---------------------------------------------------------------\n",
      "0 | bert_model            | MegatronBertEncoder  | 332 M \n",
      "1 | classifier            | TokenClassifier      | 1.1 M \n",
      "2 | loss                  | CrossEntropyLoss     | 0     \n",
      "3 | classification_report | ClassificationReport | 0     \n",
      "---------------------------------------------------------------\n",
      "333 M     Trainable params\n",
      "0         Non-trainable params\n",
      "333 M     Total params\n",
      "1,334.575 Total estimated model params size (MB)\n",
      "[NeMo W 2024-05-23 02:10:03 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      warnings.warn(*args, **kwargs)\n",
      "    \n",
      "Validation sanity check:   0%|                            | 0/2 [00:00<?, ?it/s]torch distributed is already initialized, skipping initialization ...\n",
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "INFO:root:Added key: store_based_barrier_key:2 to store for rank: 0\n",
      "INFO:root:Added key: store_based_barrier_key:3 to store for rank: 0\n",
      "INFO:root:Added key: store_based_barrier_key:4 to store for rank: 0\n",
      "INFO:root:Added key: store_based_barrier_key:5 to store for rank: 0\n",
      "INFO:root:Added key: store_based_barrier_key:6 to store for rank: 0\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "Traceback (most recent call last):\n",
      "  File \"nemo/examples/nlp/token_classification/token_classification_train.py\", line 143, in main\n",
      "    trainer.fit(model)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 499, in fit\n",
      "    self.dispatch()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 546, in dispatch\n",
      "    self.accelerator.start_training(self)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 73, in start_training\n",
      "    self.training_type_plugin.start_training(trainer)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/nemo/collections/nlp/parts/nlp_overrides.py\", line 108, in start_training\n",
      "    return super().start_training(trainer)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 114, in start_training\n",
      "    self._results = trainer.run_train()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 607, in run_train\n",
      "    self.run_sanity_check(self.lightning_module)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 864, in run_sanity_check\n",
      "    _, eval_results = self.run_evaluation(max_batches=self.num_sanity_val_batches)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 726, in run_evaluation\n",
      "    output = self.evaluation_loop.evaluation_step(batch, batch_idx, dataloader_idx)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 166, in evaluation_step\n",
      "    output = self.trainer.accelerator.validation_step(args)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 177, in validation_step\n",
      "    return self.training_type_plugin.validation_step(*args)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp.py\", line 315, in validation_step\n",
      "    return self.model(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py\", line 707, in forward\n",
      "    output = self.module(*inputs[0], **kwargs[0])\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/overrides/base.py\", line 63, in forward\n",
      "    output = self.module.validation_step(*inputs, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/nemo/collections/nlp/models/token_classification/token_classification_model.py\", line 156, in validation_step\n",
      "    logits = self(input_ids=input_ids, token_type_ids=input_type_ids, attention_mask=input_mask)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/nemo/core/classes/common.py\", line 791, in __call__\n",
      "    outputs = wrapped(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/nemo/collections/nlp/models/token_classification/token_classification_model.py\", line 126, in forward\n",
      "    hidden_states = self.bert_model(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/nemo/core/classes/common.py\", line 791, in __call__\n",
      "    outputs = wrapped(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/nemo/collections/nlp/modules/common/megatron/megatron_bert.py\", line 163, in forward\n",
      "    sequence_output = self.language_model(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/megatron/model/language_model.py\", line 414, in forward\n",
      "    return super(TransformerLanguageModel, self).forward(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/megatron/model/language_model.py\", line 328, in forward\n",
      "    transformer_output = self.transformer(transformer_input,\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/megatron/model/transformer.py\", line 589, in forward\n",
      "    hidden_states = layer(hidden_states,\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/megatron/model/transformer.py\", line 474, in forward\n",
      "    mlp_output, mlp_bias = self.mlp(layernorm_output)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/megatron/model/transformer.py\", line 98, in forward\n",
      "    intermediate_parallel, bias_parallel = self.dense_h_to_4h(hidden_states)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/megatron/mpu/layers.py\", line 280, in forward\n",
      "    output_parallel = F.linear(input_parallel, self.weight, bias)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py\", line 1753, in linear\n",
      "    return torch._C._nn.linear(input, weight, bias)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 9.53 GiB total capacity; 3.03 GiB already allocated; 96.25 MiB free; 3.08 GiB reserved in total by PyTorch)\n",
      "\n",
      "Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n",
      "CPU times: user 172 ms, sys: 53.9 ms, total: 226 ms                             \n",
      "Wall time: 20.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# The training takes about 5-6 minutes to run\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:6000\"\n",
    "\n",
    "TOKEN_DIR = \"nemo/examples/nlp/token_classification\"\n",
    "\n",
    "# set the values we want to override\n",
    "PRETRAINED_MODEL_NAME = 'biomegatron-bert-345m-cased'\n",
    "DATA_DIR = 'data/NCBI_ner-3'\n",
    "MAX_SEQ_LENGTH = 2\n",
    "BATCH_SIZE = 2\n",
    "AMP_LEVEL = 'O1'\n",
    "MAX_EPOCHS = 3\n",
    "\n",
    "# Override the config values in the command line\n",
    "!python $TOKEN_DIR/token_classification_train.py \\\n",
    "        model.language_model.pretrained_model_name=$PRETRAINED_MODEL_NAME \\\n",
    "        model.dataset.data_dir=$DATA_DIR \\\n",
    "        model.dataset.max_seq_length=$MAX_SEQ_LENGTH \\\n",
    "        model.train_ds.batch_size=$BATCH_SIZE \\\n",
    "        model.validation_ds.batch_size=$BATCH_SIZE \\\n",
    "        model.test_ds.batch_size=$BATCH_SIZE \\\n",
    "        trainer.amp_level=$AMP_LEVEL \\\n",
    "        trainer.max_epochs=$MAX_EPOCHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.1 Visualize the Results with TensorBoard\n",
    "The [experiment manager](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/core/core.html?highlight=tensorboard#experiment-manager) saves results for viewing with TensorBoard. <br>\n",
    "Take a look by opening [TensorBoard](/tensorboard/) for your instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the performance of the models you've run, select the \"f1\" scaler.  You can see all the models compared together or select individual models for comparison.  In this example comparison, five epochs were run.  The orange line shows results from the `bert-base-cased` model and the blue line is the `biomegatron-bert-345m-cased` model.  The BioMegatron model does quite well very quickly, as it is better able to discern the disease names. It still has a slightly higher f1 after five epochs. The model you choose for your own project depends on your constraints in memory, time, and performance requirements.  Note that your results may vary from the example due to randomness in the learning algorithm.\n",
    "\n",
    "<img src=\"images/tensorboard_02.png\" width=800px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3.3 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Restart the kernel\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model over the test set, we must specify the location of the `.nemo` trained model. Each experiment runs results in a time-stamped directory under `nemo_experiments`.  If we drill down, we can find the `checkpoints` folder where the final `token_classification_model.nemo` resides. In the next cell, a bit of Python logic is used to capture a list of models, and identify the latest one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The latest model is \n",
      "nemo_experiments/token_classification_model/2024-05-23_02-01-37/checkpoints/token_classification_model.nemo\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "nemo_model_paths = glob.glob('nemo_experiments/token_classification_model/*/checkpoints/*.nemo')\n",
    "\n",
    "# Sort newest first\n",
    "nemo_model_paths.sort(reverse=True)\n",
    "print(\"The latest model is \\n{}\".format(nemo_model_paths[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of ways to run an evaluation over the test set:\n",
    "1. Execute `token_classification_evaluate.py` with the same overrides, plus an override for the `pretrained_model`, which must be in `.nemo` format.\n",
    "\n",
    "```text\n",
    "   !python $TOKEN_DIR/token_classification_evaluate.py \\\n",
    "        model.dataset.data_dir=$DATA_DIR \\\n",
    "        model.dataset.max_seq_length=$MAX_SEQ_LENGTH \\\n",
    "        model.train_ds.batch_size=$BATCH_SIZE \\\n",
    "        model.validation_ds.batch_size=$BATCH_SIZE \\\n",
    "        model.test_ds.batch_size=$BATCH_SIZE \\\n",
    "        model.language_model.pretrained_model_name=$PRETRAINED_MODEL_NAME \\\n",
    "        pretrained_model=$LATEST_MODEL\n",
    "```\n",
    "        \n",
    "2. Instantiate the model by restoring the trained model checkpoint and execute a NeMo method to evaluate the test set.<br>\n",
    "   This is the method we will step through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "[NeMo W 2024-05-23 02:11:29 modelPT:137] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    text_file: text_train.txt\n",
      "    labels_file: labels_train.txt\n",
      "    shuffle: true\n",
      "    num_samples: -1\n",
      "    batch_size: 16\n",
      "    \n",
      "[NeMo W 2024-05-23 02:11:29 modelPT:144] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    text_file: text_dev.txt\n",
      "    labels_file: labels_dev.txt\n",
      "    shuffle: false\n",
      "    num_samples: -1\n",
      "    batch_size: 16\n",
      "    \n",
      "[NeMo W 2024-05-23 02:11:29 modelPT:151] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    text_file: text_dev.txt\n",
      "    labels_file: labels_dev.txt\n",
      "    shuffle: false\n",
      "    num_samples: -1\n",
      "    batch_size: 16\n",
      "    \n",
      "[NeMo W 2024-05-23 02:11:29 modelPT:1198] World size can only be set by PyTorch Lightning Trainer.\n",
      "[NeMo W 2024-05-23 02:11:29 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/nemo/core/classes/modelPT.py:243: UserWarning: update_node() is deprecated, use OmegaConf.update(). (Since 2.0)\n",
      "      self.cfg.update_node(config_path, return_path)\n",
      "    \n",
      "[NeMo W 2024-05-23 02:11:29 modelPT:197] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact forit has already been registered.\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-23 02:11:32 modelPT:434] Model TokenClassificationModel was successfully restored from nemo_experiments/token_classification_model/2024-05-23_02-01-37/checkpoints/token_classification_model.nemo.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model by restoring from the .nemo checkpoint\n",
    "from nemo.collections import nlp as nemo_nlp\n",
    "\n",
    "LATEST_MODEL = nemo_model_paths[0]\n",
    "model = nemo_nlp.models.TokenClassificationModel.restore_from(LATEST_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model with the test set using the `evaluate_from_file` method.  Set the `add_confusion_matrix` to True to get a nice visual representation of how well the model did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-23 02:11:40 token_classification_dataset:116] Setting Max Seq length to: 157\n",
      "[NeMo I 2024-05-23 02:11:40 data_preprocessing:299] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2024-05-23 02:11:40 data_preprocessing:301] Min: 4 |                  Max: 157 |                  Mean: 37.204255319148935 |                  Median: 35.0\n",
      "[NeMo I 2024-05-23 02:11:40 data_preprocessing:307] 75 percentile: 46.00\n",
      "[NeMo I 2024-05-23 02:11:40 data_preprocessing:308] 99 percentile: 94.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-05-23 02:11:40 token_classification_dataset:145] 0 are longer than 157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-23 02:11:40 token_classification_dataset:148] *** Example ***\n",
      "[NeMo I 2024-05-23 02:11:40 token_classification_dataset:149] i: 0\n",
      "[NeMo I 2024-05-23 02:11:40 token_classification_dataset:150] subtokens: [CLS] C ##luster ##ing of miss ##ense mutations in the at ##ax ##ia - te ##lang ##ie ##ct ##asi ##a gene in a s ##poradic T - cell le ##uka ##emia . [SEP]\n",
      "[NeMo I 2024-05-23 02:11:40 token_classification_dataset:151] loss_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2024-05-23 02:11:40 token_classification_dataset:152] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2024-05-23 02:11:40 token_classification_dataset:153] subtokens_mask: 0 1 0 0 1 1 0 1 1 1 1 0 0 1 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2024-05-23 02:11:48 token_classification_model:482] Labels save to /dli/task/Downloads/nvidia-dli-llm/part_2/nemo_experiments/token_classification_model/logs/infer_text_test.txt\n",
      "[NeMo I 2024-05-23 02:11:48 token_classification_model:488] Predictions saved to /dli/task/Downloads/nvidia-dli-llm/part_2/nemo_experiments/token_classification_model/logs/infer_text_test.txt\n",
      "[NeMo I 2024-05-23 02:11:49 utils_funcs:94] Confusion matrix saved to /dli/task/Downloads/nvidia-dli-llm/part_2/nemo_experiments/token_classification_model/logs/Normalized_Confusion_matrix_20240523-021149\n",
      "[NeMo I 2024-05-23 02:11:49 token_classification_model:499]                  precision    recall  f1-score   support\n",
      "    \n",
      "    O (label id: 0)     0.9924    0.9887    0.9905     22450\n",
      "    B (label id: 1)     0.8332    0.8896    0.8605       960\n",
      "    I (label id: 2)     0.8650    0.8786    0.8717      1087\n",
      "    \n",
      "           accuracy                         0.9800     24497\n",
      "          macro avg     0.8969    0.9190    0.9076     24497\n",
      "       weighted avg     0.9805    0.9800    0.9802     24497\n",
      "    \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os.path\n",
    "\n",
    "DATA_DIR = 'data/NCBI_ner-3'\n",
    "OUTPUT_DIR = 'nemo_experiments/token_classification_model/logs'\n",
    "model.evaluate_from_file(\n",
    "    text_file=os.path.join(DATA_DIR, 'text_test.txt'),\n",
    "    labels_file=os.path.join(DATA_DIR, 'labels_test.txt'),\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    add_confusion_matrix=True,\n",
    "    normalize_confusion_matrix=True,\n",
    "    batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results should look something like:\n",
    "\n",
    "```\n",
    "[NeMo I 2021-06-29 00:42:16 token_classification_model:499]                  precision    recall  f1-score   support\n",
    "    \n",
    "    O (label id: 0)     0.9958    0.9910    0.9934     22450\n",
    "    B (label id: 1)     0.8886    0.9135    0.9009       960\n",
    "    I (label id: 2)     0.8724    0.9374    0.9038      1087\n",
    "    \n",
    "           accuracy                         0.9856     24497\n",
    "          macro avg     0.9189    0.9473    0.9327     24497\n",
    "       weighted avg     0.9861    0.9856    0.9858     24497\n",
    "\n",
    "\n",
    "The final confusion matrix visualization shows a bright diagonal, indicating that the predicted label matched the true label with high accuracy for all the label types (IOB).\n",
    "```\n",
    "\n",
    "<img src=\"images/ner_confusion_matrix.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3.4 Inference\n",
    "To run inference on a list of queries, use the same model already loaded with the `add_predictions` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\"Clustering of missense mutations in the ataxia - telangiectasia gene in a sporadic T - cell leukaemia . \",\n",
    "    \"Ataxia - telangiectasia ( A - T ) is a recessive multi - system disorder caused by mutations in the ATM gene at 11q22 - q23 ( ref . 3 ) . \",\n",
    "    \"The risk of cancer , especially lymphoid neoplasias , is substantially elevated in A - T patients and has long been associated with chromosomal instability . \",\n",
    "    \"By analysing tumour DNA from patients with sporadic T - cell prolymphocytic leukaemia ( T - PLL ) , a rare clonal malignancy with similarities to a mature T - cell leukaemia seen in A - T , we demonstrate a high frequency of ATM mutations in T - PLL . \",\n",
    "    \"In marked contrast to the ATM mutation pattern in A - T , the most frequent nucleotide changes in this leukaemia were missense mutations . \",\n",
    "    \"These clustered in the region corresponding to the kinase domain , which is highly conserved in ATM - related proteins in mouse , yeast and Drosophila . \",\n",
    "    \"The resulting amino - acid substitutions are predicted to interfere with ATP binding or substrate recognition . \",\n",
    "    \"Two of seventeen mutated T - PLL samples had a previously reported A - T allele . \",\n",
    "    \"In contrast , no mutations were detected in the p53 gene , suggesting that this tumour suppressor is not frequently altered in this leukaemia . \",\n",
    "    \"Occasional missense mutations in ATM were also found in tumour DNA from patients with B - cell non - Hodgkins lymphomas ( B - NHL ) and a B - NHL cell line . \"\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-23 02:11:59 token_classification_dataset:116] Setting Max Seq length to: 74\n",
      "[NeMo I 2024-05-23 02:11:59 data_preprocessing:299] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2024-05-23 02:11:59 data_preprocessing:301] Min: 20 |                  Max: 74 |                  Mean: 38.1 |                  Median: 34.0\n",
      "[NeMo I 2024-05-23 02:11:59 data_preprocessing:307] 75 percentile: 44.25\n",
      "[NeMo I 2024-05-23 02:11:59 data_preprocessing:308] 99 percentile: 71.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-05-23 02:11:59 token_classification_dataset:145] 0 are longer than 74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-23 02:11:59 token_classification_dataset:148] *** Example ***\n",
      "[NeMo I 2024-05-23 02:11:59 token_classification_dataset:149] i: 0\n",
      "[NeMo I 2024-05-23 02:11:59 token_classification_dataset:150] subtokens: [CLS] C ##luster ##ing of miss ##ense mutations in the at ##ax ##ia - te ##lang ##ie ##ct ##asi ##a gene in a s ##poradic T - cell le ##uka ##emia . [SEP]\n",
      "[NeMo I 2024-05-23 02:11:59 token_classification_dataset:151] loss_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2024-05-23 02:11:59 token_classification_dataset:152] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2024-05-23 02:11:59 token_classification_dataset:153] subtokens_mask: 0 1 0 0 1 1 0 1 1 1 1 0 0 1 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2024-05-23 02:11:59 token_classification_model:436] Predictions saved to predictions.txt\n"
     ]
    }
   ],
   "source": [
    "results = model.add_predictions(queries, output_file='predictions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering of missense mutations in the ataxia[B] [I]- telangiectasia[I] gene in a sporadic T[B] [I]- cell[I] leukaemia[I] .\n",
      "Ataxia[B] [I]- telangiectasia[I] ( A[B] [I]- T[I] ) is a recessive[B] multi[I] [I]- system[I] disorder[I] caused by mutations in the ATM gene at 11q22 - q23 ( ref . 3 ) .\n",
      "The risk of cancer[B] , especially lymphoid[B] neoplasias[I] , is substantially elevated in A[B] [I]- T[I] patients and has long been associated with chromosomal instability .\n",
      "By analysing tumour[B] DNA from patients with sporadic T[B] [I]- cell[I] prolymphocytic[I] leukaemia[I] ( T[B] [I]- PLL[I] ) , a rare clonal[B] malignancy[I] with similarities to a mature T[B] [I]- cell[I] leukaemia[I] seen in A[B] [I]- T[I] , we demonstrate a high frequency of ATM mutations in T[B] [I]- PLL[I] .\n",
      "In marked contrast to the ATM mutation pattern in A[B] [I]- T[I] , the most frequent nucleotide changes in this leukaemia[B] were missense mutations .\n",
      "These clustered in the region corresponding to the kinase domain , which is highly conserved in ATM - related proteins in mouse , yeast and Drosophila .\n",
      "The resulting amino - acid substitutions are predicted to interfere with ATP binding or substrate recognition .\n",
      "Two of seventeen mutated T - PLL samples had a previously reported A[B] [I]- T[I] allele .\n",
      "In contrast , no mutations were detected in the p53 gene , suggesting that this tumour[B] suppressor is not frequently altered in this leukaemia[B] .\n",
      "Occasional missense mutations in ATM were also found in tumour[B] DNA from patients with B[B] [I]- cell[I] non[I] [I]- Hodgkins[I] lymphomas[I] ( B[B] - NHL ) and a B - NHL cell line .\n"
     ]
    }
   ],
   "source": [
    "!cat predictions.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "You've mastered NeMo and learned:\n",
    "* How to build a named entity recognizer\n",
    "* How to apply a domain-specific model\n",
    "* How to test an NER model with queries from a checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
