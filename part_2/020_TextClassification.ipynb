{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Build a Text Classifier\n",
    "\n",
    "<img style=\"float: right;\" src=\"images/nemo/nemo-app-stack.png\" width=400>\n",
    "\n",
    "In this notebook, you'll build an application to classify medical disease abstracts into one of three categories: cancer diseases, neurological diseases and disorders, and \"other\" for anything else.\n",
    "You'll use [NVIDIA NeMo](https://developer.nvidia.com/nvidia-nemo) (Neural Modules) to quickly set up the problem from the command line. \n",
    "\n",
    "**[2.1 NeMo Overview](#2.1-NeMo-Overview)**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.1.1 NeMo Models](#2.1.1-NeMo-Models)<br>\n",
    "**[2.2 Text Classification from the Command Line](#2.2-Text-Classification-from-the-Command-Line)**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.2.1 Prepare the Data](#2.2.1-Prepare-the-Data)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.2.2 Configuration File](#2.2.2-Configuration-File)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2.2.2.1 OmegaConf Tool](#2.2.2.1-OmegaConf-Tool)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.2.3 Hydra-Enabled Python Script](#2.2.3-Hydra-Enabled-Python-Script)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.2.4 Exercise: Run an Experiment](#2.2.4-Exercise:-Run-an-Experiment)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.2.5 Visualize the Results with TensorBoard](#2.2.5-Visualize-the-Results-with-TensorBoard)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.2.6 Exercise: Change the Language Model](#2.2.6-Exercise:-Change-the-Language-Model)<br>\n",
    "**[2.3 PyTorch Lightning Model and Trainer Workflow](#2.3-PyTorch-Lightning-Model-and-Trainer-Workflow)**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.3.1 Script Key Features](#2.3.1-Script-Key-Features)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.3.2 Model Training from Scratch](#2.3.2-Model-Training-from-Scratch)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.3.3 Exercise: Query the Model](#2.3.3-Exercise:-Query-the-Model)<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2.1 NeMo Overview\n",
    "NeMo is an open source toolkit for building conversational AI applications. NeMo is built around [Neural Modules](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/core/core.html#neural-modules), conceptual blocks of neural networks that take typed inputs and produce typed outputs. Such modules typically represent data layers, encoders, decoders, language models, loss functions, or methods of combining activations. NeMo makes it easy to combine and re-use these building blocks while providing a level of semantic correctness checking via its neural type system.\n",
    "\n",
    "The NeMo deep learning framework is based on [PyTorch Lightning](https://github.com/PyTorchLightning/pytorch-lightning), a PyTorch wrapper that organizes PyTorch code for neural network training.  PyTorch Lightning provides easy and high-performant multi-GPU/multi-node mixed precision training options. Creating a deep neural network project, or **experiment**, with PyTorch Lightning requires two main components:\n",
    "1. [LightningModule](https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html)\n",
    "2. [Trainer](https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html)\n",
    "\n",
    "The _LightningModule_ is used to organize PyTorch code into computation, optimizers, and loops for training, validation, and test.  This abstraction makes deep learning experiments easier to understand and reproduce. \n",
    "\n",
    "The _Trainer_ is then able to take the LightningModule and automate everything needed for deep learning training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.1 NeMo Models\n",
    "\n",
    "[NeMo models](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/core/core.html) are LightningModules that come equipped with all supporting infrastructure for training and reproducibility. This includes the deep learning model architecture, data preprocessing, optimizer, checkpoints, and experiment logging. NeMo models, like LightningModules, are also PyTorch modules and are fully compatible with the broader PyTorch ecosystem. Any NeMo model can be taken and plugged into any PyTorch workflow.  \n",
    "\n",
    "**Every NeMo model has an example configuration file and training script that can be found in the [NVIDIA NeMo GitHub Repo](https://github.com/NVIDIA/NeMo/tree/main/examples).**\n",
    "\n",
    "For this class, we'll use a local repo copy included in this environment, based on the downloadable [NGC NeMo container](https://ngc.nvidia.com/catalog/containers/nvidia:nemo), and focus on NLP models.  Execute the following cell to see a tree of NeMo models in the `examples/nlp` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mnemo/examples/nlp\u001b[00m\n",
      "├── \u001b[01;34mtext_classification\u001b[00m\n",
      "│   ├── \u001b[01;34mconf\u001b[00m\n",
      "│   ├── \u001b[01;34mdata\u001b[00m\n",
      "│   ├── model_parallel_text_classification_evaluation.py\n",
      "│   └── text_classification_with_bert.py\n",
      "└── \u001b[01;34mtoken_classification\u001b[00m\n",
      "    ├── \u001b[01;34mdata\u001b[00m\n",
      "    ├── punctuation_capitalization_evaluate.py\n",
      "    ├── punctuation_capitalization_train.py\n",
      "    ├── token_classification_evaluate.py\n",
      "    └── token_classification_train.py\n",
      "\n",
      "5 directories, 6 files\n"
     ]
    }
   ],
   "source": [
    "!tree nemo/examples/nlp -L 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of models listed covering several classic NLP tasks.  We will focus on [text classification](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/text_classification.html) in this notebook and [token classification](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/token_classification.html) in the next notebook on named entity recognition (NER). \n",
    "\n",
    "Notice that each NeMo model type includes a `conf` folder for configuration files and at least one Python training script file.  \n",
    "\n",
    "Execute the following cell to see more detail for the text classification model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mnemo/examples/nlp/text_classification\u001b[00m\n",
      "├── \u001b[01;34mconf\u001b[00m\n",
      "│   └── text_classification_config.yaml\n",
      "├── \u001b[01;34mdata\u001b[00m\n",
      "│   ├── \u001b[01;34mTextClassification\u001b[00m\n",
      "│   └── import_datasets.py\n",
      "├── model_parallel_text_classification_evaluation.py\n",
      "└── text_classification_with_bert.py\n",
      "\n",
      "3 directories, 4 files\n"
     ]
    }
   ],
   "source": [
    "TC_DIR = \"nemo/examples/nlp/text_classification\"\n",
    "!tree $TC_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The config file, `text_classification_config.yaml`, specifies model, training, and experiment management details, such as file locations, pretrained models, and hyperparameters.\n",
    "\n",
    "The Python script, `text_classification_with_bert.py`, encapsulates everything you need to run a text classification experiment defined by the configuration file.  It employs Facebook's [Hydra](https://hydra.cc/) tool for configuration management, which allows you to run the entire experiment just with the script, using command line options to override the config values!\n",
    "\n",
    "The key to building an experiment quickly, is to  understand what the default config file includes, and what needs to be changed for your own project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2.2 Text Classification from the Command Line\n",
    "The question we want to answer is: \n",
    "\n",
    "**Given a medical disease abstract, is the abstract about cancer, a neurological disorder, or something else?**\n",
    "\n",
    "This is a 3-class text classification problem.  We'll use the NeMo [text classification model](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/text_classification.html) with three classes (labels): \"cancer\" (0), \"neurological disorders\" (1), and \"other\" (2).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.1 Prepare the Data\n",
    "You've already explored the [NCBI-disease corpus](https://www.ncbi.nlm.nih.gov/CBBresearch/Dogan/DISEASE/) and the text classification dataset derived from it in the [Explore the Data](010_ExploreData.ipynb) notebook.  Recall that the text classification files consist of tab-delimited abstracts and labels, with a row of headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/NCBI_tc-3/dev.tsv\tdata/NCBI_tc-3/test.tsv  data/NCBI_tc-3/train.tsv\n"
     ]
    }
   ],
   "source": [
    "TC3_DATA_DIR = 'data/NCBI_tc-3'\n",
    "!ls $TC3_DATA_DIR/*.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****\n",
      "train.tsv sample\n",
      "*****\n",
      "sentence\tlabel\n",
      "Identification of APC2, a homologue of the adenomatous polyposis coli tumour suppressor . The adenomatous polyposis coli ( APC ) tumour-suppressor protein controls the Wnt signalling pathway by forming a complex with glycogen synthase kinase 3beta ( GSK-3beta ) , axin / conductin and betacatenin . Complex formation induces the rapid degradation of betacatenin . In colon carcinoma cells , loss of APC leads to the accumulation of betacatenin in the nucleus , where it binds to and activates the Tcf-4 transcription factor ( reviewed in [ 1 ] [ 2 ] ) . Here , we report the identification and genomic structure of APC homologues . Mammalian APC2 , which closely resembles APC in overall domain structure , was functionally analyzed and shown to contain two SAMP domains , both of which are required for binding to conductin . Like APC , APC2 regulates the formation of active betacatenin-Tcf complexes , as demonstrated using transient transcriptional activation assays in APC - / - colon carcinoma cells . Human APC2 maps to chromosome 19p13 . 3 . APC and APC2 may therefore have comparable functions in development and cancer .\t0\n",
      "A common MSH2 mutation in English and North American HNPCC families: origin, phenotypic expression, and sex specific differences in colorectal cancer . The frequency , origin , and phenotypic expression of a germline MSH2 gene mutation previously identified in seven kindreds with hereditary non-polyposis cancer syndrome (HNPCC) was investigated . The mutation ( A-- > T at nt943 + 3 ) disrupts the 3 splice site of exon 5 leading to the deletion of this exon from MSH2 mRNA and represents the only frequent MSH2 mutation so far reported . Although this mutation was initially detected in four of 33 colorectal cancer families analysed from eastern England , more extensive analysis has reduced the frequency to four of 52 ( 8 % ) English HNPCC kindreds analysed . In contrast , the MSH2 mutation was identified in 10 of 20 ( 50 % ) separately identified colorectal families from Newfoundland . To investigate the origin of this mutation in colorectal cancer families from England ( n = 4 ) , Newfoundland ( n = 10 ) , and the United States ( n = 3 ) , haplotype analysis using microsatellite markers linked to MSH2 was performed . Within the English and US families there was little evidence for a recent common origin of the MSH2 splice site mutation in most families . In contrast , a common haplotype was identified at the two flanking markers ( CA5 and D2S288 ) in eight of the Newfoundland families . These findings suggested a founder effect within Newfoundland similar to that reported by others for two MLH1 mutations in Finnish HNPCC families . We calculated age related risks of all , colorectal , endometrial , and ovarian cancers in nt943 + 3 A-- > T MSH2 mutation carriers ( n = 76 ) for all patients and for men and women separately . For both sexes combined , the penetrances at age 60 years for all cancers  and for colorectal cancer were 0 . 86 and 0 . 57 , respectively . The risk of colorectal cancer was significantly higher ( p < 0 . 01 ) in males than females ( 0 . 63 v 0 . 30 and 0 . 84 v 0 . 44 at ages 50 and 60 years , respectively ) . For females there was a high risk of endometrial cancer ( 0 . 5 at age 60 years ) and premenopausal ovarian cancer ( 0 . 2 at 50 years ) . These intersex differences in colorectal cancer risks have implications for screening programmes and for attempts to identify colorectal cancer susceptibility modifiers .\t0\n",
      "\n",
      "\n",
      "*****\n",
      "dev.tsv sample\n",
      "*****\n",
      "sentence\tlabel\n",
      "BRCA1 is secreted and exhibits properties of a granin. Germline mutations in BRCA1 are responsible for most cases of inherited breast and ovarian cancer . However , the function of the BRCA1 protein has remained elusive . We now show that BRCA1 encodes a 190-kD protein with sequence homology and biochemical analogy to the granin protein family . Interestingly , BRCA2 also includes a motif similar to the granin consensus at the C terminus of the protein . Both BRCA1 and the granins localize to secretory vesicles , are secreted by a regulated pathway , are post-translationally glycosylated and are responsive to hormones . As a regulated secretory protein , BRCA1 appears to function by a mechanism not previously described for tumour suppressor gene products . . \t0\n",
      "Ovarian cancer risk in BRCA1 carriers is modified by the HRAS1 variable number of tandem repeat (VNTR) locus. Women who carry a mutation in the BRCA1 gene ( on chromosome 17q21 ) , have an 80 % risk of breast cancer and a 40 % risk of ovarian cancer by the age of 70 ( ref . 1 ) . The variable penetrance of BRCA1 suggests that other genetic and non-genetic factors play a role in tumourigenesis in these individuals . The HRAS1 variable number of tandem repeats ( VNTR ) polymorphism , located 1 kilobase ( kb ) downstream of the HRAS1 proto-oncogene ( chromosome 11p15 . 5 ) is one possible genetic modifier of cancer penetrance . Individuals who have rare alleles of the VNTR have an increased risk of certain types of cancers , including breast cancer ( 2-4 ) . To investigate whether the presence of rare HRAS1 alleles increases susceptibility to hereditary breast and ovarian cancer , we have typed a panel of 307 female BRCA1 carriers at this locus using a PCR-based technique . The risk for ovarian cancer was 2 . 11 times greater for BRCA1 carriers harbouring one or two rare HRAS1 alleles , compared to carriers with only common alleles ( P = 0 . 015 ) . The magnitude of the relative risk associated with a rare HRAS1 allele was not altered by adjusting for the other known risk factors for hereditary ovarian cancer ( 5 ) . Susceptibility to breast cancer did not appear to be affected by the presence of rare HRAS1 alleles . This study is the first to show the effect of a modifying gene on the penetrance of an inherited cancer syndrome \t0\n",
      "\n",
      "\n",
      "*****\n",
      "test.tsv sample\n",
      "*****\n",
      "sentence\tlabel\n",
      "Clustering of missense mutations in the ataxia-telangiectasia gene in a sporadic T-cell leukaemia. Ataxia-telangiectasia ( A-T ) is a recessive multi-system disorder caused by mutations in the ATM gene at 11q22-q23 ( ref . 3 ) . The risk of cancer , especially lymphoid neoplasias , is substantially elevated in A-T patients and has long been associated with chromosomal instability . By analysing tumour DNA from patients with sporadic T-cell prolymphocytic leukaemia ( T-PLL ) , a rare clonal malignancy with similarities to a mature T-cell leukaemia seen in A-T , we demonstrate a high frequency of ATM mutations in T-PLL . In marked contrast to the ATM mutation pattern in A-T , the most frequent nucleotide changes in this leukaemia were missense mutations . These clustered in the region corresponding to the kinase domain , which is highly conserved in ATM-related proteins in mouse , yeast and Drosophila . The resulting amino-acid substitutions are predicted to interfere with ATP binding or substrate recognition . Two of seventeen mutated T-PLL samples had a previously reported A-T allele . In contrast , no mutations were detected in the p53 gene , suggesting that this tumour suppressor is not frequently altered in this leukaemia . Occasional missense mutations in ATM were also found in tumour DNA from patients with B-cell non-Hodgkins lymphomas ( B-NHL ) and a B-NHL cell line . The evidence of a significant proportion of loss-of-function mutations and a complete absence of the normal copy of ATM in the majority of mutated tumours establishes somatic inactivation of this gene in the pathogenesis of sporadic T-PLL and suggests that ATM acts as a tumour suppressor . As constitutional DNA was not available , a putative hereditary predisposition to T-PLL will require further investigation . . \t0\n",
      "Myotonic dystrophy protein kinase is involved in the modulation of the Ca2+ homeostasis in skeletal muscle cells. Myotonic dystrophy ( DM ) , the most prevalent muscular disorder in adults , is caused by ( CTG ) n-repeat expansion in a gene encoding a protein kinase ( DM protein kinase ; DMPK ) and involves changes in cytoarchitecture and ion homeostasis . To obtain clues to the normal biological role of DMPK in cellular ion homeostasis , we have compared the resting [ Ca2 + ] i , the amplitude and shape of depolarization-induced Ca2 + transients , and the content of ATP-driven ion pumps in cultured skeletal muscle cells of wild-type and DMPK [ - / - ] knockout mice . In vitro-differentiated DMPK [ - / - ] myotubes exhibit a higher resting [ Ca2 + ] i than do wild-type myotubes because of an altered open probability of voltage-dependent l-type Ca2 + and Na + channels . The mutant myotubes exhibit smaller and slower Ca2 + responses upon triggering by acetylcholine or high external K + . In addition , we observed that these Ca2 + transients partially result from an influx of extracellular Ca2 + through the l-type Ca2 + channel . Neither the content nor the activity of Na + / K + ATPase and sarcoplasmic reticulum Ca2 + -ATPase are affected by DMPK absence . In conclusion , our data suggest that DMPK is involved in modulating the initial events of excitation-contraction coupling in skeletal muscle . . \t1\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the tab separated data\n",
    "print(\"*****\\ntrain.tsv sample\\n*****\")\n",
    "!head -n 3 $TC3_DATA_DIR/train.tsv\n",
    "print(\"\\n\\n*****\\ndev.tsv sample\\n*****\")\n",
    "!head -n 3 $TC3_DATA_DIR/dev.tsv\n",
    "print(\"\\n\\n*****\\ntest.tsv sample\\n*****\")\n",
    "!head -n 3 $TC3_DATA_DIR/test.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Note a few features of the data:\n",
    "* The preprocessed data is already in the \n",
    "\n",
    "   ```\n",
    "   [WORD][SPACE][WORD][SPACE][WORD][TAB][LABEL]\n",
    "   ``` \n",
    "   format specified in the [documentation](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/text_classification.html).\n",
    "* There is a header row, \"sentence label\", that should be removed.\n",
    "* The text is quite long, so `max_seq_length` values will need to take this into account for training.\n",
    "\n",
    "Start by removing the header rows.  There are a number of ways to do this, but since it is a simple change we can use a bash stream editor (`sed`) command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed 1d $TC3_DATA_DIR/train.tsv > $TC3_DATA_DIR/train_nemo_format.tsv\n",
    "!sed 1d $TC3_DATA_DIR/dev.tsv > $TC3_DATA_DIR/dev_nemo_format.tsv\n",
    "!sed 1d $TC3_DATA_DIR/test.tsv > $TC3_DATA_DIR/test_nemo_format.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****\n",
      "train_nemo_format.tsv sample\n",
      "*****\n",
      "Identification of APC2, a homologue of the adenomatous polyposis coli tumour suppressor . The adenomatous polyposis coli ( APC ) tumour-suppressor protein controls the Wnt signalling pathway by forming a complex with glycogen synthase kinase 3beta ( GSK-3beta ) , axin / conductin and betacatenin . Complex formation induces the rapid degradation of betacatenin . In colon carcinoma cells , loss of APC leads to the accumulation of betacatenin in the nucleus , where it binds to and activates the Tcf-4 transcription factor ( reviewed in [ 1 ] [ 2 ] ) . Here , we report the identification and genomic structure of APC homologues . Mammalian APC2 , which closely resembles APC in overall domain structure , was functionally analyzed and shown to contain two SAMP domains , both of which are required for binding to conductin . Like APC , APC2 regulates the formation of active betacatenin-Tcf complexes , as demonstrated using transient transcriptional activation assays in APC - / - colon carcinoma cells . Human APC2 maps to chromosome 19p13 . 3 . APC and APC2 may therefore have comparable functions in development and cancer .\t0\n",
      "A common MSH2 mutation in English and North American HNPCC families: origin, phenotypic expression, and sex specific differences in colorectal cancer . The frequency , origin , and phenotypic expression of a germline MSH2 gene mutation previously identified in seven kindreds with hereditary non-polyposis cancer syndrome (HNPCC) was investigated . The mutation ( A-- > T at nt943 + 3 ) disrupts the 3 splice site of exon 5 leading to the deletion of this exon from MSH2 mRNA and represents the only frequent MSH2 mutation so far reported . Although this mutation was initially detected in four of 33 colorectal cancer families analysed from eastern England , more extensive analysis has reduced the frequency to four of 52 ( 8 % ) English HNPCC kindreds analysed . In contrast , the MSH2 mutation was identified in 10 of 20 ( 50 % ) separately identified colorectal families from Newfoundland . To investigate the origin of this mutation in colorectal cancer families from England ( n = 4 ) , Newfoundland ( n = 10 ) , and the United States ( n = 3 ) , haplotype analysis using microsatellite markers linked to MSH2 was performed . Within the English and US families there was little evidence for a recent common origin of the MSH2 splice site mutation in most families . In contrast , a common haplotype was identified at the two flanking markers ( CA5 and D2S288 ) in eight of the Newfoundland families . These findings suggested a founder effect within Newfoundland similar to that reported by others for two MLH1 mutations in Finnish HNPCC families . We calculated age related risks of all , colorectal , endometrial , and ovarian cancers in nt943 + 3 A-- > T MSH2 mutation carriers ( n = 76 ) for all patients and for men and women separately . For both sexes combined , the penetrances at age 60 years for all cancers  and for colorectal cancer were 0 . 86 and 0 . 57 , respectively . The risk of colorectal cancer was significantly higher ( p < 0 . 01 ) in males than females ( 0 . 63 v 0 . 30 and 0 . 84 v 0 . 44 at ages 50 and 60 years , respectively ) . For females there was a high risk of endometrial cancer ( 0 . 5 at age 60 years ) and premenopausal ovarian cancer ( 0 . 2 at 50 years ) . These intersex differences in colorectal cancer risks have implications for screening programmes and for attempts to identify colorectal cancer susceptibility modifiers .\t0\n",
      "Age of onset in Huntington disease : sex specific influence of apolipoprotein E genotype and normal CAG repeat length . Age of onset ( AO ) of Huntington disease ( HD) is known to be correlated with the length of an expanded CAG repeat in the HD gene . Apolipoprotein E ( APOE ) genotype , in turn , is known to influence AO in Alzheimer disease , rendering the APOE gene a likely candidate to affect AO in other neurological diseases too . We therefore determined APOE genotype and normal CAG repeat length in the HD gene for 138 HD patients who were previously analysed with respect to CAG repeat length . Genotyping for APOE was performed blind to clinical information . In addition to highlighting the effect of the normal repeat length upon AO in maternally inherited HD and in male patients , we show that the APOE epsilon2epsilon3 genotype is associated with significantly earlier AO in males than in females . Such a sex difference in AO was not apparent for any of the other APOE genotypes . Our findings suggest that subtle differences in the course of the neurodegeneration  in HD  may allow interacting genes to exert gender specific effects upon AO.\t1\n",
      "\n",
      "\n",
      "*****\n",
      "dev_nemo_format.tsv sample\n",
      "*****\n",
      "BRCA1 is secreted and exhibits properties of a granin. Germline mutations in BRCA1 are responsible for most cases of inherited breast and ovarian cancer . However , the function of the BRCA1 protein has remained elusive . We now show that BRCA1 encodes a 190-kD protein with sequence homology and biochemical analogy to the granin protein family . Interestingly , BRCA2 also includes a motif similar to the granin consensus at the C terminus of the protein . Both BRCA1 and the granins localize to secretory vesicles , are secreted by a regulated pathway , are post-translationally glycosylated and are responsive to hormones . As a regulated secretory protein , BRCA1 appears to function by a mechanism not previously described for tumour suppressor gene products . . \t0\n",
      "Ovarian cancer risk in BRCA1 carriers is modified by the HRAS1 variable number of tandem repeat (VNTR) locus. Women who carry a mutation in the BRCA1 gene ( on chromosome 17q21 ) , have an 80 % risk of breast cancer and a 40 % risk of ovarian cancer by the age of 70 ( ref . 1 ) . The variable penetrance of BRCA1 suggests that other genetic and non-genetic factors play a role in tumourigenesis in these individuals . The HRAS1 variable number of tandem repeats ( VNTR ) polymorphism , located 1 kilobase ( kb ) downstream of the HRAS1 proto-oncogene ( chromosome 11p15 . 5 ) is one possible genetic modifier of cancer penetrance . Individuals who have rare alleles of the VNTR have an increased risk of certain types of cancers , including breast cancer ( 2-4 ) . To investigate whether the presence of rare HRAS1 alleles increases susceptibility to hereditary breast and ovarian cancer , we have typed a panel of 307 female BRCA1 carriers at this locus using a PCR-based technique . The risk for ovarian cancer was 2 . 11 times greater for BRCA1 carriers harbouring one or two rare HRAS1 alleles , compared to carriers with only common alleles ( P = 0 . 015 ) . The magnitude of the relative risk associated with a rare HRAS1 allele was not altered by adjusting for the other known risk factors for hereditary ovarian cancer ( 5 ) . Susceptibility to breast cancer did not appear to be affected by the presence of rare HRAS1 alleles . This study is the first to show the effect of a modifying gene on the penetrance of an inherited cancer syndrome \t0\n",
      "A novel homeodomain-encoding gene is associated with a large CpG island interrupted by the myotonic dystrophy unstable (CTG)n repeat. Myotonic dystrophy ( DM ) is associated with a ( CTG ) n trinucleotide repeat expansion in the 3-untranslated region of a protein kinase-encoding gene , DMPK , which maps to chromosome 19q13 . 3 . Characterisation of the expression of this gene in patient tissues has thus far generated conflicting data on alterations in the steady state levels of DMPK mRNA , and on the final DMPK protein levels in the presence of the expansion . The DM region of chromosome 19 is gene rich , and it is possible that the repeat expansion may lead to dysfunction of a number of transcription units in the vicinity , perhaps as a consequence of chromatin disruption . We have searched for genes associated with a CpG island at the 3 end of DMPK . Sequencing of this region shows that the island extends over 3 . 5 kb and is interrupted by the ( CTG ) n repeat . Comparison of genomic sequences downstream ( centromeric ) of the repeat in human and mouse identified regions of significant homology . These correspond to exons of a gene predicted to encode a homeodomain protein . RT-PCR analysis shows that this gene , which we have called DM locus-associated homeodomain protein ( DMAHP ) , is expressed in a number of human tissues , including skeletal muscle , heart and brain .\t1\n",
      "\n",
      "\n",
      "*****\n",
      "test_nemo_format.tsv sample\n",
      "*****\n",
      "Clustering of missense mutations in the ataxia-telangiectasia gene in a sporadic T-cell leukaemia. Ataxia-telangiectasia ( A-T ) is a recessive multi-system disorder caused by mutations in the ATM gene at 11q22-q23 ( ref . 3 ) . The risk of cancer , especially lymphoid neoplasias , is substantially elevated in A-T patients and has long been associated with chromosomal instability . By analysing tumour DNA from patients with sporadic T-cell prolymphocytic leukaemia ( T-PLL ) , a rare clonal malignancy with similarities to a mature T-cell leukaemia seen in A-T , we demonstrate a high frequency of ATM mutations in T-PLL . In marked contrast to the ATM mutation pattern in A-T , the most frequent nucleotide changes in this leukaemia were missense mutations . These clustered in the region corresponding to the kinase domain , which is highly conserved in ATM-related proteins in mouse , yeast and Drosophila . The resulting amino-acid substitutions are predicted to interfere with ATP binding or substrate recognition . Two of seventeen mutated T-PLL samples had a previously reported A-T allele . In contrast , no mutations were detected in the p53 gene , suggesting that this tumour suppressor is not frequently altered in this leukaemia . Occasional missense mutations in ATM were also found in tumour DNA from patients with B-cell non-Hodgkins lymphomas ( B-NHL ) and a B-NHL cell line . The evidence of a significant proportion of loss-of-function mutations and a complete absence of the normal copy of ATM in the majority of mutated tumours establishes somatic inactivation of this gene in the pathogenesis of sporadic T-PLL and suggests that ATM acts as a tumour suppressor . As constitutional DNA was not available , a putative hereditary predisposition to T-PLL will require further investigation . . \t0\n",
      "Myotonic dystrophy protein kinase is involved in the modulation of the Ca2+ homeostasis in skeletal muscle cells. Myotonic dystrophy ( DM ) , the most prevalent muscular disorder in adults , is caused by ( CTG ) n-repeat expansion in a gene encoding a protein kinase ( DM protein kinase ; DMPK ) and involves changes in cytoarchitecture and ion homeostasis . To obtain clues to the normal biological role of DMPK in cellular ion homeostasis , we have compared the resting [ Ca2 + ] i , the amplitude and shape of depolarization-induced Ca2 + transients , and the content of ATP-driven ion pumps in cultured skeletal muscle cells of wild-type and DMPK [ - / - ] knockout mice . In vitro-differentiated DMPK [ - / - ] myotubes exhibit a higher resting [ Ca2 + ] i than do wild-type myotubes because of an altered open probability of voltage-dependent l-type Ca2 + and Na + channels . The mutant myotubes exhibit smaller and slower Ca2 + responses upon triggering by acetylcholine or high external K + . In addition , we observed that these Ca2 + transients partially result from an influx of extracellular Ca2 + through the l-type Ca2 + channel . Neither the content nor the activity of Na + / K + ATPase and sarcoplasmic reticulum Ca2 + -ATPase are affected by DMPK absence . In conclusion , our data suggest that DMPK is involved in modulating the initial events of excitation-contraction coupling in skeletal muscle . . \t1\n",
      "Constitutional RB1-gene mutations in patients with isolated unilateral retinoblastoma. In most patients with isolated unilateral retinoblastoma , tumor development is initiated by somatic inactivation of both alleles of the RB1 gene . However , some of these patients can transmit retinoblastoma predisposition to their offspring . To determine the frequency and nature of constitutional RB1-gene mutations in patients with isolated unilateral retinoblastoma , we analyzed DNA from peripheral blood and from tumor tissue . The analysis of tumors from 54 ( 71 % ) of 76 informative patients showed loss of constitutional heterozygosity ( LOH ) at intragenic loci . Three of 13 uninformative patients had constitutional deletions . For 39 randomly selected tumors , SSCP , hetero-duplex analysis , sequencing , and Southern blot analysis were used to identify mutations . Mutations were detected in 21 ( 91 % ) of 23 tumors with LOH . In 6 ( 38 % ) of 16 tumors without LOH , one mutation was detected , and in 9 ( 56 % ) of the tumors without LOH , both mutations were found . Thus , a total of 45 mutations were identified in tumors of 36 patients . Thirty-nine of the mutations-including 34 small mutations , 2 large structural alterations , and hypermethylation in 3 tumors-were not detected in the corresponding peripheral blood DNA . In 6 ( 17 % ) of the 36 patients , a mutation was detected in constitutional DNA , and 1 of these mutations is known to be associated with reduced expressivity . The presence of a constitutional mutation was not associated with an early age at treatment . In 1 patient , somatic mosaicism was demonstrated by molecular analysis of DNA and RNA from peripheral blood . In 2 patients without a detectable mutation in peripheral blood , mosaicism was suggested because 1 of the patients showed multifocal tumors and the other later developed bilateral retinoblastoma . In conclusion , our results emphasize that the manifestation and transmissibility of retinoblastoma depend on the nature of the first mutation , its time in development , and the number and types of cells that are affected . . \t2\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the tab separated data\n",
    "# \"1\" is \"positive\" and \"0\" is \"negative\"\n",
    "print(\"*****\\ntrain_nemo_format.tsv sample\\n*****\")\n",
    "!head -n 3 $TC3_DATA_DIR/train_nemo_format.tsv\n",
    "print(\"\\n\\n*****\\ndev_nemo_format.tsv sample\\n*****\")\n",
    "!head -n 3 $TC3_DATA_DIR/dev_nemo_format.tsv\n",
    "print(\"\\n\\n*****\\ntest_nemo_format.tsv sample\\n*****\")\n",
    "!head -n 3 $TC3_DATA_DIR/test_nemo_format.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/NCBI_tc-3/dev.tsv\t\t    data/NCBI_tc-3/test_nemo_format.tsv\n",
      "data/NCBI_tc-3/dev_nemo_format.tsv  data/NCBI_tc-3/train.tsv\n",
      "data/NCBI_tc-3/test.tsv\t\t    data/NCBI_tc-3/train_nemo_format.tsv\n"
     ]
    }
   ],
   "source": [
    "TC3_DATA_DIR = 'data/NCBI_tc-3'\n",
    "!ls $TC3_DATA_DIR/*.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.2 Configuration File\n",
    "List the config file `text_classification_config.yaml` and take a look at the keys and default values.  Note the hierarchy of the keys and, especially, the three top-level keys: `trainer`, `model`, and `exp_manager`.\n",
    "\n",
    "```yaml\n",
    "trainer:\n",
    "  gpus:\n",
    "  num_nodes:\n",
    "  max_epochs:\n",
    "  ...\n",
    "  \n",
    "model:\n",
    "  nemo_path:\n",
    "  tokenizer:  \n",
    "  language_model:\n",
    "  classifier_head:\n",
    "  ...\n",
    "\n",
    "exp_manager:\n",
    "  ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "# Config file for text classification with pre-trained BERT models\n",
      "\n",
      "trainer:\n",
      "  gpus: 1 # number of GPUs (0 for CPU), or list of the GPUs to use e.g. [0, 1]\n",
      "  num_nodes: 1\n",
      "  max_epochs: 100\n",
      "  max_steps: null # precedence over max_epochs\n",
      "  accumulate_grad_batches: 1 # accumulates grads every k batches\n",
      "  gradient_clip_val: 0.0\n",
      "  amp_level: O0 # O1/O2 for mixed precision\n",
      "  precision: 32 # Should be set to 16 for O1 and O2 to enable the AMP.\n",
      "  accelerator: ddp\n",
      "  log_every_n_steps: 1  # Interval of logging.\n",
      "  val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations\n",
      "  resume_from_checkpoint: null # The path to a checkpoint file to continue the training, restores the whole state including the epoch, step, LR schedulers, apex, etc.\n",
      "  num_sanity_val_steps: 0 # number of steps to perform validation steps for sanity check the validation process before starting the training, setting to 0 disables it\n",
      "\n",
      "  checkpoint_callback: False  # Provided by exp_manager\n",
      "  logger: False  # Provided by exp_manager\n",
      "\n",
      "model:\n",
      "  nemo_path: text_classification_model.nemo # filename to save the model and associated artifacts to .nemo file\n",
      "  tokenizer:\n",
      "      tokenizer_name: ${model.language_model.pretrained_model_name} # or sentencepiece\n",
      "      vocab_file: null # path to vocab file\n",
      "      tokenizer_model: null # only used if tokenizer is sentencepiece\n",
      "      special_tokens: null\n",
      "\n",
      "  language_model:\n",
      "    pretrained_model_name: bert-base-uncased\n",
      "    lm_checkpoint: null\n",
      "    config_file: null # json file, precedence over config\n",
      "    config: null\n",
      "\n",
      "  classifier_head:\n",
      "    num_output_layers: 2\n",
      "    fc_dropout: 0.1\n",
      "\n",
      "  class_labels:\n",
      "    class_labels_file : null # optional to specify a file containing the list of the labels\n",
      "\n",
      "  dataset:\n",
      "    num_classes: ??? # The number of classes. 0 < Label <num_classes.\n",
      "    do_lower_case: false # true for uncased models, false for cased models, will be set automatically if pre-trained tokenizer model is used\n",
      "    max_seq_length: 256 # the maximum length BERT supports is 512\n",
      "    class_balancing: null # null or 'weighted_loss'. 'weighted_loss' enables the weighted class balancing of the loss, may be used for handling unbalanced classes\n",
      "    use_cache: false # uses a cache to store the processed dataset, you may use it for large datasets for speed up\n",
      "\n",
      "  train_ds:\n",
      "    file_path: null\n",
      "    batch_size: 64\n",
      "    shuffle: true\n",
      "    num_samples: -1 # number of samples to be considered, -1 means all the dataset\n",
      "    num_workers: 3\n",
      "    drop_last: false\n",
      "    pin_memory: false\n",
      "\n",
      "  validation_ds:\n",
      "    file_path: null\n",
      "    batch_size: 64\n",
      "    shuffle: false\n",
      "    num_samples: -1 # number of samples to be considered, -1 means all the dataset\n",
      "    num_workers: 3\n",
      "    drop_last: false\n",
      "    pin_memory: false\n",
      "\n",
      "  test_ds:\n",
      "    file_path: null\n",
      "    batch_size: 64\n",
      "    shuffle: false\n",
      "    num_samples: -1 # number of samples to be considered, -1 means all the dataset\n",
      "    num_workers: 3\n",
      "    drop_last: false\n",
      "    pin_memory: false\n",
      "\n",
      "  optim:\n",
      "    name: adam\n",
      "    lr: 2e-5\n",
      "    # optimizer arguments\n",
      "    betas: [0.9, 0.999]\n",
      "    weight_decay: 0.01\n",
      "\n",
      "    # scheduler setup\n",
      "    sched:\n",
      "      name: WarmupAnnealing\n",
      "      # Scheduler params\n",
      "      warmup_steps: null\n",
      "      warmup_ratio: 0.1\n",
      "      last_epoch: -1\n",
      "      # pytorch lightning args\n",
      "      reduce_on_plateau: false\n",
      "\n",
      "  # List of some sample queries for inference after training is done\n",
      "  infer_samples: [\n",
      "    'by the end of no such thing the audience , like beatrice , has a watchful affection for the monster .',\n",
      "    'director rob marshall went out gunning to make a great one .',\n",
      "    'uneasy mishmash of styles and genres .',\n",
      "  ]\n",
      "\n",
      "exp_manager:\n",
      "  exp_dir: \"./data_text_output\"  # exp_dir for your experiment, if None, defaults to \"./nemo_experiments\"\n",
      "  name: \"TextClassification\"  # The name of your model\n",
      "  create_tensorboard_logger: True  # Whether you want exp_manger to create a tb logger\n",
      "  create_checkpoint_callback: True  # Whether you want exp_manager to create a modelcheckpoint callback\n"
     ]
    }
   ],
   "source": [
    "CONFIG_DIR = \"nemo/examples/nlp/text_classification/conf\"\n",
    "CONFIG_FILE = \"text_classification_config.yaml\"\n",
    "!cat $CONFIG_DIR/$CONFIG_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2.1 OmegaConf Tool\n",
    "The YAML config file provides default values for most of the parameters, but there are a few items that must be specified for the text classification experiment in order to run it at all.  \n",
    "\n",
    "Each YAML section is a bit easier to view using the [omegaconf](https://omegaconf.readthedocs.io/en/2.1_branch/#) package, which allows you to access and manipulate the configuration keys using a \"dot\" protocol.  \n",
    "\n",
    "Start by instantiating an `OmegaConf` object from the config file. Keys in the object can be changed, added, viewed, saved, and so on.  \n",
    "\n",
    "For example, to look at just the `model` section, we can load the config file and specify just the `config.model` section to view through a print statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nemo_path: text_classification_model.nemo\n",
      "tokenizer:\n",
      "  tokenizer_name: ${model.language_model.pretrained_model_name}\n",
      "  vocab_file: null\n",
      "  tokenizer_model: null\n",
      "  special_tokens: null\n",
      "language_model:\n",
      "  pretrained_model_name: bert-base-uncased\n",
      "  lm_checkpoint: null\n",
      "  config_file: null\n",
      "  config: null\n",
      "classifier_head:\n",
      "  num_output_layers: 2\n",
      "  fc_dropout: 0.1\n",
      "class_labels:\n",
      "  class_labels_file: null\n",
      "dataset:\n",
      "  num_classes: ???\n",
      "  do_lower_case: false\n",
      "  max_seq_length: 256\n",
      "  class_balancing: null\n",
      "  use_cache: false\n",
      "train_ds:\n",
      "  file_path: null\n",
      "  batch_size: 64\n",
      "  shuffle: true\n",
      "  num_samples: -1\n",
      "  num_workers: 3\n",
      "  drop_last: false\n",
      "  pin_memory: false\n",
      "validation_ds:\n",
      "  file_path: null\n",
      "  batch_size: 64\n",
      "  shuffle: false\n",
      "  num_samples: -1\n",
      "  num_workers: 3\n",
      "  drop_last: false\n",
      "  pin_memory: false\n",
      "test_ds:\n",
      "  file_path: null\n",
      "  batch_size: 64\n",
      "  shuffle: false\n",
      "  num_samples: -1\n",
      "  num_workers: 3\n",
      "  drop_last: false\n",
      "  pin_memory: false\n",
      "optim:\n",
      "  name: adam\n",
      "  lr: 2.0e-05\n",
      "  betas:\n",
      "  - 0.9\n",
      "  - 0.999\n",
      "  weight_decay: 0.01\n",
      "  sched:\n",
      "    name: WarmupAnnealing\n",
      "    warmup_steps: null\n",
      "    warmup_ratio: 0.1\n",
      "    last_epoch: -1\n",
      "    reduce_on_plateau: false\n",
      "infer_samples:\n",
      "- by the end of no such thing the audience , like beatrice , has a watchful affection\n",
      "  for the monster .\n",
      "- director rob marshall went out gunning to make a great one .\n",
      "- uneasy mishmash of styles and genres .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "config = OmegaConf.load(CONFIG_DIR + \"/\" + CONFIG_FILE)\n",
    "print(OmegaConf.to_yaml(config.model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details about the model arguments can be found in the [documentation](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/text_classification.html#model-arguments).  The `dataset.num_classes` value as well as locations of the data in `train_ds.file_path`, `val_ds.file_path`, and `test_ds.file_path` are required.\n",
    "\n",
    "To make sure we don't run out of memory, we can limit the `dataset.max_seq_length` to 128.  It also looks like the `infer_samples` are related to movie reviews, so we can change those to sentences that are meaningful in the disease domain.\n",
    "\n",
    "There are some other parameters we might want to change later, but for now, this is all we absolutely must provide.  \n",
    "\n",
    "Next take a look at the `trainer` subsection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpus: 1\n",
      "num_nodes: 1\n",
      "max_epochs: 100\n",
      "max_steps: null\n",
      "accumulate_grad_batches: 1\n",
      "gradient_clip_val: 0.0\n",
      "amp_level: O0\n",
      "precision: 32\n",
      "accelerator: ddp\n",
      "log_every_n_steps: 1\n",
      "val_check_interval: 1.0\n",
      "resume_from_checkpoint: null\n",
      "num_sanity_val_steps: 0\n",
      "checkpoint_callback: false\n",
      "logger: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(OmegaConf.to_yaml(config.trainer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only have one GPU right now, so that setting is fine, but we might want to limit the `max_epochs` to just a few to start with.  As with the `model` configs, there are some other parameters we might want to change, but we can go with the default for our first try.  \n",
    "\n",
    "Finally, what about the `exp_manager`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp_dir: ./data_text_output\n",
      "name: TextClassification\n",
      "create_tensorboard_logger: true\n",
      "create_checkpoint_callback: true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(OmegaConf.to_yaml(config.exp_manager))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is fine as it is. When the `exp_dir` is `null`, it will default to placing the experiment results in a new directory named `nemo_experiments`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.3 Hydra-Enabled Python Script\n",
    "To recap, the parameters we need to change or override are:\n",
    "\n",
    "* `model.dataset.num_classes`: set to 3\n",
    "* `model.dataset.max_seq_length`: set to 128\n",
    "* `model.train_ds.file_path`: set to train_nemo_format.tsv\n",
    "* `model.val_ds.file_path`: set to dev_nemo_format.tsv\n",
    "* `model.test_ds.file_path`: set to test_nemo_format.tsv\n",
    "* `model.infer_samples` : set to relevent sentences\n",
    "* `trainer.max_epochs`: set to 3\n",
    "\n",
    "We can train, infer, and test it all **in one command** using the text classification training script!  \n",
    "\n",
    "The script uses Hydra to manage the config file, so that means we can just override the values we want to from the command line as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-05-22 02:02:40 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/omegaconf/basecontainer.py:225: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n",
      "    Use OmegaConf.to_yaml(cfg)\n",
      "    \n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo I 2024-05-22 02:02:40 text_classification_with_bert:110] \n",
      "    Config Params:\n",
      "    trainer:\n",
      "      gpus: 1\n",
      "      num_nodes: 1\n",
      "      max_epochs: 3\n",
      "      max_steps: null\n",
      "      accumulate_grad_batches: 1\n",
      "      gradient_clip_val: 0.0\n",
      "      amp_level: O0\n",
      "      precision: 32\n",
      "      accelerator: ddp\n",
      "      log_every_n_steps: 1\n",
      "      val_check_interval: 1.0\n",
      "      resume_from_checkpoint: null\n",
      "      num_sanity_val_steps: 0\n",
      "      checkpoint_callback: false\n",
      "      logger: false\n",
      "    model:\n",
      "      nemo_path: text_classification_model.nemo\n",
      "      tokenizer:\n",
      "        tokenizer_name: ${model.language_model.pretrained_model_name}\n",
      "        vocab_file: null\n",
      "        tokenizer_model: null\n",
      "        special_tokens: null\n",
      "      language_model:\n",
      "        pretrained_model_name: bert-base-uncased\n",
      "        lm_checkpoint: null\n",
      "        config_file: null\n",
      "        config: null\n",
      "      classifier_head:\n",
      "        num_output_layers: 2\n",
      "        fc_dropout: 0.1\n",
      "      class_labels:\n",
      "        class_labels_file: null\n",
      "      dataset:\n",
      "        num_classes: 3\n",
      "        do_lower_case: false\n",
      "        max_seq_length: 128\n",
      "        class_balancing: null\n",
      "        use_cache: false\n",
      "      train_ds:\n",
      "        file_path: data/NCBI_tc-3/train_nemo_format.tsv\n",
      "        batch_size: 64\n",
      "        shuffle: true\n",
      "        num_samples: -1\n",
      "        num_workers: 3\n",
      "        drop_last: false\n",
      "        pin_memory: false\n",
      "      validation_ds:\n",
      "        file_path: data/NCBI_tc-3/dev_nemo_format.tsv\n",
      "        batch_size: 64\n",
      "        shuffle: false\n",
      "        num_samples: -1\n",
      "        num_workers: 3\n",
      "        drop_last: false\n",
      "        pin_memory: false\n",
      "      test_ds:\n",
      "        file_path: data/NCBI_tc-3/test_nemo_format.tsv\n",
      "        batch_size: 64\n",
      "        shuffle: false\n",
      "        num_samples: -1\n",
      "        num_workers: 3\n",
      "        drop_last: false\n",
      "        pin_memory: false\n",
      "      optim:\n",
      "        name: adam\n",
      "        lr: 2.0e-05\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.999\n",
      "        weight_decay: 0.01\n",
      "        sched:\n",
      "          name: WarmupAnnealing\n",
      "          warmup_steps: null\n",
      "          warmup_ratio: 0.1\n",
      "          last_epoch: -1\n",
      "          reduce_on_plateau: false\n",
      "      infer_samples:\n",
      "      - In contrast no mutations were detected in the p53 gene suggesting that this tumour\n",
      "        suppressor is not frequently altered in this leukaemia\n",
      "      - The first predictive testing for Huntington disease  was based on analysis of\n",
      "        linked polymorphic DNA markers to estimate the likelihood of inheriting the mutation\n",
      "        for HD\n",
      "      - Further studies suggested that low dilutions of C5D serum contain a factor or\n",
      "        factors interfering at some step in the hemolytic assay of C5 rather than a true\n",
      "        C5 inhibitor or inactivator\n",
      "    exp_manager:\n",
      "      exp_dir: ./data_text_output\n",
      "      name: TextClassification\n",
      "      create_tensorboard_logger: true\n",
      "      create_checkpoint_callback: true\n",
      "    \n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "[NeMo I 2024-05-22 02:02:40 exp_manager:216] Experiments will be logged at data_text_output/TextClassification/2024-05-22_02-02-40\n",
      "[NeMo I 2024-05-22 02:02:40 exp_manager:563] TensorboardLogger has been set up\n",
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "[NeMo I 2024-05-22 02:02:43 text_classification_dataset:120] Read 683 examples from data/NCBI_tc-3/train_nemo_format.tsv.\n",
      "[NeMo I 2024-05-22 02:02:43 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2024-05-22 02:02:43 text_classification_dataset:239] example 0: ['Inactivation', 'of', 'germline', 'mutant', 'APC', 'alleles', 'by', 'attenuated', 'somatic', 'mutations:', 'a', 'molecular', 'genetic', 'mechanism', 'for', 'attenuated', 'familial', 'adenomatous', 'polyposis.', 'Germline', 'mutations', 'of', 'the', 'adenomatous', 'polyposis', 'coli', '(', 'APC', ')', 'tumor-suppressor', 'gene', 'result', 'in', 'familial', 'adenomatous', 'polyposis', '(', 'FAP', ')', '.', 'Patients', 'with', 'FAP', 'typically', 'develop', 'hundreds', 'to', 'thousands', 'of', 'benign', 'colorectal', 'tumors', 'and', 'early-onset', 'colorectal', 'cancer', '.', 'A', 'subset', 'of', 'germline', 'APC', 'mutations', 'results', 'in', 'an', 'attenuated', 'FAP', '(', 'AFAP', ')', 'phenotype', ',', 'in', 'which', 'patients', 'develop', 'fewer', 'tumors', 'and', 'develop', 'them', 'at', 'an', 'older', 'age', '.', 'Although', 'a', 'genotype-phenotype', 'correlation', 'between', 'the', 'locations', 'of', 'APC', 'germline', 'mutations', 'and', 'the', 'development', 'of', 'AFAP', 'has', 'been', 'well', 'documented', ',', 'the', 'mechanism', 'for', 'AFAP', 'has', 'not', 'been', 'well', 'defined', '.', 'We', 'investigated', 'the', 'mechanism', 'for', 'AFAP', 'in', 'patients', 'carrying', 'a', 'mutant', 'APC', 'allele', '(', 'APC', '(', 'AS9', ')', ')', 'that', 'has', 'a', 'mutation', 'in', 'the', 'alternatively', 'spliced', 'region', 'of', 'exon', '9', '.', 'APC', '(', 'AS9', ')', 'was', 'found', 'to', 'down-regulate', 'beta-catenin-regulated', 'transcription', ',', 'the', 'major', 'tumor-suppressor', 'function', 'of', 'APC', ',', 'as', 'did', 'the', 'wild-type', 'APC', '.', 'Mutation', 'analysis', 'showed', 'that', 'both', 'APC', '(', 'AS9', ')', 'and', 'the', 'wild-type', 'APC', 'alleles', 'were', 'somatically', 'mutated', 'in', 'most', 'colorectal', 'tumors', 'from', 'these', 'patients', '.', 'Functional', 'analysis', 'showed', 'that', '4666insA', ',', 'a', 'common', 'somatic', 'mutation', 'in', 'APC', '(', 'AS9', ')', 'in', 'these', 'tumors', ',', 'did', 'not', 'inactivate', 'the', 'wild-type', 'APC', '.', 'Our', 'results', 'indicate', 'that', 'carriers', 'of', 'APC', '(', 'AS9', ')', 'develop', 'fewer', 'colorectal', 'tumors', 'than', 'do', 'typical', 'patients', 'with', 'FAP', 'because', 'somatic', 'inactivation', 'of', 'both', 'APC', 'alleles', 'is', 'necessary', 'for', 'colorectal', 'tumorigenesis', '.', 'However', ',', 'these', 'patients', 'develop', 'colorectal', 'tumors', 'more', 'frequently', 'than', 'does', 'the', 'general', 'population', 'because', 'APC', '(', 'AS9', ')', 'is', 'inactivated', 'by', 'mutations', 'that', 'do', 'not', 'inactivate', 'the', 'wild-type', 'APC', '.', '.']\n",
      "[NeMo I 2024-05-22 02:02:43 text_classification_dataset:240] subtokens: [CLS] ina ##ct ##ivation of ge ##rm ##line mutant ap ##c all ##eles by at ##ten ##uated so ##matic mutations : a molecular genetic mechanism for at ##ten ##uated fa ##mi ##lia ##l aden ##oma ##tou ##s poly ##po ##sis . ge ##rm ##line mutations of the aden ##oma ##tou ##s poly ##po ##sis coli ( ap ##c ) tumor - suppress ##or gene result in fa ##mi ##lia ##l aden ##oma ##tou ##s poly ##po ##sis ( fa ##p ) . patients with fa ##p typically develop hundreds to thousands of benign color ##ect ##al tumors and early - onset color ##ect ##al cancer . a subset of ge ##rm ##line ap ##c mutations results in an at ##ten ##uated fa ##p ( af ##ap ) ph [SEP]\n",
      "[NeMo I 2024-05-22 02:02:43 text_classification_dataset:241] input_ids: 101 27118 6593 25761 1997 16216 10867 4179 15527 9706 2278 2035 26741 2011 2012 6528 16453 2061 12644 14494 1024 1037 8382 7403 7337 2005 2012 6528 16453 6904 4328 6632 2140 16298 9626 24826 2015 26572 6873 6190 1012 16216 10867 4179 14494 1997 1996 16298 9626 24826 2015 26572 6873 6190 27441 1006 9706 2278 1007 13656 1011 16081 2953 4962 2765 1999 6904 4328 6632 2140 16298 9626 24826 2015 26572 6873 6190 1006 6904 2361 1007 1012 5022 2007 6904 2361 4050 4503 5606 2000 5190 1997 28378 3609 22471 2389 21434 1998 2220 1011 14447 3609 22471 2389 4456 1012 1037 16745 1997 16216 10867 4179 9706 2278 14494 3463 1999 2019 2012 6528 16453 6904 2361 1006 21358 9331 1007 6887 102\n",
      "[NeMo I 2024-05-22 02:02:43 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2024-05-22 02:02:43 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2024-05-22 02:02:43 text_classification_dataset:244] label: 0\n",
      "[NeMo I 2024-05-22 02:02:43 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2024-05-22 02:02:43 text_classification_dataset:239] example 1: ['Heterozygous', 'loss', 'of', 'Six5', 'in', 'mice', 'is', 'sufficient', 'to', 'cause', 'ocular', 'cataracts.', 'Myotonic', 'dystrophy', '(', 'DM', ')', 'is', 'an', 'autosomal', 'dominant', 'disorder', 'characterized', 'by', 'skeletal', 'muscle', 'wasting', ',', 'myotonia', ',', 'cardiac', 'arrhythmia', ',', 'hyperinsulinaemia', ',', 'mental', 'retardation', 'and', 'ocular', 'cataracts', '.', 'The', 'genetic', 'defect', 'in', 'DM', 'is', 'a', 'CTG', 'repeat', 'expansion', 'located', 'in', 'the', '3', 'untranslated', 'region', 'of', 'DMPK', 'and', '5', 'of', 'a', 'homeodomain-encoding', 'gene', ',', 'SIX5', '(', 'formerly', 'DMAHP', ';', 'refs', '2-5', ')', '.', 'There', 'are', 'three', 'mechanisms', 'by', 'which', 'CTG', 'expansion', 'can', 'result', 'in', 'DM', '.', 'First', ',', 'repeat', 'expansion', 'may', 'alter', 'the', 'processing', 'or', 'transport', 'of', 'the', 'mutant', 'DMPK', 'mRNA', 'and', 'consequently', 'reduce', 'DMPK', 'levels', '.', 'Second', ',', 'CTG', 'expansion', 'may', 'establish', 'a', 'region', 'of', 'heterochromatin', '3', 'of', 'the', 'repeat', 'sequence', 'and', 'decrease', 'SIX5', 'transcription', '.', 'Third', ',', 'toxic', 'effects', 'of', 'the', 'repeat', 'expansion', 'may', 'be', 'intrinsic', 'to', 'the', 'repeated', 'elements', 'at', 'the', 'level', 'of', 'DNA', 'or', 'RNA', '(', 'refs', '10', ',', '11', ')', '.', 'Previous', 'studies', 'have', 'demonstrated', 'that', 'a', 'dose-dependent', 'loss', 'of', 'Dm15', '(', 'the', 'mouse', 'DMPK', 'homologue', ')', 'in', 'mice', 'produces', 'a', 'partial', 'DM', 'phenotype', 'characterized', 'by', 'decreased', 'development', 'of', 'skeletal', 'muscle', 'force', 'and', 'cardiac', 'conduction', 'disorders', '.', 'To', 'test', 'the', 'role', 'of', 'Six5', 'loss', 'in', 'DM', ',', 'we', 'have', 'analysed', 'a', 'strain', 'of', 'mice', 'in', 'which', 'Six5', 'was', 'deleted', '.', 'Our', 'results', 'demonstrate', 'that', 'the', 'rate', 'and', 'severity', 'of', 'cataract', 'formation', 'is', 'inversely', 'related', 'to', 'Six5', 'dosage', 'and', 'is', 'temporally', 'progressive', '.', 'Six5', '+', '/', '-', 'and', 'Six5-', '/', '-', 'mice', 'show', 'increased', 'steady-state', 'levels', 'of', 'the', 'Na', '+', '/', 'K', '+', '-ATPase', 'alpha-1', 'subunit', 'and', 'decreased', 'Dm15', 'mRNA', 'levels', '.', 'Thus', ',', 'altered', 'ion', 'homeostasis', 'within', 'the', 'lens', 'may', 'contribute', 'to', 'cataract', 'formation', '.', 'As', 'ocular', 'cataracts', 'are', 'a', 'characteristic', 'feature', 'of', 'DM', ',', 'these', 'results', 'demonstrate', 'that', 'decreased', 'SIX5', 'transcription', 'is', 'important', 'in', 'the', 'aetiology', 'of', 'DM', '.', 'Our', 'data', 'support', 'the', 'hypothesis', 'that', 'DM', 'is', 'a', 'contiguous', 'gene', 'syndrome', 'associated', 'with', 'the', 'partial', 'loss', 'of', 'both', 'DMPK', 'and', 'SIX5', '.', '.']\n",
      "[NeMo I 2024-05-22 02:02:43 text_classification_dataset:240] subtokens: [CLS] het ##ero ##zy ##go ##us loss of six ##5 in mice is sufficient to cause o ##cular cat ##ara ##cts . my ##oton ##ic d ##yst ##rop ##hy ( d ##m ) is an auto ##som ##al dominant disorder characterized by skeletal muscle wasting , my ##oton ##ia , cardiac ar ##rh ##yt ##hmi ##a , hyper ##ins ##ulin ##ae ##mia , mental re ##tar ##dation and o ##cular cat ##ara ##cts . the genetic defect in d ##m is a ct ##g repeat expansion located in the 3 un ##tra ##ns ##lated region of d ##mp ##k and 5 of a home ##od ##oma ##in - encoding gene , six ##5 ( formerly d ##mah ##p ; ref ##s 2 - 5 ) . there are [SEP]\n",
      "[NeMo I 2024-05-22 02:02:43 text_classification_dataset:241] input_ids: 101 21770 10624 9096 3995 2271 3279 1997 2416 2629 1999 12328 2003 7182 2000 3426 1051 15431 4937 5400 16649 1012 2026 25862 2594 1040 27268 18981 10536 1006 1040 2213 1007 2003 2019 8285 25426 2389 7444 8761 7356 2011 20415 6740 18313 1010 2026 25862 2401 1010 15050 12098 25032 22123 26837 2050 1010 23760 7076 18639 6679 10092 1010 5177 2128 7559 20207 1998 1051 15431 4937 5400 16649 1012 1996 7403 21262 1999 1040 2213 2003 1037 14931 2290 9377 4935 2284 1999 1996 1017 4895 6494 3619 13776 2555 1997 1040 8737 2243 1998 1019 1997 1037 2188 7716 9626 2378 1011 17181 4962 1010 2416 2629 1006 3839 1040 25687 2361 1025 25416 2015 1016 1011 1019 1007 1012 2045 2024 102\n",
      "[NeMo I 2024-05-22 02:02:43 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2024-05-22 02:02:43 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2024-05-22 02:02:43 text_classification_dataset:244] label: 1\n",
      "[NeMo W 2024-05-22 02:02:49 text_classification_dataset:250] Found 664 out of 683 sentences with more than 128 subtokens. Truncated long sentences from the end.\n",
      "[NeMo I 2024-05-22 02:02:49 data_preprocessing:299] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2024-05-22 02:02:49 data_preprocessing:301] Min: 74 |                  Max: 129 |                  Mean: 128.36163982430455 |                  Median: 129.0\n",
      "[NeMo I 2024-05-22 02:02:49 data_preprocessing:307] 75 percentile: 129.00\n",
      "[NeMo I 2024-05-22 02:02:49 data_preprocessing:308] 99 percentile: 129.00\n",
      "[NeMo I 2024-05-22 02:02:49 text_classification_dataset:120] Read 100 examples from data/NCBI_tc-3/dev_nemo_format.tsv.\n",
      "[NeMo I 2024-05-22 02:02:49 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2024-05-22 02:02:49 text_classification_dataset:239] example 0: ['BRCA1', 'is', 'secreted', 'and', 'exhibits', 'properties', 'of', 'a', 'granin.', 'Germline', 'mutations', 'in', 'BRCA1', 'are', 'responsible', 'for', 'most', 'cases', 'of', 'inherited', 'breast', 'and', 'ovarian', 'cancer', '.', 'However', ',', 'the', 'function', 'of', 'the', 'BRCA1', 'protein', 'has', 'remained', 'elusive', '.', 'We', 'now', 'show', 'that', 'BRCA1', 'encodes', 'a', '190-kD', 'protein', 'with', 'sequence', 'homology', 'and', 'biochemical', 'analogy', 'to', 'the', 'granin', 'protein', 'family', '.', 'Interestingly', ',', 'BRCA2', 'also', 'includes', 'a', 'motif', 'similar', 'to', 'the', 'granin', 'consensus', 'at', 'the', 'C', 'terminus', 'of', 'the', 'protein', '.', 'Both', 'BRCA1', 'and', 'the', 'granins', 'localize', 'to', 'secretory', 'vesicles', ',', 'are', 'secreted', 'by', 'a', 'regulated', 'pathway', ',', 'are', 'post-translationally', 'glycosylated', 'and', 'are', 'responsive', 'to', 'hormones', '.', 'As', 'a', 'regulated', 'secretory', 'protein', ',', 'BRCA1', 'appears', 'to', 'function', 'by', 'a', 'mechanism', 'not', 'previously', 'described', 'for', 'tumour', 'suppressor', 'gene', 'products', '.', '.']\n",
      "[NeMo I 2024-05-22 02:02:49 text_classification_dataset:240] subtokens: [CLS] br ##ca ##1 is secret ##ed and exhibits properties of a gran ##in . ge ##rm ##line mutations in br ##ca ##1 are responsible for most cases of inherited breast and o ##var ##ian cancer . however , the function of the br ##ca ##1 protein has remained elusive . we now show that br ##ca ##1 en ##codes a 190 - k ##d protein with sequence homo ##logy and bio ##chemical analogy to the gran ##in protein family . interesting ##ly , br ##ca ##2 also includes a motif similar to the gran ##in consensus at the c terminus of the protein . both br ##ca ##1 and the gran ##ins local ##ize to secret ##ory ve ##sic ##les , are secret ##ed by a regulated [SEP]\n",
      "[NeMo I 2024-05-22 02:02:49 text_classification_dataset:241] input_ids: 101 7987 3540 2487 2003 3595 2098 1998 10637 5144 1997 1037 12604 2378 1012 16216 10867 4179 14494 1999 7987 3540 2487 2024 3625 2005 2087 3572 1997 7900 7388 1998 1051 10755 2937 4456 1012 2174 1010 1996 3853 1997 1996 7987 3540 2487 5250 2038 2815 26475 1012 2057 2085 2265 2008 7987 3540 2487 4372 23237 1037 11827 1011 1047 2094 5250 2007 5537 24004 6483 1998 16012 15869 23323 2000 1996 12604 2378 5250 2155 1012 5875 2135 1010 7987 3540 2475 2036 2950 1037 16226 2714 2000 1996 12604 2378 10465 2012 1996 1039 7342 1997 1996 5250 1012 2119 7987 3540 2487 1998 1996 12604 7076 2334 4697 2000 3595 10253 2310 19570 4244 1010 2024 3595 2098 2011 1037 12222 102\n",
      "[NeMo I 2024-05-22 02:02:49 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2024-05-22 02:02:49 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2024-05-22 02:02:49 text_classification_dataset:244] label: 0\n",
      "[NeMo I 2024-05-22 02:02:49 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2024-05-22 02:02:49 text_classification_dataset:239] example 1: ['Ovarian', 'cancer', 'risk', 'in', 'BRCA1', 'carriers', 'is', 'modified', 'by', 'the', 'HRAS1', 'variable', 'number', 'of', 'tandem', 'repeat', '(VNTR)', 'locus.', 'Women', 'who', 'carry', 'a', 'mutation', 'in', 'the', 'BRCA1', 'gene', '(', 'on', 'chromosome', '17q21', ')', ',', 'have', 'an', '80', '%', 'risk', 'of', 'breast', 'cancer', 'and', 'a', '40', '%', 'risk', 'of', 'ovarian', 'cancer', 'by', 'the', 'age', 'of', '70', '(', 'ref', '.', '1', ')', '.', 'The', 'variable', 'penetrance', 'of', 'BRCA1', 'suggests', 'that', 'other', 'genetic', 'and', 'non-genetic', 'factors', 'play', 'a', 'role', 'in', 'tumourigenesis', 'in', 'these', 'individuals', '.', 'The', 'HRAS1', 'variable', 'number', 'of', 'tandem', 'repeats', '(', 'VNTR', ')', 'polymorphism', ',', 'located', '1', 'kilobase', '(', 'kb', ')', 'downstream', 'of', 'the', 'HRAS1', 'proto-oncogene', '(', 'chromosome', '11p15', '.', '5', ')', 'is', 'one', 'possible', 'genetic', 'modifier', 'of', 'cancer', 'penetrance', '.', 'Individuals', 'who', 'have', 'rare', 'alleles', 'of', 'the', 'VNTR', 'have', 'an', 'increased', 'risk', 'of', 'certain', 'types', 'of', 'cancers', ',', 'including', 'breast', 'cancer', '(', '2-4', ')', '.', 'To', 'investigate', 'whether', 'the', 'presence', 'of', 'rare', 'HRAS1', 'alleles', 'increases', 'susceptibility', 'to', 'hereditary', 'breast', 'and', 'ovarian', 'cancer', ',', 'we', 'have', 'typed', 'a', 'panel', 'of', '307', 'female', 'BRCA1', 'carriers', 'at', 'this', 'locus', 'using', 'a', 'PCR-based', 'technique', '.', 'The', 'risk', 'for', 'ovarian', 'cancer', 'was', '2', '.', '11', 'times', 'greater', 'for', 'BRCA1', 'carriers', 'harbouring', 'one', 'or', 'two', 'rare', 'HRAS1', 'alleles', ',', 'compared', 'to', 'carriers', 'with', 'only', 'common', 'alleles', '(', 'P', '=', '0', '.', '015', ')', '.', 'The', 'magnitude', 'of', 'the', 'relative', 'risk', 'associated', 'with', 'a', 'rare', 'HRAS1', 'allele', 'was', 'not', 'altered', 'by', 'adjusting', 'for', 'the', 'other', 'known', 'risk', 'factors', 'for', 'hereditary', 'ovarian', 'cancer', '(', '5', ')', '.', 'Susceptibility', 'to', 'breast', 'cancer', 'did', 'not', 'appear', 'to', 'be', 'affected', 'by', 'the', 'presence', 'of', 'rare', 'HRAS1', 'alleles', '.', 'This', 'study', 'is', 'the', 'first', 'to', 'show', 'the', 'effect', 'of', 'a', 'modifying', 'gene', 'on', 'the', 'penetrance', 'of', 'an', 'inherited', 'cancer', 'syndrome']\n",
      "[NeMo I 2024-05-22 02:02:49 text_classification_dataset:240] subtokens: [CLS] o ##var ##ian cancer risk in br ##ca ##1 carriers is modified by the hr ##as ##1 variable number of tandem repeat ( v ##nt ##r ) locus . women who carry a mutation in the br ##ca ##1 gene ( on chromosome 17 ##q ##21 ) , have an 80 % risk of breast cancer and a 40 % risk of o ##var ##ian cancer by the age of 70 ( ref . 1 ) . the variable pen ##et ##rance of br ##ca ##1 suggests that other genetic and non - genetic factors play a role in tu ##mour ##igen ##esis in these individuals . the hr ##as ##1 variable number of tandem repeats ( v ##nt ##r ) poly ##morphism , located 1 ki [SEP]\n",
      "[NeMo I 2024-05-22 02:02:49 text_classification_dataset:241] input_ids: 101 1051 10755 2937 4456 3891 1999 7987 3540 2487 11363 2003 6310 2011 1996 17850 3022 2487 8023 2193 1997 18231 9377 1006 1058 3372 2099 1007 25206 1012 2308 2040 4287 1037 16221 1999 1996 7987 3540 2487 4962 1006 2006 16706 2459 4160 17465 1007 1010 2031 2019 3770 1003 3891 1997 7388 4456 1998 1037 2871 1003 3891 1997 1051 10755 2937 4456 2011 1996 2287 1997 3963 1006 25416 1012 1015 1007 1012 1996 8023 7279 3388 21621 1997 7987 3540 2487 6083 2008 2060 7403 1998 2512 1011 7403 5876 2377 1037 2535 1999 10722 20360 29206 19009 1999 2122 3633 1012 1996 17850 3022 2487 8023 2193 1997 18231 17993 1006 1058 3372 2099 1007 26572 19539 1010 2284 1015 11382 102\n",
      "[NeMo I 2024-05-22 02:02:49 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2024-05-22 02:02:49 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2024-05-22 02:02:49 text_classification_dataset:244] label: 0\n",
      "[NeMo W 2024-05-22 02:02:50 text_classification_dataset:250] Found 99 out of 100 sentences with more than 128 subtokens. Truncated long sentences from the end.\n",
      "[NeMo I 2024-05-22 02:02:50 data_preprocessing:299] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2024-05-22 02:02:50 data_preprocessing:301] Min: 120 |                  Max: 129 |                  Mean: 128.91 |                  Median: 129.0\n",
      "[NeMo I 2024-05-22 02:02:50 data_preprocessing:307] 75 percentile: 129.00\n",
      "[NeMo I 2024-05-22 02:02:50 data_preprocessing:308] 99 percentile: 129.00\n",
      "[NeMo I 2024-05-22 02:02:50 text_classification_dataset:120] Read 10 examples from data/NCBI_tc-3/test_nemo_format.tsv.\n",
      "[NeMo I 2024-05-22 02:02:50 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2024-05-22 02:02:50 text_classification_dataset:239] example 0: ['Clustering', 'of', 'missense', 'mutations', 'in', 'the', 'ataxia-telangiectasia', 'gene', 'in', 'a', 'sporadic', 'T-cell', 'leukaemia.', 'Ataxia-telangiectasia', '(', 'A-T', ')', 'is', 'a', 'recessive', 'multi-system', 'disorder', 'caused', 'by', 'mutations', 'in', 'the', 'ATM', 'gene', 'at', '11q22-q23', '(', 'ref', '.', '3', ')', '.', 'The', 'risk', 'of', 'cancer', ',', 'especially', 'lymphoid', 'neoplasias', ',', 'is', 'substantially', 'elevated', 'in', 'A-T', 'patients', 'and', 'has', 'long', 'been', 'associated', 'with', 'chromosomal', 'instability', '.', 'By', 'analysing', 'tumour', 'DNA', 'from', 'patients', 'with', 'sporadic', 'T-cell', 'prolymphocytic', 'leukaemia', '(', 'T-PLL', ')', ',', 'a', 'rare', 'clonal', 'malignancy', 'with', 'similarities', 'to', 'a', 'mature', 'T-cell', 'leukaemia', 'seen', 'in', 'A-T', ',', 'we', 'demonstrate', 'a', 'high', 'frequency', 'of', 'ATM', 'mutations', 'in', 'T-PLL', '.', 'In', 'marked', 'contrast', 'to', 'the', 'ATM', 'mutation', 'pattern', 'in', 'A-T', ',', 'the', 'most', 'frequent', 'nucleotide', 'changes', 'in', 'this', 'leukaemia', 'were', 'missense', 'mutations', '.', 'These', 'clustered', 'in', 'the', 'region', 'corresponding', 'to', 'the', 'kinase', 'domain', ',', 'which', 'is', 'highly', 'conserved', 'in', 'ATM-related', 'proteins', 'in', 'mouse', ',', 'yeast', 'and', 'Drosophila', '.', 'The', 'resulting', 'amino-acid', 'substitutions', 'are', 'predicted', 'to', 'interfere', 'with', 'ATP', 'binding', 'or', 'substrate', 'recognition', '.', 'Two', 'of', 'seventeen', 'mutated', 'T-PLL', 'samples', 'had', 'a', 'previously', 'reported', 'A-T', 'allele', '.', 'In', 'contrast', ',', 'no', 'mutations', 'were', 'detected', 'in', 'the', 'p53', 'gene', ',', 'suggesting', 'that', 'this', 'tumour', 'suppressor', 'is', 'not', 'frequently', 'altered', 'in', 'this', 'leukaemia', '.', 'Occasional', 'missense', 'mutations', 'in', 'ATM', 'were', 'also', 'found', 'in', 'tumour', 'DNA', 'from', 'patients', 'with', 'B-cell', 'non-Hodgkins', 'lymphomas', '(', 'B-NHL', ')', 'and', 'a', 'B-NHL', 'cell', 'line', '.', 'The', 'evidence', 'of', 'a', 'significant', 'proportion', 'of', 'loss-of-function', 'mutations', 'and', 'a', 'complete', 'absence', 'of', 'the', 'normal', 'copy', 'of', 'ATM', 'in', 'the', 'majority', 'of', 'mutated', 'tumours', 'establishes', 'somatic', 'inactivation', 'of', 'this', 'gene', 'in', 'the', 'pathogenesis', 'of', 'sporadic', 'T-PLL', 'and', 'suggests', 'that', 'ATM', 'acts', 'as', 'a', 'tumour', 'suppressor', '.', 'As', 'constitutional', 'DNA', 'was', 'not', 'available', ',', 'a', 'putative', 'hereditary', 'predisposition', 'to', 'T-PLL', 'will', 'require', 'further', 'investigation', '.', '.']\n",
      "[NeMo I 2024-05-22 02:02:50 text_classification_dataset:240] subtokens: [CLS] cluster ##ing of miss ##ense mutations in the ata ##xia - tel ##ang ##iec ##tas ##ia gene in a sporadic t - cell le ##uka ##emia . ata ##xia - tel ##ang ##iec ##tas ##ia ( a - t ) is a recess ##ive multi - system disorder caused by mutations in the atm gene at 11 ##q ##22 - q ##23 ( ref . 3 ) . the risk of cancer , especially l ##ym ##ph ##oid neo ##pl ##asia ##s , is substantially elevated in a - t patients and has long been associated with ch ##rom ##osomal instability . by anal ##ys ##ing tu ##mour dna from patients with sporadic t - cell pro ##ly ##mp ##ho ##cy ##tic le ##uka ##emia ( t [SEP]\n",
      "[NeMo I 2024-05-22 02:02:50 text_classification_dataset:241] input_ids: 101 9324 2075 1997 3335 16700 14494 1999 1996 29533 14787 1011 10093 5654 23783 10230 2401 4962 1999 1037 24590 1056 1011 3526 3393 15750 17577 1012 29533 14787 1011 10093 5654 23783 10230 2401 1006 1037 1011 1056 1007 2003 1037 28290 3512 4800 1011 2291 8761 3303 2011 14494 1999 1996 27218 4962 2012 2340 4160 19317 1011 1053 21926 1006 25416 1012 1017 1007 1012 1996 3891 1997 4456 1010 2926 1048 24335 8458 9314 9253 24759 15396 2015 1010 2003 12381 8319 1999 1037 1011 1056 5022 1998 2038 2146 2042 3378 2007 10381 21716 27642 18549 1012 2011 20302 7274 2075 10722 20360 6064 2013 5022 2007 24590 1056 1011 3526 4013 2135 8737 6806 5666 4588 3393 15750 17577 1006 1056 102\n",
      "[NeMo I 2024-05-22 02:02:50 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2024-05-22 02:02:50 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2024-05-22 02:02:50 text_classification_dataset:244] label: 0\n",
      "[NeMo I 2024-05-22 02:02:50 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2024-05-22 02:02:50 text_classification_dataset:239] example 1: ['Myotonic', 'dystrophy', 'protein', 'kinase', 'is', 'involved', 'in', 'the', 'modulation', 'of', 'the', 'Ca2+', 'homeostasis', 'in', 'skeletal', 'muscle', 'cells.', 'Myotonic', 'dystrophy', '(', 'DM', ')', ',', 'the', 'most', 'prevalent', 'muscular', 'disorder', 'in', 'adults', ',', 'is', 'caused', 'by', '(', 'CTG', ')', 'n-repeat', 'expansion', 'in', 'a', 'gene', 'encoding', 'a', 'protein', 'kinase', '(', 'DM', 'protein', 'kinase', ';', 'DMPK', ')', 'and', 'involves', 'changes', 'in', 'cytoarchitecture', 'and', 'ion', 'homeostasis', '.', 'To', 'obtain', 'clues', 'to', 'the', 'normal', 'biological', 'role', 'of', 'DMPK', 'in', 'cellular', 'ion', 'homeostasis', ',', 'we', 'have', 'compared', 'the', 'resting', '[', 'Ca2', '+', ']', 'i', ',', 'the', 'amplitude', 'and', 'shape', 'of', 'depolarization-induced', 'Ca2', '+', 'transients', ',', 'and', 'the', 'content', 'of', 'ATP-driven', 'ion', 'pumps', 'in', 'cultured', 'skeletal', 'muscle', 'cells', 'of', 'wild-type', 'and', 'DMPK', '[', '-', '/', '-', ']', 'knockout', 'mice', '.', 'In', 'vitro-differentiated', 'DMPK', '[', '-', '/', '-', ']', 'myotubes', 'exhibit', 'a', 'higher', 'resting', '[', 'Ca2', '+', ']', 'i', 'than', 'do', 'wild-type', 'myotubes', 'because', 'of', 'an', 'altered', 'open', 'probability', 'of', 'voltage-dependent', 'l-type', 'Ca2', '+', 'and', 'Na', '+', 'channels', '.', 'The', 'mutant', 'myotubes', 'exhibit', 'smaller', 'and', 'slower', 'Ca2', '+', 'responses', 'upon', 'triggering', 'by', 'acetylcholine', 'or', 'high', 'external', 'K', '+', '.', 'In', 'addition', ',', 'we', 'observed', 'that', 'these', 'Ca2', '+', 'transients', 'partially', 'result', 'from', 'an', 'influx', 'of', 'extracellular', 'Ca2', '+', 'through', 'the', 'l-type', 'Ca2', '+', 'channel', '.', 'Neither', 'the', 'content', 'nor', 'the', 'activity', 'of', 'Na', '+', '/', 'K', '+', 'ATPase', 'and', 'sarcoplasmic', 'reticulum', 'Ca2', '+', '-ATPase', 'are', 'affected', 'by', 'DMPK', 'absence', '.', 'In', 'conclusion', ',', 'our', 'data', 'suggest', 'that', 'DMPK', 'is', 'involved', 'in', 'modulating', 'the', 'initial', 'events', 'of', 'excitation-contraction', 'coupling', 'in', 'skeletal', 'muscle', '.', '.']\n",
      "[NeMo I 2024-05-22 02:02:50 text_classification_dataset:240] subtokens: [CLS] my ##oton ##ic d ##yst ##rop ##hy protein kinase is involved in the modulation of the ca ##2 + home ##osta ##sis in skeletal muscle cells . my ##oton ##ic d ##yst ##rop ##hy ( d ##m ) , the most prevalent muscular disorder in adults , is caused by ( ct ##g ) n - repeat expansion in a gene encoding a protein kinase ( d ##m protein kinase ; d ##mp ##k ) and involves changes in cy ##to ##ar ##chi ##tec ##ture and ion home ##osta ##sis . to obtain clues to the normal biological role of d ##mp ##k in cellular ion home ##osta ##sis , we have compared the resting [ ca ##2 + ] i , the amplitude and shape of [SEP]\n",
      "[NeMo I 2024-05-22 02:02:50 text_classification_dataset:241] input_ids: 101 2026 25862 2594 1040 27268 18981 10536 5250 21903 2003 2920 1999 1996 25502 1997 1996 6187 2475 1009 2188 28696 6190 1999 20415 6740 4442 1012 2026 25862 2594 1040 27268 18981 10536 1006 1040 2213 1007 1010 1996 2087 15157 13472 8761 1999 6001 1010 2003 3303 2011 1006 14931 2290 1007 1050 1011 9377 4935 1999 1037 4962 17181 1037 5250 21903 1006 1040 2213 5250 21903 1025 1040 8737 2243 1007 1998 7336 3431 1999 22330 3406 2906 5428 26557 11244 1998 10163 2188 28696 6190 1012 2000 6855 15774 2000 1996 3671 6897 2535 1997 1040 8737 2243 1999 12562 10163 2188 28696 6190 1010 2057 2031 4102 1996 8345 1031 6187 2475 1009 1033 1045 1010 1996 22261 1998 4338 1997 102\n",
      "[NeMo I 2024-05-22 02:02:50 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2024-05-22 02:02:50 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2024-05-22 02:02:50 text_classification_dataset:244] label: 1\n",
      "[NeMo W 2024-05-22 02:02:50 text_classification_dataset:250] Found 10 out of 10 sentences with more than 128 subtokens. Truncated long sentences from the end.\n",
      "[NeMo I 2024-05-22 02:02:50 data_preprocessing:299] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2024-05-22 02:02:50 data_preprocessing:301] Min: 129 |                  Max: 129 |                  Mean: 129.0 |                  Median: 129.0\n",
      "[NeMo I 2024-05-22 02:02:50 data_preprocessing:307] 75 percentile: 129.00\n",
      "[NeMo I 2024-05-22 02:02:50 data_preprocessing:308] 99 percentile: 129.00\n",
      "[NeMo W 2024-05-22 02:02:50 modelPT:197] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact forit has already been registered.\n",
      "[NeMo W 2024-05-22 02:02:50 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/nemo/core/classes/modelPT.py:243: UserWarning: update_node() is deprecated, use OmegaConf.update(). (Since 2.0)\n",
      "      self.cfg.update_node(config_path, return_path)\n",
      "    \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertEncoder: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[NeMo I 2024-05-22 02:02:52 text_classification_with_bert:118] ===========================================================================================\n",
      "[NeMo I 2024-05-22 02:02:52 text_classification_with_bert:119] Starting training...\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[NeMo I 2024-05-22 02:02:53 modelPT:748] Optimizer config = Adam (\n",
      "    Parameter Group 0\n",
      "        amsgrad: False\n",
      "        betas: [0.9, 0.999]\n",
      "        eps: 1e-08\n",
      "        lr: 2e-05\n",
      "        weight_decay: 0.01\n",
      "    )\n",
      "[NeMo I 2024-05-22 02:02:53 lr_scheduler:617] Scheduler \"<nemo.core.optim.lr_scheduler.WarmupAnnealing object at 0x7fbddfb30190>\" \n",
      "    will be used during training (effective maximum steps = 33) - \n",
      "    Parameters : \n",
      "    (warmup_steps: null\n",
      "    warmup_ratio: 0.1\n",
      "    last_epoch: -1\n",
      "    max_steps: 33\n",
      "    )\n",
      "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "\n",
      "  | Name                  | Type                 | Params\n",
      "---------------------------------------------------------------\n",
      "0 | loss                  | CrossEntropyLoss     | 0     \n",
      "1 | bert_model            | BertEncoder          | 109 M \n",
      "2 | classifier            | SequenceClassifier   | 592 K \n",
      "3 | classification_report | ClassificationReport | 0     \n",
      "---------------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "440.301   Total estimated model params size (MB)\n",
      "Epoch 0:   0%|                                           | 0/13 [00:00<?, ?it/s]\n",
      "[NeMo W 2024-05-22 02:02:55 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/nemo/core/classes/modelPT.py:308: UserWarning: update_node() is deprecated, use OmegaConf.update(). (Since 2.0)\n",
      "      conf.update_node(conf_path, item.path)\n",
      "    \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 637, in run_train\n",
      "    self.train_loop.run_training_epoch()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 492, in run_training_epoch\n",
      "    batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 654, in run_training_batch\n",
      "    self.optimizer_step(optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 425, in optimizer_step\n",
      "    model_ref.optimizer_step(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py\", line 1390, in optimizer_step\n",
      "    optimizer.step(closure=optimizer_closure)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py\", line 214, in step\n",
      "    self.__optimizer_step(*args, closure=closure, profiler_name=profiler_name, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py\", line 134, in __optimizer_step\n",
      "    trainer.accelerator.optimizer_step(optimizer, self._optimizer_idx, lambda_closure=closure, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 277, in optimizer_step\n",
      "    self.run_optimizer_step(optimizer, opt_idx, lambda_closure, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 282, in run_optimizer_step\n",
      "    self.training_type_plugin.optimizer_step(optimizer, lambda_closure=lambda_closure, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 163, in optimizer_step\n",
      "    optimizer.step(closure=lambda_closure, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py\", line 65, in wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/optim/optimizer.py\", line 88, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/optim/adam.py\", line 66, in step\n",
      "    loss = closure()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 648, in train_step_and_backward_closure\n",
      "    result = self.training_step_and_backward(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 742, in training_step_and_backward\n",
      "    result = self.training_step(split_batch, batch_idx, opt_idx, hiddens)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 293, in training_step\n",
      "    training_step_output = self.trainer.accelerator.training_step(args)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 156, in training_step\n",
      "    return self.training_type_plugin.training_step(*args)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp.py\", line 312, in training_step\n",
      "    return self.model(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py\", line 707, in forward\n",
      "    output = self.module(*inputs[0], **kwargs[0])\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/overrides/base.py\", line 48, in forward\n",
      "    output = self.module.training_step(*inputs, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/nemo/utils/model_utils.py\", line 338, in wrap_training_step\n",
      "    output_dict = wrapped(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/nemo/collections/nlp/models/text_classification/text_classification_model.py\", line 118, in training_step\n",
      "    logits = self.forward(input_ids=input_ids, token_type_ids=input_type_ids, attention_mask=input_mask)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/nemo/core/classes/common.py\", line 791, in __call__\n",
      "    outputs = wrapped(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/nemo/collections/nlp/models/text_classification/text_classification_model.py\", line 105, in forward\n",
      "    hidden_states = self.bert_model(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/nemo/core/classes/common.py\", line 791, in __call__\n",
      "    outputs = wrapped(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/nemo/collections/nlp/modules/common/huggingface/bert.py\", line 32, in forward\n",
      "    res = super().forward(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)[0]\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\", line 971, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\", line 568, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\", line 456, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\", line 387, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\", line 309, in forward\n",
      "    attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 9.53 GiB total capacity; 4.31 GiB already allocated; 99.94 MiB free; 4.51 GiB reserved in total by PyTorch)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"nemo/examples/nlp/text_classification/text_classification_with_bert.py\", line 120, in main\n",
      "    trainer.fit(model)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 499, in fit\n",
      "    self.dispatch()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 546, in dispatch\n",
      "    self.accelerator.start_training(self)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 73, in start_training\n",
      "    self.training_type_plugin.start_training(trainer)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/nemo/collections/nlp/parts/nlp_overrides.py\", line 108, in start_training\n",
      "    return super().start_training(trainer)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 114, in start_training\n",
      "    self._results = trainer.run_train()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 670, in run_train\n",
      "    self.train_loop.on_train_end()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 138, in on_train_end\n",
      "    self.trainer.call_hook(\"on_train_end\")\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1095, in call_hook\n",
      "    trainer_hook(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/callback_hook.py\", line 125, in on_train_end\n",
      "    callback.on_train_end(self, self.lightning_module)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py\", line 48, in wrapped_fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/nemo/utils/exp_manager.py\", line 646, in on_train_end\n",
      "    pl_module.save_to(save_path=os.path.join(self.dirpath, self.nemo_prefix + self.postfix))\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/nemo/collections/nlp/models/nlp_model.py\", line 269, in save_to\n",
      "    return super().save_to(save_path)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py\", line 48, in wrapped_fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/nemo/core/classes/modelPT.py\", line 354, in save_to\n",
      "    self._default_save_to(save_path)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/nemo/collections/nlp/models/nlp_model.py\", line 324, in _default_save_to\n",
      "    return super()._default_save_to(save_path)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/nemo/core/classes/modelPT.py\", line 334, in _default_save_to\n",
      "    self._make_nemo_file_from_folder(filename=save_path, source_dir=tmpdir)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/nemo/collections/nlp/models/nlp_model.py\", line 455, in _make_nemo_file_from_folder\n",
      "    return super(NLPModel, NLPModel)._make_nemo_file_from_folder(filename, source_dir)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/nemo/core/classes/modelPT.py\", line 1269, in _make_nemo_file_from_folder\n",
      "    with tarfile.open(filename, \"w:gz\") as tar:\n",
      "  File \"/opt/conda/lib/python3.8/tarfile.py\", line 1621, in open\n",
      "    return func(name, filemode, fileobj, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/tarfile.py\", line 1667, in gzopen\n",
      "    fileobj = GzipFile(name, mode + \"b\", compresslevel, fileobj)\n",
      "  File \"/opt/conda/lib/python3.8/gzip.py\", line 173, in __init__\n",
      "    fileobj = self.myfileobj = builtins.open(filename, mode or 'rb')\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/dli/task/Downloads/nvidia-dli-llm/part_2/data_text_output/TextClassification/2024-05-22_02-02-40/checkpoints/TextClassification.nemo'\n",
      "\n",
      "Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n",
      "CPU times: user 186 ms, sys: 55.8 ms, total: 242 ms\n",
      "Wall time: 20 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# The training takes about 3 minutes to run\n",
    "\n",
    "TC_DIR = \"nemo/examples/nlp/text_classification\"\n",
    "\n",
    "# set the values we want to override\n",
    "NUM_CLASSES = 3\n",
    "MAX_SEQ_LENGTH = 128\n",
    "PATH_TO_TRAIN_FILE = \"data/NCBI_tc-3/train_nemo_format.tsv\"\n",
    "PATH_TO_VAL_FILE = \"data/NCBI_tc-3/dev_nemo_format.tsv\"\n",
    "PATH_TO_TEST_FILE = \"data/NCBI_tc-3/test_nemo_format.tsv\"\n",
    "# disease domain inference sample answers should be 0, 1, 2 \n",
    "INFER_SAMPLES_0 = \"In contrast no mutations were detected in the p53 gene suggesting that this tumour suppressor is not frequently altered in this leukaemia \"\n",
    "INFER_SAMPLES_1 = \"The first predictive testing for Huntington disease  was based on analysis of linked polymorphic DNA markers to estimate the likelihood of inheriting the mutation for HD\"\n",
    "INFER_SAMPLES_2 = \"Further studies suggested that low dilutions of C5D serum contain a factor or factors interfering at some step in the hemolytic assay of C5 rather than a true C5 inhibitor or inactivator\"\n",
    "MAX_EPOCHS = 3\n",
    "\n",
    "# Run the training script, overriding the config values in the command line\n",
    "!python $TC_DIR/text_classification_with_bert.py \\\n",
    "        model.dataset.num_classes=$NUM_CLASSES \\\n",
    "        model.dataset.max_seq_length=$MAX_SEQ_LENGTH \\\n",
    "        model.train_ds.file_path=$PATH_TO_TRAIN_FILE \\\n",
    "        model.validation_ds.file_path=$PATH_TO_VAL_FILE \\\n",
    "        model.test_ds.file_path=$PATH_TO_TEST_FILE \\\n",
    "        model.infer_samples=[\"$INFER_SAMPLES_0\",\"$INFER_SAMPLES_1\",\"$INFER_SAMPLES_2\"] \\\n",
    "        trainer.max_epochs=$MAX_EPOCHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the start of each training experiment, there is a printed log of the experiment specification including any parameters added or overridden via the command-line. It also shows additional information, such as which GPUs are available, where logs are saved, and some samples from the datasets with their corresponding inputs to the model. The log also provides some stats on the lengths of sequences in the dataset.\n",
    "\n",
    "After each epoch, there is a summary table of metrics on the validation set which includes precision, recall, and f1 score. The f1 score takes both false positives and false negatives into account and is considered more useful than simple accuracy. \n",
    "\n",
    "At the end of training, NeMo saves the last checkpoint at the path specified by `model.nemo_file_path`.  Since we left this value at the default, it should have been written to our workspace in `.nemo` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '*.nemo': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls *.nemo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results you achieved in the experiment may not have been very good.  However, it is pretty easy to try another experiment with just a few changes.  Longer training, adjusted learning rate, and changing the batch size for the training and validation datasets can improve results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.4 Exercise: Run an Experiment\n",
    "Try running another similar experiment using the same text classification problem, this time with some suggested improvements:\n",
    "  \n",
    "* Set the mixed-precision `amp_level` to \"O1\" with a `precision` of 16 to make the model run faster with little or no reduction in accuracy.\n",
    "* Adjust the number of epochs upward a bit to improve results (larger `max_epochs`)\n",
    "* Increase the learning rate a little to allow more rapid response to the estimated error each time the model weights are updated\n",
    "\n",
    "The new values have been provided for you in the cell below.  Add the command with appropriate overrides and run the cell.  If you get stuck, refer to the [solution](solutions/ex2.2.4.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 8.58 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# The training takes about 3 minutes to run\n",
    "\n",
    "TC_DIR = \"nemo/examples/nlp/text_classification\"\n",
    "\n",
    "# set the values we want to override\n",
    "NUM_CLASSES = 3\n",
    "MAX_SEQ_LENGTH = 128\n",
    "PATH_TO_TRAIN_FILE = \"data/NCBI_tc-3/train_nemo_format.tsv\"\n",
    "PATH_TO_VAL_FILE = \"data/NCBI_tc-3/dev_nemo_format.tsv\"\n",
    "PATH_TO_TEST_FILE = \"data/NCBI_tc-3/test_nemo_format.tsv\"\n",
    "# disease domain inference sample answers should be 0, 1, 2 \n",
    "INFER_SAMPLES_0 = \"In contrast no mutations were detected in the p53 gene suggesting that this tumour suppressor is not frequently altered in this leukaemia \"\n",
    "INFER_SAMPLES_1 = \"The first predictive testing for Huntington disease  was based on analysis of linked polymorphic DNA markers to estimate the likelihood of inheriting the mutation for HD\"\n",
    "INFER_SAMPLES_2 = \"Further studies suggested that low dilutions of C5D serum contain a factor or factors interfering at some step in the hemolytic assay of C5 rather than a true C5 inhibitor or inactivator\"\n",
    "MAX_EPOCHS = 5\n",
    "AMP_LEVEL = 'O1'\n",
    "PRECISION = 16\n",
    "LR = 5.0e-05\n",
    "\n",
    "# Override the config values in the command line\n",
    "# FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did the result from this experiment compare to the previous one?  Check the F1 scores and inference results in the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.5 Visualize the Results with TensorBoard\n",
    "The [experiment manager](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/core/core.html?highlight=tensorboard#experiment-manager) saves results for viewing with TensorBoard. <br>\n",
    "Take a look by opening [TensorBoard](/tensorboard/) for your instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "To compare the performance of the models you've run, select the \"train loss\" scaler.  You can see all the models you've run compared together or select individual models for comparison.  The example below shows the first experiment in orange and the exercise experiment in blue.  You can see that the loss was smaller in the second experiment.\n",
    "\n",
    "<img src=\"images/tensorboard_01.png\" width=1000px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.6 Exercise: Change the Language Model\n",
    "So far, you've used the basic `bert-base-uncased` language model, but that is just one of many you could try.  Run the following cell to see what language models are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['megatron-bert-345m-uncased',\n",
       " 'megatron-bert-345m-cased',\n",
       " 'megatron-bert-uncased',\n",
       " 'megatron-bert-cased',\n",
       " 'biomegatron-bert-345m-uncased',\n",
       " 'biomegatron-bert-345m-cased',\n",
       " 'bert-base-uncased',\n",
       " 'bert-large-uncased',\n",
       " 'bert-base-cased',\n",
       " 'bert-large-cased',\n",
       " 'bert-base-multilingual-uncased',\n",
       " 'bert-base-multilingual-cased',\n",
       " 'bert-base-chinese',\n",
       " 'bert-base-german-cased',\n",
       " 'bert-large-uncased-whole-word-masking',\n",
       " 'bert-large-cased-whole-word-masking',\n",
       " 'bert-large-uncased-whole-word-masking-finetuned-squad',\n",
       " 'bert-large-cased-whole-word-masking-finetuned-squad',\n",
       " 'bert-base-cased-finetuned-mrpc',\n",
       " 'bert-base-german-dbmdz-cased',\n",
       " 'bert-base-german-dbmdz-uncased',\n",
       " 'cl-tohoku/bert-base-japanese',\n",
       " 'cl-tohoku/bert-base-japanese-whole-word-masking',\n",
       " 'cl-tohoku/bert-base-japanese-char',\n",
       " 'cl-tohoku/bert-base-japanese-char-whole-word-masking',\n",
       " 'TurkuNLP/bert-base-finnish-cased-v1',\n",
       " 'TurkuNLP/bert-base-finnish-uncased-v1',\n",
       " 'wietsedv/bert-base-dutch-cased',\n",
       " 'distilbert-base-uncased',\n",
       " 'distilbert-base-uncased-distilled-squad',\n",
       " 'distilbert-base-cased',\n",
       " 'distilbert-base-cased-distilled-squad',\n",
       " 'distilbert-base-german-cased',\n",
       " 'distilbert-base-multilingual-cased',\n",
       " 'distilbert-base-uncased-finetuned-sst-2-english',\n",
       " 'roberta-base',\n",
       " 'roberta-large',\n",
       " 'roberta-large-mnli',\n",
       " 'distilroberta-base',\n",
       " 'roberta-base-openai-detector',\n",
       " 'roberta-large-openai-detector',\n",
       " 'albert-base-v1',\n",
       " 'albert-large-v1',\n",
       " 'albert-xlarge-v1',\n",
       " 'albert-xxlarge-v1',\n",
       " 'albert-base-v2',\n",
       " 'albert-large-v2',\n",
       " 'albert-xlarge-v2',\n",
       " 'albert-xxlarge-v2']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# complete list of supported BERT-like models\n",
    "from nemo.collections import nlp as nemo_nlp\n",
    "nemo_nlp.modules.get_pretrained_lm_models_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, choose a new language model, such as `megatron-bert-345m-cased`.  \n",
    "\n",
    "You may need to restart the notebook kernal to clear memory.  If you use a large model, other ways to save GPU memory space are to reduce the `batch_size` to 32, 16, or even 8 and reduce the `max_seq_length` to 64. There is no right answer to this exercise.  Rather, this is an opportunity for you to experiment.  Some of the models can take several minutes to run, so feel free to move on to the next notebook and return here when you have time later. If you get stuck, take a look at an example [solution](solutions/ex2.2.6.ipynb).  Be sure to take note of the loss and f1 results with this model, or check TensorBoard for a visualization of the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Restart the kernel\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO Try your own experiment with a different language model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2.3 PyTorch Lightning Model and Trainer Workflow\n",
    "The NeMo model script is the quickest way to get up and running.  Sometimes, though, you may want to create your own script or work through your project in a more customized manner.  In that case, you can step through the PyTorch Lightning workflow, which is otherwise abstracted (encapsulated and hidden) within the model training script. We'll take a look at the script, then try working through the same workflow from scratch without the script or Hydra.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.1 Script Key Features\n",
    "You can open the [text_classification_with_bert.py](nemo/examples/nlp/text_classification/text_classification_with_bert.py) script to see exactly what is happening.  \n",
    "\n",
    "Here's an abbreviated version with logging and initial comments removed:\n",
    "\n",
    "```python\n",
    "import pytorch_lightning as pl\n",
    "from omegaconf import DictConfig\n",
    "\n",
    "from nemo.collections.nlp.models.text_classification import TextClassificationModel\n",
    "from nemo.collections.nlp.parts.nlp_overrides import NLPDDPPlugin\n",
    "from nemo.core.config import hydra_runner\n",
    "from nemo.utils.exp_manager import exp_manager\n",
    "\n",
    "\n",
    "@hydra_runner(config_path=\"conf\", config_name=\"text_classification_config\")\n",
    "def main(cfg: DictConfig) -> None:\n",
    "    trainer = pl.Trainer(plugins=[NLPDDPPlugin()], **cfg.trainer)\n",
    "    exp_manager(trainer, cfg.get(\"exp_manager\", None))\n",
    "\n",
    "    if not cfg.model.train_ds.file_path:\n",
    "        raise ValueError(\"'train_ds.file_path' need to be set for the training!\")\n",
    "\n",
    "    model = TextClassificationModel(cfg.model, trainer=trainer)\n",
    "    trainer.fit(model)\n",
    "\n",
    "    if cfg.model.nemo_path:\n",
    "        # '.nemo' file contains the last checkpoint and the params to initialize the model\n",
    "        model.save_to(cfg.model.nemo_path)\n",
    "\n",
    "    # We evaluate the trained model on the test set if test_ds is set in the config file\n",
    "    if cfg.model.test_ds.file_path:\n",
    "        trainer.test(model=model, ckpt_path=None, verbose=False)\n",
    "\n",
    "    # perform inference on a list of queries.\n",
    "    if \"infer_samples\" in cfg.model and cfg.model.infer_samples:       \n",
    "        # max_seq_length=512 is the maximum length BERT supports.\n",
    "        results = model.classifytext(queries=cfg.model.infer_samples, batch_size=16, max_seq_length=512)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "```\n",
    "The Hydra decorator, `@hydra_runner`, connects the configuration file and provides the mechanism for the command line overrides. \n",
    "\n",
    "Once the configuration is established, the key steps are:\n",
    "1. Instantiate the trainer with `trainer = pl.Trainer(plugins=[NLPDDPPlugin()], **cfg.trainer)`\n",
    "1. Instantiate the model with `model = TextClassificationModel(cfg.model, trainer=trainer)`\n",
    "1. Train the model with `trainer.fit(model)`\n",
    "\n",
    "Additional steps for optional inference and evaluation are:\n",
    "* Evaluate with `trainer.test(model=model, ckpt_path=None, verbose=False)`\n",
    "* Infer with `results = model.classifytext(queries=cfg.model.infer_samples, batch_size=16, max_seq_length=512)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.3.2 Model Training from Scratch\n",
    "Execute the following cell to restart the notebook kernel to clear variables and GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart the kernel\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the `nemo_nlp` collection, the experiment manager, PyTorch Lightning, and OmegaConf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections import nlp as nemo_nlp\n",
    "from nemo.utils.exp_manager import exp_manager\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running these training steps manually without the script or Hydra, the correct configuration must be set prior to instantiation.  We've already determined the changes we want to make to the config file for this project.  Previously, we used the Hydra override feature in the command line, but those changes are made this time using `OmegaConf`. The syntax looks similar, except this time we are directly changing the `OmegaConf` object, `config`, in Python, and will pass that object to `trainer`, `exp_manager`, and `model`.\n",
    "\n",
    "The default language model is `bert-base-uncased`.  To override it, add to the cell (for example):\n",
    "```python\n",
    "    PRETRAINED_MODEL_NAME = 'bert-base-cased'\n",
    "    config.model.language_model.pretrained_model_name=PRETRAINED_MODEL_NAME\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the OmegaConf object by loading the config file\n",
    "TC_DIR = \"nemo/examples/nlp/text_classification\"\n",
    "CONFIG_FILE = \"text_classification_config.yaml\"\n",
    "config = OmegaConf.load(TC_DIR + \"/conf/\" + CONFIG_FILE)\n",
    "\n",
    "# set the values we want to change\n",
    "NUM_CLASSES = 3\n",
    "MAX_SEQ_LENGTH = 128\n",
    "PATH_TO_TRAIN_FILE = \"data/NCBI_tc-3/train_nemo_format.tsv\"\n",
    "PATH_TO_VAL_FILE = \"data/NCBI_tc-3/dev_nemo_format.tsv\"\n",
    "PATH_TO_TEST_FILE = \"data/NCBI_tc-3/test_nemo_format.tsv\"\n",
    "# disease domain inference sample answers should be 0, 1, 2 \n",
    "INFER_SAMPLES = [\"Germline mutations in BRCA1 are responsible for most cases of inherited breast and ovarian cancer \",\n",
    "        \"The first predictive testing for Huntington disease  was based on analysis of linked polymorphic DNA markers to estimate the likelihood of inheriting the mutation for HD\",\n",
    "        \"Further studies suggested that low dilutions of C5D serum contain a factor or factors interfering at some step in the hemolytic assay of C5 rather than a true C5 inhibitor or inactivator\"\n",
    "        ]\n",
    "MAX_EPOCHS = 5\n",
    "AMP_LEVEL = 'O1'\n",
    "PRECISION = 16\n",
    "LR = 5.0e-05\n",
    "\n",
    "# set the config values using omegaconf\n",
    "config.model.dataset.num_classes = NUM_CLASSES\n",
    "config.model.dataset.max_seq_length = MAX_SEQ_LENGTH\n",
    "config.model.train_ds.file_path = PATH_TO_TRAIN_FILE\n",
    "config.model.validation_ds.file_path = PATH_TO_VAL_FILE\n",
    "config.model.test_ds.file_path = PATH_TO_TEST_FILE\n",
    "config.model.infer_samples = INFER_SAMPLES\n",
    "config.trainer.max_epochs = MAX_EPOCHS\n",
    "config.trainer.amp_level = AMP_LEVEL\n",
    "config.trainer.precision = PRECISION\n",
    "config.model.optim.lr = LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that `config` has been updated with the correct values, instantiate the trainer and experiment manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Using native 16bit precision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-22 02:04:18 exp_manager:216] Experiments will be logged at data_text_output/TextClassification/2024-05-22_02-04-18\n",
      "[NeMo I 2024-05-22 02:04:18 exp_manager:563] TensorboardLogger has been set up\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('data_text_output/TextClassification/2024-05-22_02-04-18')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the trainer and experiment manager\n",
    "trainer = pl.Trainer(**config.trainer)\n",
    "exp_manager(trainer, config.exp_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-22 02:04:27 text_classification_dataset:120] Read 683 examples from data/NCBI_tc-3/train_nemo_format.tsv.\n",
      "[NeMo I 2024-05-22 02:04:27 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2024-05-22 02:04:27 text_classification_dataset:239] example 0: ['Detection', 'of', 'a', 'novel', 'missense', 'mutation', 'and', 'second', 'recurrent', 'mutation', 'in', 'the', 'CACNA1A', 'gene', 'in', 'individuals', 'with', 'EA-2', 'and', 'FHM.', 'Mutations', 'in', 'the', 'brain', 'specific', 'P', '/', 'Q', 'type', 'Ca2', '+', 'channel', 'alpha1', 'subunit', 'gene', ',', 'CACNA1A', ',', 'have', 'been', 'identified', 'in', 'three', 'clinically', 'distinct', 'disorders', ',', 'viz', '.', 'episodic', 'ataxia', 'type', '2', '(', 'EA-2', ')', ',', 'familial', 'hemiplegic', 'migraine', '(', 'FHM', ')', 'and', 'spinocerebellar', 'ataxia', '6', '(', 'SCA6', ')', '.', 'For', 'individuals', 'with', 'EA-2', ',', 'the', 'mutations', 'described', 'thus', 'far', 'are', 'presumed', 'to', 'result', 'in', 'a', 'truncated', 'protein', 'product', '.', 'Several', 'different', 'missense', 'mutations', 'have', 'been', 'identified', 'in', 'patients', 'with', 'FHM', '.', 'At', 'least', 'two', 'of', 'these', 'mutations', 'have', 'been', 'identified', 'on', 'two', 'different', 'chromosome', '19p13', 'haplotypes', 'and', 'thus', 'represent', 'recurrent', 'mutations', '.', 'In', 'the', 'present', 'study', ',', 'we', 'have', 'screened', 'several', 'individuals', 'for', 'mutations', 'in', 'all', '47', 'exons', 'in', 'the', 'CACNA1A', 'gene', 'by', 'single-strand', 'conformation', 'analysis', '.', 'We', 'have', 'characterised', 'a', 'novel', 'missense', 'mutation', ',', 'G5260A', ',', 'in', 'exon', '32', 'in', 'a', 'family', 'segregating', 'for', 'EA-2', '.', 'The', 'consequence', 'of', 'this', 'mutation', 'is', 'an', 'amino', 'acid', 'substitution', 'at', 'a', 'highly', 'conserved', 'position', 'within', 'the', 'CACNA1A', 'gene', '.', 'This', 'represents', 'the', 'first', 'point', 'mutation', 'not', 'resulting', 'in', 'a', 'proposed', 'truncated', 'protein', '.', 'Furthermore', ',', 'this', 'mutation', 'has', 'been', 'detected', 'in', 'a', 'family', 'member', 'with', 'mild', 'clinical', 'signs', 'including', 'only', 'migraine', '.', 'Additionally', ',', 'a', 'second', 'previously', 'identified', 'recurrent', 'muta', 'tion', ',', 'C2272T', ',', 'in', 'exon', '16', 'has', 'been', 'discovered', 'in', 'a', 'patient', 'with', 'FHM', '.', '.']\n",
      "[NeMo I 2024-05-22 02:04:27 text_classification_dataset:240] subtokens: [CLS] detection of a novel miss ##ense mutation and second rec ##urrent mutation in the ca ##c ##na ##1 ##a gene in individuals with ea - 2 and f ##hm . mutations in the brain specific p / q type ca ##2 + channel alpha ##1 subunit gene , ca ##c ##na ##1 ##a , have been identified in three clinical ##ly distinct disorders , viz . ep ##iso ##dic ata ##xia type 2 ( ea - 2 ) , fa ##mi ##lia ##l hem ##ip ##leg ##ic mig ##raine ( f ##hm ) and spin ##oc ##ere ##bella ##r ata ##xia 6 ( sc ##a ##6 ) . for individuals with ea - 2 , the mutations described thus far are presumed to result in a truncated [SEP]\n",
      "[NeMo I 2024-05-22 02:04:27 text_classification_dataset:241] input_ids: 101 10788 1997 1037 3117 3335 16700 16221 1998 2117 28667 29264 16221 1999 1996 6187 2278 2532 2487 2050 4962 1999 3633 2007 19413 1011 1016 1998 1042 14227 1012 14494 1999 1996 4167 3563 1052 1013 1053 2828 6187 2475 1009 3149 6541 2487 24312 4962 1010 6187 2278 2532 2487 2050 1010 2031 2042 4453 1999 2093 6612 2135 5664 10840 1010 26619 1012 4958 19565 14808 29533 14787 2828 1016 1006 19413 1011 1016 1007 1010 6904 4328 6632 2140 19610 11514 23115 2594 19117 26456 1006 1042 14227 1007 1998 6714 10085 7869 21700 2099 29533 14787 1020 1006 8040 2050 2575 1007 1012 2005 3633 2007 19413 1011 1016 1010 1996 14494 2649 2947 2521 2024 14609 2000 2765 1999 1037 25449 102\n",
      "[NeMo I 2024-05-22 02:04:27 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2024-05-22 02:04:27 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2024-05-22 02:04:27 text_classification_dataset:244] label: 1\n",
      "[NeMo I 2024-05-22 02:04:27 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2024-05-22 02:04:27 text_classification_dataset:239] example 1: ['A', 'new', 'CT', 'pattern', 'in', 'adrenoleukodystrophy.', 'A', 'new', 'CT', 'pattern', 'was', 'observed', 'in', '2', 'patients', 'with', 'adrenoleukodystrophy', '(', 'ALD', ')', '.', 'This', 'pattern', ',', 'which', 'the', 'authors', 'call', 'Type', 'II', ',', 'is', 'characterized', 'by', 'the', 'absence', 'of', 'posterior', 'periventricular', 'areas', 'of', 'decreased', 'attenuation', 'around', 'the', 'trigone', 'on', 'non-contrast', 'scans', 'after', 'contrast', 'infusion', ',', 'however', ',', 'there', 'is', 'striking', 'enhancement', 'of', 'various', 'white-matter', 'structures', '(', 'tracts', 'or', 'fiber', 'systems', ')', 'such', 'as', 'the', 'internal', 'capsules', ',', 'corpus', 'callosum', ',', 'corona', 'radiata', ',', 'forceps', 'major', ',', 'and', 'cerebral', 'peduncles', '.', 'This', 'is', 'different', 'from', 'numerous', 'previous', 'descriptions', 'of', 'the', 'CT', 'pattern', 'in', 'ALD', '.', 'Type', 'II', 'ALD', 'does', 'not', 'appear', 'to', 'have', 'been', 'seen', 'in', 'any', 'other', 'leukoencephalopathy', 'and', 'is', 'probably', 'specific', 'for', 'a', 'phenotypic', 'variant', 'or', 'an', 'evolving', 'stage', 'of', 'ALD', '.', '.']\n",
      "[NeMo I 2024-05-22 02:04:27 text_classification_dataset:240] subtokens: [CLS] a new ct pattern in ad ##ren ##ole ##uk ##od ##yst ##rop ##hy . a new ct pattern was observed in 2 patients with ad ##ren ##ole ##uk ##od ##yst ##rop ##hy ( al ##d ) . this pattern , which the authors call type ii , is characterized by the absence of posterior per ##ive ##nt ##ric ##ular areas of decreased at ##ten ##uation around the tri ##gon ##e on non - contrast scans after contrast in ##fusion , however , there is striking enhancement of various white - matter structures ( tracts or fiber systems ) such as the internal capsule ##s , corpus call ##os ##um , corona ra ##dia ##ta , force ##ps major , and cerebral pe ##dun ##cles . this is [SEP]\n",
      "[NeMo I 2024-05-22 02:04:27 text_classification_dataset:241] input_ids: 101 1037 2047 14931 5418 1999 4748 7389 9890 6968 7716 27268 18981 10536 1012 1037 2047 14931 5418 2001 5159 1999 1016 5022 2007 4748 7389 9890 6968 7716 27268 18981 10536 1006 2632 2094 1007 1012 2023 5418 1010 2029 1996 6048 2655 2828 2462 1010 2003 7356 2011 1996 6438 1997 15219 2566 3512 3372 7277 7934 2752 1997 10548 2012 6528 14505 2105 1996 13012 7446 2063 2006 2512 1011 5688 27404 2044 5688 1999 20523 1010 2174 1010 2045 2003 8478 22415 1997 2536 2317 1011 3043 5090 1006 22069 2030 11917 3001 1007 2107 2004 1996 4722 18269 2015 1010 13931 2655 2891 2819 1010 21887 10958 9032 2696 1010 2486 4523 2350 1010 1998 18439 21877 27584 18954 1012 2023 2003 102\n",
      "[NeMo I 2024-05-22 02:04:27 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2024-05-22 02:04:27 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2024-05-22 02:04:27 text_classification_dataset:244] label: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-05-22 02:04:34 text_classification_dataset:250] Found 664 out of 683 sentences with more than 128 subtokens. Truncated long sentences from the end.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-22 02:04:34 data_preprocessing:299] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2024-05-22 02:04:34 data_preprocessing:301] Min: 74 |                  Max: 129 |                  Mean: 128.36163982430455 |                  Median: 129.0\n",
      "[NeMo I 2024-05-22 02:04:34 data_preprocessing:307] 75 percentile: 129.00\n",
      "[NeMo I 2024-05-22 02:04:34 data_preprocessing:308] 99 percentile: 129.00\n",
      "[NeMo I 2024-05-22 02:04:34 text_classification_dataset:120] Read 100 examples from data/NCBI_tc-3/dev_nemo_format.tsv.\n",
      "[NeMo I 2024-05-22 02:04:34 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2024-05-22 02:04:34 text_classification_dataset:239] example 0: ['BRCA1', 'is', 'secreted', 'and', 'exhibits', 'properties', 'of', 'a', 'granin.', 'Germline', 'mutations', 'in', 'BRCA1', 'are', 'responsible', 'for', 'most', 'cases', 'of', 'inherited', 'breast', 'and', 'ovarian', 'cancer', '.', 'However', ',', 'the', 'function', 'of', 'the', 'BRCA1', 'protein', 'has', 'remained', 'elusive', '.', 'We', 'now', 'show', 'that', 'BRCA1', 'encodes', 'a', '190-kD', 'protein', 'with', 'sequence', 'homology', 'and', 'biochemical', 'analogy', 'to', 'the', 'granin', 'protein', 'family', '.', 'Interestingly', ',', 'BRCA2', 'also', 'includes', 'a', 'motif', 'similar', 'to', 'the', 'granin', 'consensus', 'at', 'the', 'C', 'terminus', 'of', 'the', 'protein', '.', 'Both', 'BRCA1', 'and', 'the', 'granins', 'localize', 'to', 'secretory', 'vesicles', ',', 'are', 'secreted', 'by', 'a', 'regulated', 'pathway', ',', 'are', 'post-translationally', 'glycosylated', 'and', 'are', 'responsive', 'to', 'hormones', '.', 'As', 'a', 'regulated', 'secretory', 'protein', ',', 'BRCA1', 'appears', 'to', 'function', 'by', 'a', 'mechanism', 'not', 'previously', 'described', 'for', 'tumour', 'suppressor', 'gene', 'products', '.', '.']\n",
      "[NeMo I 2024-05-22 02:04:34 text_classification_dataset:240] subtokens: [CLS] br ##ca ##1 is secret ##ed and exhibits properties of a gran ##in . ge ##rm ##line mutations in br ##ca ##1 are responsible for most cases of inherited breast and o ##var ##ian cancer . however , the function of the br ##ca ##1 protein has remained elusive . we now show that br ##ca ##1 en ##codes a 190 - k ##d protein with sequence homo ##logy and bio ##chemical analogy to the gran ##in protein family . interesting ##ly , br ##ca ##2 also includes a motif similar to the gran ##in consensus at the c terminus of the protein . both br ##ca ##1 and the gran ##ins local ##ize to secret ##ory ve ##sic ##les , are secret ##ed by a regulated [SEP]\n",
      "[NeMo I 2024-05-22 02:04:34 text_classification_dataset:241] input_ids: 101 7987 3540 2487 2003 3595 2098 1998 10637 5144 1997 1037 12604 2378 1012 16216 10867 4179 14494 1999 7987 3540 2487 2024 3625 2005 2087 3572 1997 7900 7388 1998 1051 10755 2937 4456 1012 2174 1010 1996 3853 1997 1996 7987 3540 2487 5250 2038 2815 26475 1012 2057 2085 2265 2008 7987 3540 2487 4372 23237 1037 11827 1011 1047 2094 5250 2007 5537 24004 6483 1998 16012 15869 23323 2000 1996 12604 2378 5250 2155 1012 5875 2135 1010 7987 3540 2475 2036 2950 1037 16226 2714 2000 1996 12604 2378 10465 2012 1996 1039 7342 1997 1996 5250 1012 2119 7987 3540 2487 1998 1996 12604 7076 2334 4697 2000 3595 10253 2310 19570 4244 1010 2024 3595 2098 2011 1037 12222 102\n",
      "[NeMo I 2024-05-22 02:04:34 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2024-05-22 02:04:34 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2024-05-22 02:04:34 text_classification_dataset:244] label: 0\n",
      "[NeMo I 2024-05-22 02:04:34 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2024-05-22 02:04:34 text_classification_dataset:239] example 1: ['Ovarian', 'cancer', 'risk', 'in', 'BRCA1', 'carriers', 'is', 'modified', 'by', 'the', 'HRAS1', 'variable', 'number', 'of', 'tandem', 'repeat', '(VNTR)', 'locus.', 'Women', 'who', 'carry', 'a', 'mutation', 'in', 'the', 'BRCA1', 'gene', '(', 'on', 'chromosome', '17q21', ')', ',', 'have', 'an', '80', '%', 'risk', 'of', 'breast', 'cancer', 'and', 'a', '40', '%', 'risk', 'of', 'ovarian', 'cancer', 'by', 'the', 'age', 'of', '70', '(', 'ref', '.', '1', ')', '.', 'The', 'variable', 'penetrance', 'of', 'BRCA1', 'suggests', 'that', 'other', 'genetic', 'and', 'non-genetic', 'factors', 'play', 'a', 'role', 'in', 'tumourigenesis', 'in', 'these', 'individuals', '.', 'The', 'HRAS1', 'variable', 'number', 'of', 'tandem', 'repeats', '(', 'VNTR', ')', 'polymorphism', ',', 'located', '1', 'kilobase', '(', 'kb', ')', 'downstream', 'of', 'the', 'HRAS1', 'proto-oncogene', '(', 'chromosome', '11p15', '.', '5', ')', 'is', 'one', 'possible', 'genetic', 'modifier', 'of', 'cancer', 'penetrance', '.', 'Individuals', 'who', 'have', 'rare', 'alleles', 'of', 'the', 'VNTR', 'have', 'an', 'increased', 'risk', 'of', 'certain', 'types', 'of', 'cancers', ',', 'including', 'breast', 'cancer', '(', '2-4', ')', '.', 'To', 'investigate', 'whether', 'the', 'presence', 'of', 'rare', 'HRAS1', 'alleles', 'increases', 'susceptibility', 'to', 'hereditary', 'breast', 'and', 'ovarian', 'cancer', ',', 'we', 'have', 'typed', 'a', 'panel', 'of', '307', 'female', 'BRCA1', 'carriers', 'at', 'this', 'locus', 'using', 'a', 'PCR-based', 'technique', '.', 'The', 'risk', 'for', 'ovarian', 'cancer', 'was', '2', '.', '11', 'times', 'greater', 'for', 'BRCA1', 'carriers', 'harbouring', 'one', 'or', 'two', 'rare', 'HRAS1', 'alleles', ',', 'compared', 'to', 'carriers', 'with', 'only', 'common', 'alleles', '(', 'P', '=', '0', '.', '015', ')', '.', 'The', 'magnitude', 'of', 'the', 'relative', 'risk', 'associated', 'with', 'a', 'rare', 'HRAS1', 'allele', 'was', 'not', 'altered', 'by', 'adjusting', 'for', 'the', 'other', 'known', 'risk', 'factors', 'for', 'hereditary', 'ovarian', 'cancer', '(', '5', ')', '.', 'Susceptibility', 'to', 'breast', 'cancer', 'did', 'not', 'appear', 'to', 'be', 'affected', 'by', 'the', 'presence', 'of', 'rare', 'HRAS1', 'alleles', '.', 'This', 'study', 'is', 'the', 'first', 'to', 'show', 'the', 'effect', 'of', 'a', 'modifying', 'gene', 'on', 'the', 'penetrance', 'of', 'an', 'inherited', 'cancer', 'syndrome']\n",
      "[NeMo I 2024-05-22 02:04:34 text_classification_dataset:240] subtokens: [CLS] o ##var ##ian cancer risk in br ##ca ##1 carriers is modified by the hr ##as ##1 variable number of tandem repeat ( v ##nt ##r ) locus . women who carry a mutation in the br ##ca ##1 gene ( on chromosome 17 ##q ##21 ) , have an 80 % risk of breast cancer and a 40 % risk of o ##var ##ian cancer by the age of 70 ( ref . 1 ) . the variable pen ##et ##rance of br ##ca ##1 suggests that other genetic and non - genetic factors play a role in tu ##mour ##igen ##esis in these individuals . the hr ##as ##1 variable number of tandem repeats ( v ##nt ##r ) poly ##morphism , located 1 ki [SEP]\n",
      "[NeMo I 2024-05-22 02:04:34 text_classification_dataset:241] input_ids: 101 1051 10755 2937 4456 3891 1999 7987 3540 2487 11363 2003 6310 2011 1996 17850 3022 2487 8023 2193 1997 18231 9377 1006 1058 3372 2099 1007 25206 1012 2308 2040 4287 1037 16221 1999 1996 7987 3540 2487 4962 1006 2006 16706 2459 4160 17465 1007 1010 2031 2019 3770 1003 3891 1997 7388 4456 1998 1037 2871 1003 3891 1997 1051 10755 2937 4456 2011 1996 2287 1997 3963 1006 25416 1012 1015 1007 1012 1996 8023 7279 3388 21621 1997 7987 3540 2487 6083 2008 2060 7403 1998 2512 1011 7403 5876 2377 1037 2535 1999 10722 20360 29206 19009 1999 2122 3633 1012 1996 17850 3022 2487 8023 2193 1997 18231 17993 1006 1058 3372 2099 1007 26572 19539 1010 2284 1015 11382 102\n",
      "[NeMo I 2024-05-22 02:04:34 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2024-05-22 02:04:34 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2024-05-22 02:04:34 text_classification_dataset:244] label: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-05-22 02:04:35 text_classification_dataset:250] Found 99 out of 100 sentences with more than 128 subtokens. Truncated long sentences from the end.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-22 02:04:35 data_preprocessing:299] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2024-05-22 02:04:35 data_preprocessing:301] Min: 120 |                  Max: 129 |                  Mean: 128.91 |                  Median: 129.0\n",
      "[NeMo I 2024-05-22 02:04:35 data_preprocessing:307] 75 percentile: 129.00\n",
      "[NeMo I 2024-05-22 02:04:35 data_preprocessing:308] 99 percentile: 129.00\n",
      "[NeMo I 2024-05-22 02:04:35 text_classification_dataset:120] Read 10 examples from data/NCBI_tc-3/test_nemo_format.tsv.\n",
      "[NeMo I 2024-05-22 02:04:35 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2024-05-22 02:04:35 text_classification_dataset:239] example 0: ['Clustering', 'of', 'missense', 'mutations', 'in', 'the', 'ataxia-telangiectasia', 'gene', 'in', 'a', 'sporadic', 'T-cell', 'leukaemia.', 'Ataxia-telangiectasia', '(', 'A-T', ')', 'is', 'a', 'recessive', 'multi-system', 'disorder', 'caused', 'by', 'mutations', 'in', 'the', 'ATM', 'gene', 'at', '11q22-q23', '(', 'ref', '.', '3', ')', '.', 'The', 'risk', 'of', 'cancer', ',', 'especially', 'lymphoid', 'neoplasias', ',', 'is', 'substantially', 'elevated', 'in', 'A-T', 'patients', 'and', 'has', 'long', 'been', 'associated', 'with', 'chromosomal', 'instability', '.', 'By', 'analysing', 'tumour', 'DNA', 'from', 'patients', 'with', 'sporadic', 'T-cell', 'prolymphocytic', 'leukaemia', '(', 'T-PLL', ')', ',', 'a', 'rare', 'clonal', 'malignancy', 'with', 'similarities', 'to', 'a', 'mature', 'T-cell', 'leukaemia', 'seen', 'in', 'A-T', ',', 'we', 'demonstrate', 'a', 'high', 'frequency', 'of', 'ATM', 'mutations', 'in', 'T-PLL', '.', 'In', 'marked', 'contrast', 'to', 'the', 'ATM', 'mutation', 'pattern', 'in', 'A-T', ',', 'the', 'most', 'frequent', 'nucleotide', 'changes', 'in', 'this', 'leukaemia', 'were', 'missense', 'mutations', '.', 'These', 'clustered', 'in', 'the', 'region', 'corresponding', 'to', 'the', 'kinase', 'domain', ',', 'which', 'is', 'highly', 'conserved', 'in', 'ATM-related', 'proteins', 'in', 'mouse', ',', 'yeast', 'and', 'Drosophila', '.', 'The', 'resulting', 'amino-acid', 'substitutions', 'are', 'predicted', 'to', 'interfere', 'with', 'ATP', 'binding', 'or', 'substrate', 'recognition', '.', 'Two', 'of', 'seventeen', 'mutated', 'T-PLL', 'samples', 'had', 'a', 'previously', 'reported', 'A-T', 'allele', '.', 'In', 'contrast', ',', 'no', 'mutations', 'were', 'detected', 'in', 'the', 'p53', 'gene', ',', 'suggesting', 'that', 'this', 'tumour', 'suppressor', 'is', 'not', 'frequently', 'altered', 'in', 'this', 'leukaemia', '.', 'Occasional', 'missense', 'mutations', 'in', 'ATM', 'were', 'also', 'found', 'in', 'tumour', 'DNA', 'from', 'patients', 'with', 'B-cell', 'non-Hodgkins', 'lymphomas', '(', 'B-NHL', ')', 'and', 'a', 'B-NHL', 'cell', 'line', '.', 'The', 'evidence', 'of', 'a', 'significant', 'proportion', 'of', 'loss-of-function', 'mutations', 'and', 'a', 'complete', 'absence', 'of', 'the', 'normal', 'copy', 'of', 'ATM', 'in', 'the', 'majority', 'of', 'mutated', 'tumours', 'establishes', 'somatic', 'inactivation', 'of', 'this', 'gene', 'in', 'the', 'pathogenesis', 'of', 'sporadic', 'T-PLL', 'and', 'suggests', 'that', 'ATM', 'acts', 'as', 'a', 'tumour', 'suppressor', '.', 'As', 'constitutional', 'DNA', 'was', 'not', 'available', ',', 'a', 'putative', 'hereditary', 'predisposition', 'to', 'T-PLL', 'will', 'require', 'further', 'investigation', '.', '.']\n",
      "[NeMo I 2024-05-22 02:04:35 text_classification_dataset:240] subtokens: [CLS] cluster ##ing of miss ##ense mutations in the ata ##xia - tel ##ang ##iec ##tas ##ia gene in a sporadic t - cell le ##uka ##emia . ata ##xia - tel ##ang ##iec ##tas ##ia ( a - t ) is a recess ##ive multi - system disorder caused by mutations in the atm gene at 11 ##q ##22 - q ##23 ( ref . 3 ) . the risk of cancer , especially l ##ym ##ph ##oid neo ##pl ##asia ##s , is substantially elevated in a - t patients and has long been associated with ch ##rom ##osomal instability . by anal ##ys ##ing tu ##mour dna from patients with sporadic t - cell pro ##ly ##mp ##ho ##cy ##tic le ##uka ##emia ( t [SEP]\n",
      "[NeMo I 2024-05-22 02:04:35 text_classification_dataset:241] input_ids: 101 9324 2075 1997 3335 16700 14494 1999 1996 29533 14787 1011 10093 5654 23783 10230 2401 4962 1999 1037 24590 1056 1011 3526 3393 15750 17577 1012 29533 14787 1011 10093 5654 23783 10230 2401 1006 1037 1011 1056 1007 2003 1037 28290 3512 4800 1011 2291 8761 3303 2011 14494 1999 1996 27218 4962 2012 2340 4160 19317 1011 1053 21926 1006 25416 1012 1017 1007 1012 1996 3891 1997 4456 1010 2926 1048 24335 8458 9314 9253 24759 15396 2015 1010 2003 12381 8319 1999 1037 1011 1056 5022 1998 2038 2146 2042 3378 2007 10381 21716 27642 18549 1012 2011 20302 7274 2075 10722 20360 6064 2013 5022 2007 24590 1056 1011 3526 4013 2135 8737 6806 5666 4588 3393 15750 17577 1006 1056 102\n",
      "[NeMo I 2024-05-22 02:04:35 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2024-05-22 02:04:35 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2024-05-22 02:04:35 text_classification_dataset:244] label: 0\n",
      "[NeMo I 2024-05-22 02:04:35 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2024-05-22 02:04:35 text_classification_dataset:239] example 1: ['Myotonic', 'dystrophy', 'protein', 'kinase', 'is', 'involved', 'in', 'the', 'modulation', 'of', 'the', 'Ca2+', 'homeostasis', 'in', 'skeletal', 'muscle', 'cells.', 'Myotonic', 'dystrophy', '(', 'DM', ')', ',', 'the', 'most', 'prevalent', 'muscular', 'disorder', 'in', 'adults', ',', 'is', 'caused', 'by', '(', 'CTG', ')', 'n-repeat', 'expansion', 'in', 'a', 'gene', 'encoding', 'a', 'protein', 'kinase', '(', 'DM', 'protein', 'kinase', ';', 'DMPK', ')', 'and', 'involves', 'changes', 'in', 'cytoarchitecture', 'and', 'ion', 'homeostasis', '.', 'To', 'obtain', 'clues', 'to', 'the', 'normal', 'biological', 'role', 'of', 'DMPK', 'in', 'cellular', 'ion', 'homeostasis', ',', 'we', 'have', 'compared', 'the', 'resting', '[', 'Ca2', '+', ']', 'i', ',', 'the', 'amplitude', 'and', 'shape', 'of', 'depolarization-induced', 'Ca2', '+', 'transients', ',', 'and', 'the', 'content', 'of', 'ATP-driven', 'ion', 'pumps', 'in', 'cultured', 'skeletal', 'muscle', 'cells', 'of', 'wild-type', 'and', 'DMPK', '[', '-', '/', '-', ']', 'knockout', 'mice', '.', 'In', 'vitro-differentiated', 'DMPK', '[', '-', '/', '-', ']', 'myotubes', 'exhibit', 'a', 'higher', 'resting', '[', 'Ca2', '+', ']', 'i', 'than', 'do', 'wild-type', 'myotubes', 'because', 'of', 'an', 'altered', 'open', 'probability', 'of', 'voltage-dependent', 'l-type', 'Ca2', '+', 'and', 'Na', '+', 'channels', '.', 'The', 'mutant', 'myotubes', 'exhibit', 'smaller', 'and', 'slower', 'Ca2', '+', 'responses', 'upon', 'triggering', 'by', 'acetylcholine', 'or', 'high', 'external', 'K', '+', '.', 'In', 'addition', ',', 'we', 'observed', 'that', 'these', 'Ca2', '+', 'transients', 'partially', 'result', 'from', 'an', 'influx', 'of', 'extracellular', 'Ca2', '+', 'through', 'the', 'l-type', 'Ca2', '+', 'channel', '.', 'Neither', 'the', 'content', 'nor', 'the', 'activity', 'of', 'Na', '+', '/', 'K', '+', 'ATPase', 'and', 'sarcoplasmic', 'reticulum', 'Ca2', '+', '-ATPase', 'are', 'affected', 'by', 'DMPK', 'absence', '.', 'In', 'conclusion', ',', 'our', 'data', 'suggest', 'that', 'DMPK', 'is', 'involved', 'in', 'modulating', 'the', 'initial', 'events', 'of', 'excitation-contraction', 'coupling', 'in', 'skeletal', 'muscle', '.', '.']\n",
      "[NeMo I 2024-05-22 02:04:35 text_classification_dataset:240] subtokens: [CLS] my ##oton ##ic d ##yst ##rop ##hy protein kinase is involved in the modulation of the ca ##2 + home ##osta ##sis in skeletal muscle cells . my ##oton ##ic d ##yst ##rop ##hy ( d ##m ) , the most prevalent muscular disorder in adults , is caused by ( ct ##g ) n - repeat expansion in a gene encoding a protein kinase ( d ##m protein kinase ; d ##mp ##k ) and involves changes in cy ##to ##ar ##chi ##tec ##ture and ion home ##osta ##sis . to obtain clues to the normal biological role of d ##mp ##k in cellular ion home ##osta ##sis , we have compared the resting [ ca ##2 + ] i , the amplitude and shape of [SEP]\n",
      "[NeMo I 2024-05-22 02:04:35 text_classification_dataset:241] input_ids: 101 2026 25862 2594 1040 27268 18981 10536 5250 21903 2003 2920 1999 1996 25502 1997 1996 6187 2475 1009 2188 28696 6190 1999 20415 6740 4442 1012 2026 25862 2594 1040 27268 18981 10536 1006 1040 2213 1007 1010 1996 2087 15157 13472 8761 1999 6001 1010 2003 3303 2011 1006 14931 2290 1007 1050 1011 9377 4935 1999 1037 4962 17181 1037 5250 21903 1006 1040 2213 5250 21903 1025 1040 8737 2243 1007 1998 7336 3431 1999 22330 3406 2906 5428 26557 11244 1998 10163 2188 28696 6190 1012 2000 6855 15774 2000 1996 3671 6897 2535 1997 1040 8737 2243 1999 12562 10163 2188 28696 6190 1010 2057 2031 4102 1996 8345 1031 6187 2475 1009 1033 1045 1010 1996 22261 1998 4338 1997 102\n",
      "[NeMo I 2024-05-22 02:04:35 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2024-05-22 02:04:35 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2024-05-22 02:04:35 text_classification_dataset:244] label: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-05-22 02:04:35 text_classification_dataset:250] Found 10 out of 10 sentences with more than 128 subtokens. Truncated long sentences from the end.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-22 02:04:35 data_preprocessing:299] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2024-05-22 02:04:35 data_preprocessing:301] Min: 129 |                  Max: 129 |                  Mean: 129.0 |                  Median: 129.0\n",
      "[NeMo I 2024-05-22 02:04:35 data_preprocessing:307] 75 percentile: 129.00\n",
      "[NeMo I 2024-05-22 02:04:35 data_preprocessing:308] 99 percentile: 129.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-05-22 02:04:35 modelPT:197] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact forit has already been registered.\n",
      "[NeMo W 2024-05-22 02:04:35 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/nemo/core/classes/modelPT.py:243: UserWarning: update_node() is deprecated, use OmegaConf.update(). (Since 2.0)\n",
      "      self.cfg.update_node(config_path, return_path)\n",
      "    \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertEncoder: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model \n",
    "model = nemo_nlp.models.TextClassificationModel(config.model, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-22 02:04:37 modelPT:748] Optimizer config = Adam (\n",
      "    Parameter Group 0\n",
      "        amsgrad: False\n",
      "        betas: [0.9, 0.999]\n",
      "        eps: 1e-08\n",
      "        lr: 5e-05\n",
      "        weight_decay: 0.01\n",
      "    )\n",
      "[NeMo I 2024-05-22 02:04:37 lr_scheduler:617] Scheduler \"<nemo.core.optim.lr_scheduler.WarmupAnnealing object at 0x75b6c6be9f70>\" \n",
      "    will be used during training (effective maximum steps = 55) - \n",
      "    Parameters : \n",
      "    (warmup_steps: null\n",
      "    warmup_ratio: 0.1\n",
      "    last_epoch: -1\n",
      "    max_steps: 55\n",
      "    )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "\n",
      "  | Name                  | Type                 | Params\n",
      "---------------------------------------------------------------\n",
      "0 | loss                  | CrossEntropyLoss     | 0     \n",
      "1 | bert_model            | BertEncoder          | 109 M \n",
      "2 | classifier            | SequenceClassifier   | 592 K \n",
      "3 | classification_report | ClassificationReport | 0     \n",
      "---------------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "440.301   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a109a6d041d842099104867175074620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-05-22 02:04:39 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/nemo/core/classes/modelPT.py:308: UserWarning: update_node() is deprecated, use OmegaConf.update(). (Since 2.0)\n",
      "      conf.update_node(conf_path, item.path)\n",
      "    \n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/dli/task/Downloads/nvidia-dli-llm/part_2/data_text_output/TextClassification/2024-05-22_02-04-18/checkpoints/TextClassification.nemo'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    636\u001b[0m                     \u001b[0;31m# run train epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_training_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mrun_training_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_training_batch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m                 \u001b[0mbatch_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_training_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mrun_training_batch\u001b[0;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    653\u001b[0m                         \u001b[0;31m# optimizer step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_step_and_backward_closure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(self, optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0;31m# model hook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         model_ref.optimizer_step(\n\u001b[0m\u001b[1;32m    426\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs)\u001b[0m\n\u001b[1;32m   1389\u001b[0m             \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLightningOptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_lightning_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1390\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer_closure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure, *args, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__optimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofiler_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprofiler_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_total_optimizer_step_calls\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py\u001b[0m in \u001b[0;36m__optimizer_step\u001b[0;34m(self, closure, profiler_name, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofiler_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_closure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(self, optimizer, opt_idx, lambda_closure, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m         \"\"\"\n\u001b[0;32m--> 273\u001b[0;31m         make_optimizer_step = self.precision_plugin.pre_optimizer_step(\n\u001b[0m\u001b[1;32m    274\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_closure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/native_amp.py\u001b[0m in \u001b[0;36mpre_optimizer_step\u001b[0;34m(self, pl_module, optimizer, optimizer_idx, lambda_closure, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m             )\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mlambda_closure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mtrain_step_and_backward_closure\u001b[0;34m()\u001b[0m\n\u001b[1;32m    647\u001b[0m                         \u001b[0;32mdef\u001b[0m \u001b[0mtrain_step_and_backward_closure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m                             result = self.training_step_and_backward(\n\u001b[0m\u001b[1;32m    649\u001b[0m                                 \u001b[0msplit_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhiddens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mtraining_step_and_backward\u001b[0;34m(self, split_batch, batch_idx, opt_idx, optimizer, hiddens)\u001b[0m\n\u001b[1;32m    741\u001b[0m             \u001b[0;31m# lightning module hook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 742\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhiddens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    743\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_curr_step_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, split_batch, batch_idx, opt_idx, hiddens)\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training_step\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                 \u001b[0mtraining_step_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_training_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/overrides/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrunning_stage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mRunningStage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAINING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/nemo/utils/model_utils.py\u001b[0m in \u001b[0;36mwrap_training_step\u001b[0;34m(wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwrap_training_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLightningModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m     \u001b[0moutput_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/nemo/collections/nlp/models/text_classification/text_classification_model.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/nemo/core/classes/common.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0;31m# Call the method - this can be forward, or any other callable method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/nemo/collections/nlp/models/text_classification/text_classification_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m         hidden_states = self.bert_model(\n\u001b[0m\u001b[1;32m    106\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/nemo/core/classes/common.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0;31m# Call the method - this can be forward, or any other callable method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/nemo/collections/nlp/modules/common/huggingface/bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    970\u001b[0m         )\n\u001b[0;32m--> 971\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    972\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    567\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    569\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m   1994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1995\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mgelu\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m   1458\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgelu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1459\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 0; 9.53 GiB total capacity; 4.33 GiB already allocated; 124.88 MiB free; 4.49 GiB reserved in total by PyTorch)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloader, val_dataloaders, datamodule)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;31m# dispath `start_training` or `start_testing` or `start_predicting`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;31m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mdispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_or_test_or_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\u001b[0m in \u001b[0;36mstart_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_testing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\u001b[0m in \u001b[0;36mstart_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Trainer'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m# double dispatch to initiate the training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_testing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Trainer'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    668\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0;31m# hook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mon_train_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;31m# hook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_train_end\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;31m# todo: TPU 8 cores hangs in flush with TensorBoard. Might do for all loggers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mcall_hook\u001b[0;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1093\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m                 \u001b[0mtrainer_hook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1095\u001b[0;31m                 \u001b[0mtrainer_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m             \u001b[0;31m# next call hook in lightningModule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/callback_hook.py\u001b[0m in \u001b[0;36mon_train_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;34m\"\"\"Called when the train ends.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_pretrain_routine_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrank_zero_only\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/nemo/utils/exp_manager.py\u001b[0m in \u001b[0;36mon_train_end\u001b[0;34m(self, trainer, pl_module)\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_best_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m             \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_connector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_model_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_gpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0mpl_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnemo_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostfix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/nemo/collections/nlp/models/nlp_model.py\u001b[0m in \u001b[0;36msave_to\u001b[0;34m(self, save_path)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0;31m# super.save_to only runs on global rank 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_default_save_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrank_zero_only\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/nemo/core/classes/modelPT.py\u001b[0m in \u001b[0;36msave_to\u001b[0;34m(self, save_path)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_save_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/nemo/collections/nlp/models/nlp_model.py\u001b[0m in \u001b[0;36m_default_save_to\u001b[0;34m(self, save_path)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_global_rank_zero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_save_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/nemo/core/classes/modelPT.py\u001b[0m in \u001b[0;36m_default_save_to\u001b[0;34m(self, save_path)\u001b[0m\n\u001b[1;32m    332\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_artifact_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath2yaml_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig_yaml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_nemo_file_from_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtmpdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mrank_zero_only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/nemo/collections/nlp/models/nlp_model.py\u001b[0m in \u001b[0;36m_make_nemo_file_from_folder\u001b[0;34m(filename, source_dir)\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_nemo_file_from_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNLPModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNLPModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_nemo_file_from_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/nemo/core/classes/modelPT.py\u001b[0m in \u001b[0;36m_make_nemo_file_from_folder\u001b[0;34m(filename, source_dir)\u001b[0m\n\u001b[1;32m   1267\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_nemo_file_from_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1269\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w:gz\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1270\u001b[0m             \u001b[0mtar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marcname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/tarfile.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(cls, name, mode, fileobj, bufsize, **kwargs)\u001b[0m\n\u001b[1;32m   1619\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1620\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCompressionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unknown compression type %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mcomptype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1621\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1623\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;34m\"|\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/tarfile.py\u001b[0m in \u001b[0;36mgzopen\u001b[0;34m(cls, name, mode, fileobj, compresslevel, **kwargs)\u001b[0m\n\u001b[1;32m   1665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1666\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1667\u001b[0;31m             \u001b[0mfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1668\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1669\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/gzip.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0mfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmyfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/dli/task/Downloads/nvidia-dli-llm/part_2/data_text_output/TextClassification/2024-05-22_02-04-18/checkpoints/TextClassification.nemo'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# start model training and save result\n",
    "# The training takes about 3 minutes to run\n",
    "trainer.fit(model)\n",
    "model.save_to(config.model.nemo_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model with `trainer.test`, which will automatically use the file path to the test set we updated in `config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 254.00 MiB (GPU 0; 9.53 GiB total capacity; 4.33 GiB already allocated; 124.88 MiB free; 4.49 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ea867864b71d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, model, test_dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__test_given_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__test_using_best_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m__test_given_model\u001b[0;34m(self, model, test_dataloaders)\u001b[0m\n\u001b[1;32m    972\u001b[0m         \u001b[0;31m# run test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m         \u001b[0;31m# sets up testing so we short circuit to eval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 974\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    975\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[0;31m# teardown\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloader, val_dataloaders, datamodule)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;31m# plugin will setup fitting (e.g. ddp will launch child processes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;31m# dispath `start_training` or `start_testing` or `start_predicting`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mpre_dispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;31m# log hyper-parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\u001b[0m in \u001b[0;36mpre_dispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;34m\"\"\"Hook to do something before the training/evaluation/prediction starts.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp.py\u001b[0m in \u001b[0;36mpre_dispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigure_sync_batchnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigure_ddp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbarrier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp.py\u001b[0m in \u001b[0;36mconfigure_ddp\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconfigure_ddp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_configure_ddp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         self._model = DistributedDataParallel(\n\u001b[0m\u001b[1;32m    224\u001b[0m             \u001b[0mLightningDistributedModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0mdevice_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetermine_ddp_device_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, module, device_ids, output_device, dim, broadcast_buffers, process_group, bucket_cap_mb, find_unused_parameters, check_reduction, gradient_as_bucket_view)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0;31m# Sync params and buffers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sync_params_and_buffers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauthoritative_rank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ddp_init_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py\u001b[0m in \u001b[0;36m_sync_params_and_buffers\u001b[0;34m(self, authoritative_rank)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_states\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m             self._distributed_broadcast_coalesced(\n\u001b[0m\u001b[1;32m    458\u001b[0m                 \u001b[0mmodule_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_bucket_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py\u001b[0m in \u001b[0;36m_distributed_broadcast_coalesced\u001b[0;34m(self, tensors, buffer_size, authoritative_rank)\u001b[0m\n\u001b[1;32m   1152\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthoritative_rank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m     ):\n\u001b[0;32m-> 1154\u001b[0;31m         dist._broadcast_coalesced(\n\u001b[0m\u001b[1;32m   1155\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_group\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthoritative_rank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 254.00 MiB (GPU 0; 9.53 GiB total capacity; 4.33 GiB already allocated; 124.88 MiB free; 4.49 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "trainer.test(model=model, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run inference using the inference samples from `config`. We can check on them by just printing directly from the `config.model.infer_samples` key object.  It displays as a list of strings.\n",
    "\n",
    "To run inference tor text classification, use the `model.classifytext` method.  The inferred labels are output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(config.model.infer_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifytext(queries=config.model.infer_samples, batch_size=64, max_seq_length=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.3 Exercise: Query the Model\n",
    "What if we wanted to specify additional queries for inference?  The `model.classifytext` method we just used specifies the queries, but they do not _have_ to be in the config file.  We can simply create a list of strings for our queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_queries = [\n",
    "    'Occasional missense mutations in ATM were also found in tumour DNA from patients with B-cell non-Hodgkins lymphomas.',\n",
    "    'Myotonic dystrophy protein kinase is involved in the modulation of the Ca2+ homeostasis in skeletal muscle cells.',\n",
    "    'Constitutional RB1-gene mutations in patients with isolated unilateral retinoblastoma.',\n",
    "    'Hereditary deficiency of the fifth component of complement in man. I. Clinical, immunochemical, and family studies.'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run inference on the `my_queries` list.  If you get stuck, refer to the [solution](solutions/ex2.3.3.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Run inference over the my_queries list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "You've built a text classifier with three classes and learned:\n",
    "* How to use NeMo NLP model config files and scripts to quickly create experiments\n",
    "* How to override the config `model`, `trainer`, and `exp_manager settings`\n",
    "* How to train, evaluate, and infer a text classifier using a single command line\n",
    "* How to train, evaluate, and infer a text classifier using PyTorch Lightning\n",
    "\n",
    "You're ready to try a different NLP task.<br>\n",
    "\n",
    "Move on to [3.0 Build a Named Entity Recognizer](030_NamedEntityRecognition.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
